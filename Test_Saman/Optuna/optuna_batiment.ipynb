{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e849fc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 14:44:19,394] A new study created in memory with name: no-name-9d503ad0-f11e-40ca-9668-42fcf018039e\n",
      "C:\\Users\\mymar\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-06-13 14:45:08,086] Trial 0 finished with value: 0.025153128430247307 and parameters: {'lr': 0.00029517351520142707, 'filters1': 74, 'filters2': 45, 'dense_units': 118}. Best is trial 0 with value: 0.025153128430247307.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step\n",
      "scaled_dataset — MAE: 11.5322, MSE: 227.9534, R²: 0.2941\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import ConvLSTM1D, Flatten, Dense\n",
    "\n",
    "\n",
    "class WeightLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, layer_index=0):\n",
    "        self.layer_index = layer_index\n",
    "        self.weights_per_epoch = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        weights = self.model.layers[self.layer_index].get_weights()[0]\n",
    "        self.weights_per_epoch.append(weights.copy())\n",
    "\n",
    "\n",
    "def load_and_prepare_data(csv_path, production_column='production', window_size=24):\n",
    "    df = pd.read_csv(csv_path, sep=',')\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(df.values)\n",
    "    target_scaler = MinMaxScaler()\n",
    "    target_scaler.fit(df[[production_column]])\n",
    "    target_col_idx = df.columns.get_loc(production_column)\n",
    "    x, y = [], []\n",
    "    for i in range(window_size, len(data_scaled)):\n",
    "        x.append(data_scaled[i-window_size:i])\n",
    "        y.append(data_scaled[i, target_col_idx])\n",
    "    x, y = np.array(x), np.array(y)\n",
    "    train_split_index = int(0.8 * len(x))\n",
    "    test_split_index = int(0.9 * len(x))\n",
    "    x_train, y_train = x[:train_split_index], y[:train_split_index]\n",
    "    x_test, y_test = x[train_split_index:test_split_index], y[train_split_index:test_split_index]\n",
    "    x_val, y_val = x[test_split_index:], y[test_split_index:]\n",
    "    x_train_conv = np.expand_dims(x_train, axis=2)\n",
    "    x_test_conv = np.expand_dims(x_test, axis=2)\n",
    "    x_val_conv = np.expand_dims(x_val, axis=2)\n",
    "    return x_train_conv, y_train, x_test_conv, y_test, x_val_conv, y_val, df, target_scaler\n",
    "\n",
    "\n",
    "def build_convlstm_model(lr, filters1, filters2, dense_units, input_shape):\n",
    "    model = Sequential([\n",
    "        ConvLSTM1D(filters=int(filters1), kernel_size=(1,), activation='tanh',\n",
    "                   return_sequences=True, input_shape=input_shape),\n",
    "        ConvLSTM1D(filters=int(filters2), kernel_size=(1,), activation='tanh', return_sequences=False),\n",
    "        Flatten(),\n",
    "        Dense(units=int(dense_units), activation='relu'),\n",
    "        Dense(1, activation=\"linear\")\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"mae\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_val, y_val,\n",
    "                             epochs=50, batch_size=512, verbose=0, dataset_name=\"dataset\"):\n",
    "    stop_early = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    weight_logger = WeightLogger(layer_index=0)\n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=verbose,\n",
    "                        callbacks=[stop_early, weight_logger])\n",
    "    training_time = time.time() - start_time\n",
    "    w00 = [w[0, 0] for w in weight_logger.weights_per_epoch]\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(w00)\n",
    "    plt.xlabel(\"Époque\")\n",
    "    plt.ylabel(\"Poids [0,0]\")\n",
    "    plt.title(\"Évolution du poids [0,0]\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{dataset_name}_poids_w00.png\")\n",
    "    plt.close()\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history.history['loss'], label='Train')\n",
    "    plt.plot(history.history['val_loss'], label='Validation')\n",
    "    plt.title(\"Loss par époque\")\n",
    "    plt.xlabel(\"Époque\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{dataset_name}_loss_curve.png\")\n",
    "    plt.close()\n",
    "    return history, training_time, weight_logger\n",
    "\n",
    "\n",
    "def inference_and_plot(model, x_test, y_test, target_scaler, dataset_name): \n",
    "    preds = model.predict(x_test)\n",
    "    y_test_real = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    y_pred_real = target_scaler.inverse_transform(preds.reshape(-1, 1)).flatten()\n",
    "    mae = mean_absolute_error(y_test_real, y_pred_real)\n",
    "    mse = mean_squared_error(y_test_real, y_pred_real)\n",
    "    r2 = r2_score(y_test_real, y_pred_real)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_test_real, label=\"Vrai\")\n",
    "    plt.plot(y_pred_real, label=\"Prévu\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"{dataset_name} - MAE: {mae:.4f} | R²: {r2:.4f} | MSE: {mse:.4f}\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{dataset_name}_courbe_perf.png\")\n",
    "    plt.close()\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_test_real, y_pred_real, alpha=0.7, color='orange')\n",
    "    plt.plot([min(y_test_real), max(y_test_real)], [min(y_test_real), max(y_test_real)], 'r--')\n",
    "    plt.xlabel(\"Réel\")\n",
    "    plt.ylabel(\"Prédit\")\n",
    "    plt.title(\"Scatter plot\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{dataset_name}_scatter_perf.png\")\n",
    "    plt.close()\n",
    "    errors = y_test_real - y_pred_real\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(errors, bins=30, color='orange', edgecolor='black')\n",
    "    plt.title(\"Histogramme des erreurs\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{dataset_name}_hist_errors.png\")\n",
    "    plt.close()\n",
    "    # Corriger division par zéro pour les erreurs en pourcentage\n",
    "    safe_y_test_real = np.where(y_test_real == 0, np.nan, y_test_real)\n",
    "    \n",
    "    df_stats = pd.DataFrame({\n",
    "        \"Y_test\": y_test_real,\n",
    "        \"Y_pred\": y_pred_real,\n",
    "        \"Error\": errors,\n",
    "        \"Error_Percent\": np.abs(errors) / safe_y_test_real * 100\n",
    "    })\n",
    "    \n",
    "    # Sauvegarder les statistiques sans générer d'avertissements\n",
    "    df_stats.describe().to_csv(f\"{dataset_name}_stats_erreurs.csv\")\n",
    "    return mae, mse, r2\n",
    "\n",
    "\n",
    "def objective(trial, input_shape, x_train, y_train, x_val, y_val):\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    f1 = trial.suggest_int('filters1', 32, 128)\n",
    "    f2 = trial.suggest_int('filters2', 32, 128)\n",
    "    dense = trial.suggest_int('dense_units', 32, 128)\n",
    "    model = build_convlstm_model(lr, f1, f2, dense, input_shape)\n",
    "    history = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "                        epochs=10, batch_size=256, verbose=0)\n",
    "    return min(history.history['val_loss'])\n",
    "\n",
    "\n",
    "def run_experiment(csv_path, dataset_name):\n",
    "    x_train, y_train, x_test, y_test, x_val, y_val, df, target_scaler = load_and_prepare_data(csv_path)\n",
    "    input_shape = x_train.shape[1:]\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: objective(trial, input_shape, x_train, y_train, x_val, y_val), n_trials=100)\n",
    "    best = study.best_params\n",
    "    model = build_convlstm_model(best['lr'], best['filters1'], best['filters2'], best['dense_units'], input_shape)\n",
    "    history, training_time, _ = train_and_evaluate_model(model, x_train, y_train, x_val, y_val,\n",
    "                                                         epochs=700, dataset_name=dataset_name)\n",
    "    mae, mse, r2 = inference_and_plot(model, x_test, y_test, target_scaler, dataset_name)\n",
    "    print(f\"{dataset_name} — MAE: {mae:.4f}, MSE: {mse:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment(\"scaled_dataset.csv\", \"scaled_dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2435458f-a060-4433-ab64-fd3ab6d94955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 14:35:55,368] A new study created in memory with name: no-name-d3712ccc-5fa6-4ba4-ba5c-2f6a9f0e2b79\n",
      "C:\\Users\\mymar\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-06-13 14:37:05,543] Trial 0 finished with value: 0.02478390745818615 and parameters: {'lr': 0.0026302982000410034, 'filters1': 69, 'filters2': 98, 'dense_units': 96}. Best is trial 0 with value: 0.02478390745818615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step\n",
      "full_dataset — MAE: 11.7640, MSE: 235.1456, R²: 0.2789\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import ConvLSTM1D, Flatten, Dense\n",
    "\n",
    "\n",
    "class WeightLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, layer_index=0):\n",
    "        self.layer_index = layer_index\n",
    "        self.weights_per_epoch = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        weights = self.model.layers[self.layer_index].get_weights()[0]\n",
    "        self.weights_per_epoch.append(weights.copy())\n",
    "\n",
    "\n",
    "def load_and_prepare_data(csv_path, production_column='production', window_size=24):\n",
    "    # === 1. Chargement des données ===\n",
    "    df = pd.read_csv(\"full_dataset.csv\",sep=\";\")  # Remplace par ton CSV réel\n",
    "    # 1.1 Conversion de la colonne date\n",
    "    df['date1'] = pd.to_datetime(df['date'], dayfirst=True)\n",
    "    \n",
    "    # 1.2 Création des features temporelles cycliques\n",
    "    df['dayofweek'] = df['date1'].dt.dayofweek\n",
    "    df['month'] = df['date1'].dt.month\n",
    "    \n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "    df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "    \n",
    "    df.drop(columns=['month', 'dayofweek'], inplace=True)\n",
    "    \n",
    "    # 1.3 Supprimer les colonnes non numériques avant normalisation\n",
    "    df = df.select_dtypes(include=[np.number])\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(df.values)\n",
    "    target_scaler = MinMaxScaler()\n",
    "    target_scaler.fit(df[[production_column]])\n",
    "    target_col_idx = df.columns.get_loc(production_column)\n",
    "    x, y = [], []\n",
    "    for i in range(window_size, len(data_scaled)):\n",
    "        x.append(data_scaled[i-window_size:i])\n",
    "        y.append(data_scaled[i, target_col_idx])\n",
    "    x, y = np.array(x), np.array(y)\n",
    "    train_split_index = int(0.8 * len(x))\n",
    "    test_split_index = int(0.9 * len(x))\n",
    "    x_train, y_train = x[:train_split_index], y[:train_split_index]\n",
    "    x_test, y_test = x[train_split_index:test_split_index], y[train_split_index:test_split_index]\n",
    "    x_val, y_val = x[test_split_index:], y[test_split_index:]\n",
    "    x_train_conv = np.expand_dims(x_train, axis=2)\n",
    "    x_test_conv = np.expand_dims(x_test, axis=2)\n",
    "    x_val_conv = np.expand_dims(x_val, axis=2)\n",
    "    return x_train_conv, y_train, x_test_conv, y_test, x_val_conv, y_val, df, target_scaler\n",
    "\n",
    "\n",
    "def build_convlstm_model(lr, filters1, filters2, dense_units, input_shape):\n",
    "    model = Sequential([\n",
    "        ConvLSTM1D(filters=int(filters1), kernel_size=(1,), activation='tanh',\n",
    "                   return_sequences=True, input_shape=input_shape),\n",
    "        ConvLSTM1D(filters=int(filters2), kernel_size=(1,), activation='tanh', return_sequences=False),\n",
    "        Flatten(),\n",
    "        Dense(units=int(dense_units), activation='relu'),\n",
    "        Dense(1, activation=\"linear\")\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"mae\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_val, y_val,\n",
    "                             epochs=50, batch_size=512, verbose=0, dataset_name=\"dataset\"):\n",
    "    stop_early = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    weight_logger = WeightLogger(layer_index=0)\n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=verbose,\n",
    "                        callbacks=[stop_early, weight_logger])\n",
    "    training_time = time.time() - start_time\n",
    "    w00 = [w[0, 0] for w in weight_logger.weights_per_epoch]\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(w00)\n",
    "    plt.xlabel(\"Époque\")\n",
    "    plt.ylabel(\"Poids [0,0]\")\n",
    "    plt.title(\"Évolution du poids [0,0]\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{dataset_name}_poids_w00.png\")\n",
    "    plt.close()\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history.history['loss'], label='Train')\n",
    "    plt.plot(history.history['val_loss'], label='Validation')\n",
    "    plt.title(\"Loss par époque\")\n",
    "    plt.xlabel(\"Époque\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{dataset_name}_loss_curve.png\")\n",
    "    plt.close()\n",
    "    return history, training_time, weight_logger\n",
    "\n",
    "\n",
    "def inference_and_plot(model, x_test, y_test, target_scaler, dataset_name):\n",
    "    preds = model.predict(x_test)\n",
    "    y_test_real = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    y_pred_real = target_scaler.inverse_transform(preds.reshape(-1, 1)).flatten()\n",
    "    mae = mean_absolute_error(y_test_real, y_pred_real)\n",
    "    mse = mean_squared_error(y_test_real, y_pred_real)\n",
    "    r2 = r2_score(y_test_real, y_pred_real)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_test_real, label=\"Vrai\")\n",
    "    plt.plot(y_pred_real, label=\"Prévu\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"{dataset_name} - MAE: {mae:.4f} | R²: {r2:.4f} | MSE: {mse:.4f}\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{dataset_name}_courbe_perf.png\")\n",
    "    plt.close()\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_test_real, y_pred_real, alpha=0.7, color='orange')\n",
    "    plt.plot([min(y_test_real), max(y_test_real)], [min(y_test_real), max(y_test_real)], 'r--')\n",
    "    plt.xlabel(\"Réel\")\n",
    "    plt.ylabel(\"Prédit\")\n",
    "    plt.title(\"Scatter plot\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{dataset_name}_scatter_perf.png\")\n",
    "    plt.close()\n",
    "    errors = y_test_real - y_pred_real\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(errors, bins=30, color='orange', edgecolor='black')\n",
    "    plt.title(\"Histogramme des erreurs\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{dataset_name}_hist_errors.png\")\n",
    "    plt.close()\n",
    "    # Éviter la division par zéro pour les erreurs en pourcentage\n",
    "    safe_y_test_real = np.where(y_test_real == 0, np.nan, y_test_real)\n",
    "    \n",
    "    df_stats = pd.DataFrame({\n",
    "        \"Y_test\": y_test_real,\n",
    "        \"Y_pred\": y_pred_real,\n",
    "        \"Error\": errors,\n",
    "        \"Error_Percent\": np.abs(errors) / safe_y_test_real * 100\n",
    "    })\n",
    "    \n",
    "    # Sauvegarde sans warning\n",
    "    df_stats.describe().to_csv(f\"{dataset_name}_stats_erreurs.csv\")\n",
    "    return mae, mse, r2\n",
    "\n",
    "\n",
    "def objective(trial, input_shape, x_train, y_train, x_val, y_val):\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    f1 = trial.suggest_int('filters1', 32, 128)\n",
    "    f2 = trial.suggest_int('filters2', 32, 128)\n",
    "    dense = trial.suggest_int('dense_units', 32, 128)\n",
    "    model = build_convlstm_model(lr, f1, f2, dense, input_shape)\n",
    "    history = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "                        epochs=10, batch_size=256, verbose=0)\n",
    "    return min(history.history['val_loss'])\n",
    "\n",
    "\n",
    "def run_experiment(csv_path, dataset_name):\n",
    "    x_train, y_train, x_test, y_test, x_val, y_val, df, target_scaler = load_and_prepare_data(csv_path)\n",
    "    input_shape = x_train.shape[1:]\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: objective(trial, input_shape, x_train, y_train, x_val, y_val), n_trials=100)\n",
    "    best = study.best_params\n",
    "    model = build_convlstm_model(best['lr'], best['filters1'], best['filters2'], best['dense_units'], input_shape)\n",
    "    history, training_time, _ = train_and_evaluate_model(model, x_train, y_train, x_val, y_val,\n",
    "                                                         epochs=700, dataset_name=dataset_name)\n",
    "    mae, mse, r2 = inference_and_plot(model, x_test, y_test, target_scaler, dataset_name)\n",
    "    print(f\"{dataset_name} — MAE: {mae:.4f}, MSE: {mse:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment(\"full_dataset.csv\", \"full_dataset\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
