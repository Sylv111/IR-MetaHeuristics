{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b6891f0-7f90-44b5-a54c-c5ac0a93a199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 14:33:38.158484: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-16 14:33:38.222131: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-16 14:33:40.338723: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset: scaled_dataset ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 14:33:41.757711: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-06-16 14:33:41.757773: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:160] env: CUDA_VISIBLE_DEVICES=\"-1\"\n",
      "2025-06-16 14:33:41.757781: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA\n",
      "2025-06-16 14:33:41.757787: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:171] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n",
      "2025-06-16 14:33:41.757792: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:176] retrieving CUDA diagnostic information for host: ST-RECH-BLOC\n",
      "2025-06-16 14:33:41.757797: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] hostname: ST-RECH-BLOC\n",
      "2025-06-16 14:33:41.758026: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] libcuda reported version is: 535.247.1\n",
      "2025-06-16 14:33:41.758049: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:194] kernel reported version is: 535.247.1\n",
      "2025-06-16 14:33:41.758052: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:284] kernel version seems to match DSO: 535.247.1\n",
      "/home/sismail/py_envs/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 14:33:54,680 - pyswarms.single.global_best - INFO - Optimize for 5 iters with {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
      "pyswarms.single.global_best:   0%|                                                                              |0/5/home/sismail/py_envs/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "pyswarms.single.global_best: 100%|████████████████████████████████████████████████████████████|5/5, best_cost=0.0241\n",
      "2025-06-16 15:19:20,640 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.024064794182777405, best pos: [7.98232034e-03 3.36459319e+01 6.53240081e+01 7.81262379e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step\n",
      "\n",
      "========== RÉSULTATS FINAUX ==========\n",
      "          Dataset  MAE e-base  MSE e-base  R2 e-base  \\\n",
      "0  scaled_dataset    0.029263    0.001454  -0.296334   \n",
      "\n",
      "   T(entrainement-e-base)[s]  PSO best R2  PSO best cost (val_loss)  \\\n",
      "0                  10.869625     0.312563                  0.024065   \n",
      "\n",
      "                 Paramètres optimisés    T(PSO)[s]  \\\n",
      "0  lr=0.00798, f1=34, f2=65, dense=78  2725.962314   \n",
      "\n",
      "   T(entrainement-optimisé)[s]  MAE optimisé  MSE optimisé  R2 optimisé  \\\n",
      "0                    29.408428      0.021409      0.000771     0.312563   \n",
      "\n",
      "                      Graph_scatter                     Graph_hist  \\\n",
      "0  plots/scaled_dataset_scatter.png  plots/scaled_dataset_hist.png   \n",
      "\n",
      "                      Graph_courbe  T(evaluation-inference)[s]  \\\n",
      "0  plots/scaled_dataset_courbe.png                    3.122226   \n",
      "\n",
      "   Nombre d'époques e-base  Nombre d'époques optimisé  \n",
      "0                        1                          5  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pyswarms as ps\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.layers import ConvLSTM1D, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "###############################################################################\n",
    "# 1) Chargement et préparation des données\n",
    "###############################################################################\n",
    "def load_and_prepare_data(csv_path, production_column='production', window_size=24):\n",
    "    \"\"\"\n",
    "    Charge le fichier CSV, scale les données, crée des fenêtres (x,y),\n",
    "    et renvoie les splits (x_train, y_train, x_test, y_test, x_val, y_val).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Mise à l'échelle\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data_scaled = scaler.fit_transform(df.values)\n",
    "    \n",
    "    # Retrouver l'index de la colonne cible\n",
    "    target_col_idx = df.columns.get_loc(production_column)\n",
    "    \n",
    "    # Création des fenêtres\n",
    "    x, y = [], []\n",
    "    for i in range(window_size, len(data_scaled)):\n",
    "        x.append(data_scaled[i-window_size:i])\n",
    "        y.append(data_scaled[i, target_col_idx])\n",
    "    x, y = np.array(x), np.array(y)\n",
    "    \n",
    "    # Split train/test/val\n",
    "    # Ici, 80% train, 10% test, 10% val (à adapter si besoin)\n",
    "    train_split_index = int(0.8 * len(x))\n",
    "    test_split_index  = int(0.9 * len(x))\n",
    "    \n",
    "    x_train, y_train = x[:train_split_index], y[:train_split_index]\n",
    "    x_test,  y_test  = x[train_split_index:test_split_index], y[train_split_index:test_split_index]\n",
    "    x_val,   y_val   = x[test_split_index:], y[test_split_index:]\n",
    "    \n",
    "    # Adapter la forme pour ConvLSTM1D (on insère un channel dimension)\n",
    "    x_train_conv = np.expand_dims(x_train, axis=2)\n",
    "    x_test_conv  = np.expand_dims(x_test, axis=2)\n",
    "    x_val_conv   = np.expand_dims(x_val, axis=2)\n",
    "    \n",
    "    return (x_train_conv, y_train,\n",
    "            x_test_conv,  y_test,\n",
    "            x_val_conv,   y_val,\n",
    "            df)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2) Baseline (e-base) : un simple entraînement avec des hyperparamètres fixes\n",
    "###############################################################################\n",
    "def build_baseline_model(input_shape):\n",
    "    \"\"\"\n",
    "    Construit un modèle ConvLSTM basique avec des hyperparamètres\n",
    "    fixes (par ex. 64 filtres, 64 neurones denses, lr=0.001).\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        ConvLSTM1D(filters=64, kernel_size=(1,), activation='tanh',\n",
    "                   return_sequences=True, input_shape=input_shape),\n",
    "        ConvLSTM1D(filters=64, kernel_size=(1,), activation='tanh', return_sequences=False),\n",
    "        Flatten(),\n",
    "        Dense(units=64, activation='relu'),\n",
    "        Dense(1, activation=\"linear\")\n",
    "    ], name=\"baseline_conv_lstm\")\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss=\"mae\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3) Modèle paramétrable pour PSO\n",
    "###############################################################################\n",
    "def build_convlstm_model(lr, filters1, filters2, dense_units, input_shape):\n",
    "    \"\"\"\n",
    "    Construit et compile un modèle ConvLSTM1D avec hyperparamètres modulables.\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        ConvLSTM1D(filters=int(filters1), kernel_size=(1,), activation='tanh',\n",
    "                   return_sequences=True, input_shape=input_shape),\n",
    "        ConvLSTM1D(filters=int(filters2), kernel_size=(1,), activation='tanh', return_sequences=False),\n",
    "        Flatten(),\n",
    "        Dense(units=int(dense_units), activation='relu'),\n",
    "        Dense(1, activation=\"linear\")\n",
    "    ], name=\"model_conv_lstm\")\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"mae\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "def plot_and_save_analysis(y_test, y_pred, save_dir, dataset_name):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Scatter plot (prédictions vs réel)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.7, color='orange')\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--', label=\"Idéal (y = ŷ)\")\n",
    "    plt.xlabel(\"Valeurs réelles (y)\")\n",
    "    plt.ylabel(\"Prédictions (ŷ)\")\n",
    "    plt.title(f\"{dataset_name} - Prédictions vs Réel\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    scatter_path = os.path.join(save_dir, f\"{dataset_name}_scatter.png\")\n",
    "    plt.savefig(scatter_path)\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Histogramme des erreurs\n",
    "    errors = y_test - y_pred\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(errors, bins=20, color='orange', edgecolor='black')\n",
    "    plt.title(f\"{dataset_name} - Distribution des erreurs\")\n",
    "    plt.xlabel(\"Erreur (y - ŷ)\")\n",
    "    plt.ylabel(\"Fréquence\")\n",
    "    plt.grid(True)\n",
    "    hist_path = os.path.join(save_dir, f\"{dataset_name}_hist.png\")\n",
    "    plt.savefig(hist_path)\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Courbe temporelle\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(y_test, label=\"Valeurs réelles\", linewidth=2)\n",
    "    plt.plot(y_pred, '--', label=\"Prédictions\")\n",
    "    plt.title(f\"{dataset_name} - Évolution temporelle\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Valeur\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    curve_path = os.path.join(save_dir, f\"{dataset_name}_courbe.png\")\n",
    "    plt.savefig(curve_path)\n",
    "    plt.close()\n",
    "\n",
    "    return scatter_path, hist_path, curve_path\n",
    "###############################################################################\n",
    "# 4) Entraînement + évaluation (MAE, MSE, R²) + temps d'exécution\n",
    "###############################################################################\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_val, y_val,\n",
    "                             epochs=50, batch_size=512, verbose=0):\n",
    "    \"\"\"\n",
    "    Entraîne le modèle, mesure le temps d'entraînement, et renvoie l'historique.\n",
    "    \"\"\"\n",
    "    stop_early = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=verbose,\n",
    "                        callbacks=[stop_early])\n",
    "    training_time = time.time() - start_time\n",
    "    return history, training_time\n",
    "\n",
    "\n",
    "def inference_time_and_metrics(model, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Calcule le temps d'inférence, puis renvoie MAE, MSE, R².\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    preds = model.predict(x_test)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    preds = preds.reshape(-1)\n",
    "    y_test = y_test.reshape(-1)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    r2  = r2_score(y_test, preds)\n",
    "    \n",
    "    return mae, mse, r2, inference_time\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5) Fonction objectif pour PSO\n",
    "###############################################################################\n",
    "def make_objective_function(x_train, y_train, x_val, y_val, input_shape):\n",
    "    \"\"\"\n",
    "    Retourne la fonction objectif à passer à l'optimiseur PSO.\n",
    "    On entraînera rapidement (quelques époques) pour comparer.\n",
    "    \"\"\"\n",
    "    def objective_function(params):\n",
    "        \"\"\"\n",
    "        Les hyperparamètres sont dans l'ordre :\n",
    "          [lr, filters1, filters2, dense_units]\n",
    "        \"\"\"\n",
    "        n_particles = params.shape[0]\n",
    "        losses = np.zeros(n_particles)\n",
    "        \n",
    "        for i in range(n_particles):\n",
    "            lr         = params[i, 0]\n",
    "            filters1   = int(np.round(params[i, 1]))\n",
    "            filters2   = int(np.round(params[i, 2]))\n",
    "            dense_units= int(np.round(params[i, 3]))\n",
    "            \n",
    "            # Contrôles pour éviter 0 ou < 1\n",
    "            filters1    = max(filters1, 1)\n",
    "            filters2    = max(filters2, 1)\n",
    "            dense_units = max(dense_units, 1)\n",
    "            \n",
    "            # Construction du modèle\n",
    "            model = build_convlstm_model(lr, filters1, filters2, dense_units, input_shape)\n",
    "            \n",
    "            # Entraînement sur quelques époques pour l'évaluation PSO (rapide)\n",
    "            history = model.fit(x_train, y_train,\n",
    "                                validation_data=(x_val, y_val),\n",
    "                                epochs=5,   # Petit nombre d'époques pour PSO\n",
    "                                batch_size=256,\n",
    "                                verbose=0)\n",
    "            val_loss = history.history['val_loss'][-1]\n",
    "            losses[i] = val_loss\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    return objective_function\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 6) Boucle sur les datasets et compilation des résultats\n",
    "###############################################################################\n",
    "def run_experiments_on_datasets(\n",
    "    dataset_paths,\n",
    "    production_column='production',\n",
    "    window_size=24,\n",
    "    epochs_baseline=50,\n",
    "    epochs_optimized=50\n",
    "):\n",
    "    \"\"\"\n",
    "    - Pour chaque dataset :\n",
    "        1) Prépare les données\n",
    "        2) Entraîne le modèle baseline (e-base) et mesure ses métriques\n",
    "        3) Lance l'optimisation PSO\n",
    "        4) Entraîne le modèle avec les hyperparams optimisés\n",
    "        5) Mesure les métriques et temps\n",
    "        6) Stocke les résultats dans un DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for csv_path in dataset_paths:\n",
    "        dataset_name = os.path.basename(csv_path).replace('.csv','')\n",
    "        print(f\"\\n=== Dataset: {dataset_name} ===\")\n",
    "        \n",
    "        # 1) Chargement et préparation\n",
    "        x_train, y_train, x_test, y_test, x_val, y_val, df = load_and_prepare_data(\n",
    "            csv_path,\n",
    "            production_column=production_column,\n",
    "            window_size=window_size\n",
    "        )\n",
    "        input_shape = x_train.shape[1:]\n",
    "        \n",
    "        # 2) Modèle baseline\n",
    "        baseline_model = build_baseline_model(input_shape)\n",
    "        history_base, t_train_base = train_and_evaluate_model(\n",
    "            baseline_model, x_train, y_train, x_val, y_val,\n",
    "            epochs=epochs_baseline, batch_size=512, verbose=0\n",
    "        )\n",
    "        mae_base, mse_base, r2_base, t_infer_base = inference_time_and_metrics(baseline_model, x_test, y_test)\n",
    "        \n",
    "        # 3) PSO : définition des bornes et optimisation\n",
    "        #    [lr, filters1, filters2, dense_units]\n",
    "        lower_bounds = [1e-4, 32, 32, 32]\n",
    "        upper_bounds = [1e-2, 128,128,128]\n",
    "        bounds = (np.array(lower_bounds), np.array(upper_bounds))\n",
    "        \n",
    "        options = {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
    "        n_particles = 10\n",
    "        dimensions  = 4\n",
    "        \n",
    "        objective_fn = make_objective_function(x_train, y_train, x_val, y_val, input_shape)\n",
    "        optimizer = ps.single.GlobalBestPSO(n_particles=n_particles,\n",
    "                                            dimensions=dimensions,\n",
    "                                            options=options,\n",
    "                                            bounds=bounds)\n",
    "        \n",
    "        start_t_pso = time.time()\n",
    "        best_cost, best_pos = optimizer.optimize(objective_fn, iters=5)\n",
    "        pso_time = time.time() - start_t_pso\n",
    "        \n",
    "        # 4) Entraîner le modèle avec les hyperparamètres optimisés\n",
    "        lr_opt        = best_pos[0]\n",
    "        filters1_opt  = int(np.round(best_pos[1]))\n",
    "        filters2_opt  = int(np.round(best_pos[2]))\n",
    "        dense_opt     = int(np.round(best_pos[3]))\n",
    "        \n",
    "        best_model = build_convlstm_model(lr_opt, filters1_opt, filters2_opt, dense_opt, input_shape)\n",
    "        history_opt, t_train_opt = train_and_evaluate_model(\n",
    "            best_model, x_train, y_train, x_val, y_val,\n",
    "            epochs=epochs_optimized, batch_size=512, verbose=0\n",
    "        )\n",
    "        \n",
    "        # 5) Évaluation finale\n",
    "        mae_opt, mse_opt, r2_opt, t_infer_opt = inference_time_and_metrics(best_model, x_test, y_test)\n",
    "        \n",
    "        scatter_path, hist_path, curve_path = plot_and_save_analysis(\n",
    "            y_test=y_test, \n",
    "            y_pred=best_model.predict(x_test).flatten(), \n",
    "            save_dir=\"plots\", \n",
    "            dataset_name=dataset_name\n",
    "        )\n",
    "        \n",
    "        # 6) Stockage des résultats dans un dictionnaire\n",
    "        result_dict = {\n",
    "            \"Dataset\": dataset_name,\n",
    "            \n",
    "            # E-base\n",
    "            \"MAE e-base\": mae_base,\n",
    "            \"MSE e-base\": mse_base,\n",
    "            \"R2 e-base\":  r2_base,\n",
    "            \"T(entrainement-e-base)[s]\": t_train_base,\n",
    "            \n",
    "            # PSO\n",
    "            \"PSO best R2\": r2_opt,\n",
    "            \"PSO best cost (val_loss)\": best_cost,\n",
    "            \"Paramètres optimisés\": f\"lr={lr_opt:.5f}, f1={filters1_opt}, f2={filters2_opt}, dense={dense_opt}\",\n",
    "            \"T(PSO)[s]\": pso_time,\n",
    "            \n",
    "            # Entraînement optimisé\n",
    "            \"T(entrainement-optimisé)[s]\": t_train_opt,\n",
    "            \"MAE optimisé\": mae_opt,\n",
    "            \"MSE optimisé\": mse_opt,\n",
    "            \"R2 optimisé\": r2_opt,\n",
    "            \"Graph_scatter\": scatter_path,\n",
    "            \"Graph_hist\": hist_path,\n",
    "            \"Graph_courbe\": curve_path,\n",
    "            \n",
    "            # Inférence\n",
    "            \"T(evaluation-inference)[s]\": t_infer_opt,\n",
    "            \n",
    "            # Nombre d'époques\n",
    "            \"Nombre d'époques e-base\": epochs_baseline,\n",
    "            \"Nombre d'époques optimisé\": epochs_optimized\n",
    "        }\n",
    "        \n",
    "        results.append(result_dict)\n",
    "    \n",
    "    # Conversion en DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    return df_results\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 7) Lancement final\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # Liste des chemins vers vos CSV\n",
    "    dataset_paths = [\n",
    "        \"scaled_dataset.csv\",\n",
    "    ]\n",
    "    \n",
    "    # Paramètres globaux (à adapter)\n",
    "    production_column = 'production'\n",
    "    window_size = 24      # taille des fenêtres\n",
    "    epochs_baseline = 1  # nombre d'époques pour la baseline\n",
    "    epochs_optimized = 5 # nombre d'époques pour le modèle optimisé\n",
    "    \n",
    "    # Lancement des expériences\n",
    "    df_results = run_experiments_on_datasets(\n",
    "        dataset_paths,\n",
    "        production_column=production_column,\n",
    "        window_size=window_size,\n",
    "        epochs_baseline=epochs_baseline,\n",
    "        epochs_optimized=epochs_optimized\n",
    "    )\n",
    "    \n",
    "    # Affichage des résultats finaux\n",
    "    print(\"\\n========== RÉSULTATS FINAUX ==========\")\n",
    "    print(df_results)\n",
    "    # Sauvegarde éventuellement en CSV\n",
    "    df_results.to_csv(\"resume_resultats.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ca1a6e-dc44-4adc-8cd6-642f427ea415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IRPY (kernel)",
   "language": "python",
   "name": "py_envs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
