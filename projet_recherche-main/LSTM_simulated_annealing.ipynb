{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import logging\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./sampled_data.csv')\n",
    "data.head()\n",
    "\n",
    "data_train = data[data['building_id'] != 8]\n",
    "data_test = data[data['building_id'] == 8]\n",
    "\n",
    "data_train.shape, data_test.shape\n",
    "\n",
    "\n",
    "target_column = 'production'\n",
    "\n",
    "x_train = data_train.drop(target_column, axis=1)\n",
    "y_train = data_train[target_column].values.reshape(-1, 1)\n",
    "\n",
    "x_test = data_test.drop(target_column, axis=1)\n",
    "y_test = data_test[target_column].values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "x_scaler.fit(x_train)\n",
    "\n",
    "x_train_scaled = x_scaler.transform(x_train)\n",
    "x_test_scaled = x_scaler.transform(x_test)\n",
    "\n",
    "\n",
    "def get_windows(x, y, window_size):\n",
    "    x_windows, y_windows = [], []\n",
    "\n",
    "    for i in range(len(x) - window_size):\n",
    "        x_window = x[i:i+window_size]\n",
    "        y_window = y[i:i+window_size]\n",
    "\n",
    "        x_window = np.hstack((x_window, y_window))\n",
    "\n",
    "        x_windows.append(x_window)\n",
    "        y_windows.append(y[i+window_size])\n",
    "\n",
    "    return np.array(x_windows), np.array(y_windows)\n",
    "\n",
    "\n",
    "x_train_windows, y_train_windows = get_windows(x_train_scaled, y_train, 10)\n",
    "x_test_windows, y_test_windows = get_windows(x_test_scaled, y_test, 10)\n",
    "\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    print(f\"MSE: {mse}\")\n",
    "    return mse\n",
    "\n",
    "\n",
    "# Define the parameter ranges\n",
    "LSTM1_units = list(range(16, 256, 8))\n",
    "LSTM1_activation = ['tanh', 'relu', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "DROPOUT1_rate = list(np.arange(0, 0.9, 0.01))\n",
    "LSTM2_units = list(range(16, 256, 8))\n",
    "LSTM2_activation = ['tanh', 'relu', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "DROPOUT2_rate = list(np.arange(0, 0.9, 0.01))\n",
    "DENSE1_units = list(range(16, 256, 8))\n",
    "DENSE1_activation = ['tanh', 'relu', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "DENSE2_units = [1]\n",
    "DENSE2_activation = ['linear']\n",
    "OPTIMIZER_learning_rate = list(np.arange(0, 0.001, 0.0001))\n",
    "EPOCHS = list(range(200, 400, 10))\n",
    "BATCH_SIZE = list(range(32, 128, 8))\n",
    "\n",
    "\n",
    "def objective_function(params):\n",
    "    lstm1_units, lstm1_activation, dropout1_rate, lstm2_units, lstm2_activation, dropout2_rate, dense1_units, dense1_activation, dense2_units, dense2_activation, optimizer_learning_rate, epochs, batch_size = params\n",
    "    print(f\"lstm1_units: {lstm1_units}, activation: {\n",
    "          lstm1_activation}, dropout1_rate: {dropout1_rate}\")\n",
    "    print(f\"lstm2_units: {lstm2_units}, activation: {\n",
    "          lstm2_activation}, dropout2_rate: {dropout2_rate}\")\n",
    "    print(f\"dense1_units: {dense1_units}, activation: {dense1_activation}\")\n",
    "    print(f\"dense2_units: {dense2_units}, activation: {dense2_activation}\")\n",
    "    print(f\"optimizer_learning_rate: {optimizer_learning_rate}, epochs: {\n",
    "          epochs}, batch_size: {batch_size}\")\n",
    "\n",
    "    model = Sequential([\n",
    "        LSTM(lstm1_units, activation=lstm1_activation, input_shape=(\n",
    "            x_train_windows.shape[1:]), return_sequences=True),\n",
    "        Dropout(dropout1_rate),\n",
    "\n",
    "        LSTM(lstm2_units, activation=lstm2_activation, return_sequences=False),\n",
    "        Dropout(dropout2_rate),\n",
    "\n",
    "        Dense(dense1_units, activation=dense1_activation),\n",
    "        Dense(dense2_units, activation=dense2_activation)\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=optimizer_learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_absolute_error')\n",
    "\n",
    "    try:\n",
    "\n",
    "        start_time = time.time()\n",
    "        history = model.fit(x=x_train_windows,\n",
    "                            y=y_train_windows,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            validation_split=0.2,\n",
    "                            shuffle=False,\n",
    "                            verbose=0)\n",
    "        training_duration = time.time() - start_time\n",
    "    finally:\n",
    "        print(\"-------------------------------------------------------------------\")\n",
    "        # tracker.stop()\n",
    "\n",
    "    y_pred_train = model.predict(x_train_windows)\n",
    "    y_pred_test = model.predict(x_test_windows)\n",
    "    performance_metric = evaluate_model(y_test_windows, y_pred_test)\n",
    "\n",
    "    return performance_metric\n",
    "\n",
    "\n",
    "def simulated_annealing(initial_temp, final_temp, alpha, max_iterations):\n",
    "    # Initialize the default solution\n",
    "    current_solution = [\n",
    "        128, \"tanh\", 0.2,\n",
    "        64, \"tanh\", 0.2,\n",
    "        64, \"relu\", 1, \"linear\",\n",
    "        0.001, 300, 128\n",
    "    ]\n",
    "    current_mse = objective_function(current_solution)\n",
    "    best_solution = current_solution\n",
    "    best_mse = current_mse\n",
    "    temp = initial_temp\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        if temp < final_temp:\n",
    "            break\n",
    "\n",
    "        # Generate a neighboring solution\n",
    "        neighbor = list(current_solution)\n",
    "        param_index = random.randint(0, len(current_solution) - 1)\n",
    "        if param_index == 0:\n",
    "            neighbor[param_index] = random.choice(LSTM1_units)\n",
    "        elif param_index == 1:\n",
    "            neighbor[param_index] = random.choice(LSTM1_activation)\n",
    "        elif param_index == 2:\n",
    "            neighbor[param_index] = random.choice(DROPOUT1_rate)\n",
    "        elif param_index == 3:\n",
    "            neighbor[param_index] = random.choice(LSTM2_units)\n",
    "        elif param_index == 4:\n",
    "            neighbor[param_index] = random.choice(LSTM2_activation)\n",
    "        elif param_index == 5:\n",
    "            neighbor[param_index] = random.choice(DROPOUT2_rate)\n",
    "        elif param_index == 6:\n",
    "            neighbor[param_index] = random.choice(DENSE1_units)\n",
    "        elif param_index == 7:\n",
    "            neighbor[param_index] = random.choice(DENSE1_activation)\n",
    "        elif param_index == 8:\n",
    "            neighbor[param_index] = random.choice(OPTIMIZER_learning_rate)\n",
    "        elif param_index == 9:\n",
    "            neighbor[param_index] = random.choice(EPOCHS)\n",
    "        elif param_index == 10:\n",
    "            neighbor[param_index] = random.choice(BATCH_SIZE)\n",
    "        # params Dense2 units and activation should not be changed, as they are the output layer\n",
    "\n",
    "        neighbor_mse = objective_function(neighbor)\n",
    "\n",
    "        # Acceptance criterion\n",
    "        mse_diff = neighbor_mse - current_mse\n",
    "        print(f\"current_solution {current_solution}\")\n",
    "        print(f\"neighbor {neighbor}\")\n",
    "        print(f\"mse diff: {mse_diff}\")\n",
    "        if mse_diff < 0 or random.random() < math.exp(-mse_diff / temp):\n",
    "            print(\"accept new solution\")\n",
    "            current_solution = neighbor\n",
    "            current_mse = neighbor_mse\n",
    "\n",
    "            if neighbor_mse < best_mse:\n",
    "                print(\"better solution\")\n",
    "                best_solution = neighbor\n",
    "                best_mse = neighbor_mse\n",
    "\n",
    "        # Update temperature\n",
    "        temp = temp * alpha\n",
    "        print(f\"temp {temp}, mse {current_mse}, best_mse {best_mse}\")\n",
    "\n",
    "    return best_solution, best_mse\n",
    "\n",
    "\n",
    "# Run the simulated annealing algorithm\n",
    "initial_temp = 20  # Initial temperature\n",
    "final_temp = 0.2  # Final temperature\n",
    "alpha = 0.99  # Temperature decay rate\n",
    "max_iterations = 10000  # Maximum number of iterations\n",
    "\n",
    "best_params, best_mse = simulated_annealing(\n",
    "    initial_temp, final_temp, alpha, max_iterations)\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best mse:\", best_mse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
