{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   production   latitude  longitude   vmp  imp   voc   isc  p_per_m2  p_max  \\\n",
      "0         0.8  48.575437   7.768668  27.3  7.7  33.3  8.17     143.0    210   \n",
      "1        16.9  48.575437   7.768668  27.3  7.7  33.3  8.17     143.0    210   \n",
      "2         1.4  48.575437   7.768668  27.3  7.7  33.3  8.17     143.0    210   \n",
      "3         6.6  48.575437   7.768668  27.3  7.7  33.3  8.17     143.0    210   \n",
      "4         0.3  48.575437   7.768668  27.3  7.7  33.3  8.17     143.0    210   \n",
      "\n",
      "   panel_area  ...  wind_speed_10m_std  wind_speed_10m_min  wind_speed_10m_q1  \\\n",
      "0        1.72  ...            0.903114                 1.3              1.900   \n",
      "1        1.72  ...            0.969186                 0.9              2.975   \n",
      "2        1.72  ...            0.306945                 0.4              1.100   \n",
      "3        1.72  ...            0.491844                 0.4              0.800   \n",
      "4        1.72  ...            0.527737                 1.6              2.275   \n",
      "\n",
      "   wind_speed_10m_q2  wind_speed_10m_q3  wind_speed_10m_max  building_id  day  \\\n",
      "0               3.00              3.450                 4.1            1    1   \n",
      "1               3.50              3.825                 4.8            1    2   \n",
      "2               1.20              1.400                 1.9            1    3   \n",
      "3               1.20              1.725                 2.1            1    4   \n",
      "4               2.85              3.100                 3.4            1    5   \n",
      "\n",
      "   month  year  \n",
      "0      1  2019  \n",
      "1      1  2019  \n",
      "2      1  2019  \n",
      "3      1  2019  \n",
      "4      1  2019  \n",
      "\n",
      "[5 rows x 164 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lire le fichier CSV principal\n",
    "df = pd.read_csv('./full_dataset.csv')\n",
    "df = df.drop(\"date\", axis=1)\n",
    "\n",
    "# Identifier les coordonnées uniques (latitude, longitude)\n",
    "coordinates = df[['latitude', 'longitude']].drop_duplicates()\n",
    "\n",
    "# Liste pour stocker les dataframes échantillonnés\n",
    "sampled_dfs = []\n",
    "\n",
    "# Filtrer les données pour chaque bâtiment en fonction des coordonnées\n",
    "for index, row in coordinates.iterrows():\n",
    "    lat, long = row['latitude'], row['longitude']\n",
    "    building_data = df[(df['latitude'] == lat) & (df['longitude'] == long)]\n",
    "    \n",
    "    # Calculer le nombre de lignes correspondant à 20% du dataframe\n",
    "    num_rows = int(len(building_data) * 0.1)\n",
    "    \n",
    "    # Prendre les premières 20% des lignes\n",
    "    sampled_df = building_data.head(num_rows)\n",
    "    \n",
    "    # Ajouter le dataframe échantillonné à la liste\n",
    "    sampled_dfs.append(sampled_df)\n",
    "\n",
    "# Fusionner tous les dataframes échantillonnés en un seul dataframe\n",
    "data = pd.concat(sampled_dfs, ignore_index=True)\n",
    "\n",
    "# Afficher un aperçu du dataframe final\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((959, 164), (125, 164))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = data[data['building_id'] != 8]\n",
    "data_test = data[data['building_id'] == 8]\n",
    "\n",
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "target_column = 'production'\n",
    "\n",
    "x_train = data_train.drop(target_column, axis=1)\n",
    "y_train = data_train[target_column].values.reshape(-1, 1)\n",
    "\n",
    "x_test = data_test.drop(target_column, axis=1)\n",
    "y_test = data_test[target_column].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "x_scaler.fit(x_train)\n",
    "\n",
    "x_train_scaled = x_scaler.transform(x_train)\n",
    "x_test_scaled = x_scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def get_windows(x, y, window_size):\n",
    "    x_windows, y_windows = [], []\n",
    "\n",
    "    for i in range(len(x) - window_size):\n",
    "        x_window = x[i:i+window_size]\n",
    "        y_window = y[i:i+window_size]\n",
    "\n",
    "        x_window = np.hstack((x_window, y_window))\n",
    "\n",
    "        x_windows.append(x_window)\n",
    "        y_windows.append(y[i+window_size])\n",
    "\n",
    "    return np.array(x_windows), np.array(y_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "x_train_windows, y_train_windows = get_windows(x_train_scaled, y_train, 10)\n",
    "x_test_windows, y_test_windows = get_windows(x_test_scaled, y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found, using CPU instead.\n",
      "Training with parameters: {'LSTM1_units': 192, 'LSTM1_activation': 'tanh', 'DROPOUT1_rate': 0.2, 'LSTM2_units': 160, 'LSTM2_activation': 'tanh', 'DROPOUT2_rate': 0.2, 'learning_rate': 0.001} (1/8)\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 9s 323ms/step - loss: 47.4991 - val_loss: 19.0171\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 44.2589 - val_loss: 16.0718\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 42.0940 - val_loss: 14.7194\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 40.7094 - val_loss: 14.8105\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 39.4311 - val_loss: 13.5725\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 37.5627 - val_loss: 13.2717\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 36.1205 - val_loss: 12.0991\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 34.5600 - val_loss: 12.3701\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 33.2150 - val_loss: 11.0584\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 32.0434 - val_loss: 10.5007\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 30.8388 - val_loss: 10.0994\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 29.9261 - val_loss: 9.5425\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 29.0050 - val_loss: 9.6382\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 28.2944 - val_loss: 9.5193\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 27.4428 - val_loss: 9.5811\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 26.7891 - val_loss: 9.3058\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 26.8098 - val_loss: 11.0866\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 26.5224 - val_loss: 9.0668\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 25.5040 - val_loss: 9.3183\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 24.9657 - val_loss: 8.4696\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 24.1565 - val_loss: 8.6827\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 23.5469 - val_loss: 8.1068\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 22.9462 - val_loss: 8.0988\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 22.6306 - val_loss: 7.7108\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 22.3283 - val_loss: 8.3217\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 22.3961 - val_loss: 8.8011\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 21.7534 - val_loss: 9.0973\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 21.1273 - val_loss: 7.9644\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 20.2938 - val_loss: 7.4535\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 19.5657 - val_loss: 7.2125\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 19.2752 - val_loss: 7.7174\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 18.7404 - val_loss: 7.0039\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 18.0822 - val_loss: 7.0300\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 17.9033 - val_loss: 6.7772\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 17.4842 - val_loss: 6.4065\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 17.1970 - val_loss: 6.7929\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 17.4505 - val_loss: 7.5455\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 16.8837 - val_loss: 7.4881\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 16.0790 - val_loss: 6.2116\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 15.7611 - val_loss: 6.3343\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 15.4344 - val_loss: 6.4080\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 15.5268 - val_loss: 6.5110\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 15.0628 - val_loss: 6.3952\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 14.6413 - val_loss: 6.0169\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 14.5548 - val_loss: 5.8864\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 14.1913 - val_loss: 5.6267\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 14.1877 - val_loss: 6.2842\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 14.1620 - val_loss: 6.0961\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 14.0158 - val_loss: 5.9183\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 14.1898 - val_loss: 5.2983\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 13.3326 - val_loss: 5.6204\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 13.9225 - val_loss: 5.8952\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 13.7860 - val_loss: 5.5230\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 13.2956 - val_loss: 5.3978\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 13.4897 - val_loss: 5.2927\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 12.9409 - val_loss: 5.6533\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 13.0963 - val_loss: 5.1319\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 12.9736 - val_loss: 5.1362\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 12.9034 - val_loss: 4.9053\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 12.6985 - val_loss: 4.9038\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 12.6134 - val_loss: 4.7969\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 12.5187 - val_loss: 4.7397\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 12.2974 - val_loss: 4.7703\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 12.1297 - val_loss: 4.4413\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 12.2261 - val_loss: 4.7219\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 12.1439 - val_loss: 4.7137\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 12.4996 - val_loss: 4.7073\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 12.2649 - val_loss: 4.4096\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 12.2209 - val_loss: 4.5820\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 11.7675 - val_loss: 4.8497\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 11.9950 - val_loss: 4.7572\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 11.8419 - val_loss: 4.4480\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 86ms/step - loss: 11.5060 - val_loss: 4.1866\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 11.8130 - val_loss: 4.5942\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 11.5075 - val_loss: 4.2295\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 11.4011 - val_loss: 4.3320\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 11.3951 - val_loss: 4.5654\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 11.4371 - val_loss: 4.6524\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 11.3564 - val_loss: 4.4054\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 11.2866 - val_loss: 4.4472\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 11.6205 - val_loss: 4.8308\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 11.4934 - val_loss: 4.6565\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 11.3130 - val_loss: 5.1980\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 11.7371 - val_loss: 4.7436\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 11.2971 - val_loss: 4.8128\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 11.2656 - val_loss: 4.4186\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 11.0522 - val_loss: 4.3169\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 10.8693 - val_loss: 3.9878\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 10.7432 - val_loss: 4.1760\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 10.7314 - val_loss: 4.6581\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 10.8102 - val_loss: 4.1780\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 10.6546 - val_loss: 4.0138\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 10.7352 - val_loss: 4.5307\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 11.4326 - val_loss: 4.1546\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 11.1898 - val_loss: 4.3868\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 11.1576 - val_loss: 4.3334\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 10.8015 - val_loss: 4.3657\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 10.6743 - val_loss: 4.7945\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 10.6944 - val_loss: 4.3372\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 10.7607 - val_loss: 4.3223\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 10.5045 - val_loss: 4.3516\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 10.8137 - val_loss: 4.1726\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 10.7480 - val_loss: 4.2722\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 10.4250 - val_loss: 4.1600\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 10.5659 - val_loss: 4.7096\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 10.1977 - val_loss: 4.2351\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 10.0814 - val_loss: 4.1476\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 10.4590 - val_loss: 4.2368\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 10.5816 - val_loss: 4.1492\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 10.3949 - val_loss: 4.3002\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 10.0910 - val_loss: 4.1461\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 10.0388 - val_loss: 3.9386\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 9.9720 - val_loss: 3.8439\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 10.2264 - val_loss: 4.0013\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 10.0734 - val_loss: 4.5776\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.9744 - val_loss: 4.2922\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 10.2506 - val_loss: 4.3624\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 10.0776 - val_loss: 4.2360\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 9.9305 - val_loss: 4.1263\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 9.8835 - val_loss: 4.2851\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.8389 - val_loss: 4.0078\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 9.8575 - val_loss: 4.0995\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 9.7189 - val_loss: 3.8455\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.5701 - val_loss: 3.8553\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 9.8643 - val_loss: 4.2059\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 9.8460 - val_loss: 3.7018\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 10.2387 - val_loss: 4.3721\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 9.8330 - val_loss: 4.1831\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.8457 - val_loss: 3.9580\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 9.7266 - val_loss: 4.2218\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 9.6989 - val_loss: 4.0045\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 9.6818 - val_loss: 4.3753\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 9.6089 - val_loss: 3.8629\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 86ms/step - loss: 9.0852 - val_loss: 4.2548\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 9.4016 - val_loss: 3.8837\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 9.5458 - val_loss: 3.9758\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 9.6365 - val_loss: 3.7950\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 9.3273 - val_loss: 4.1039\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 9.5175 - val_loss: 4.3170\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 9.5865 - val_loss: 4.7074\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 9.5694 - val_loss: 4.2207\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.4368 - val_loss: 4.2001\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 9.6370 - val_loss: 4.1471\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.5962 - val_loss: 4.2792\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 9.1560 - val_loss: 4.7756\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 1s 76ms/step - loss: 9.0597 - val_loss: 4.1632\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 9.1136 - val_loss: 4.1703\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 8.9734 - val_loss: 4.0931\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.2504 - val_loss: 4.3282\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 9.8131 - val_loss: 4.5025\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 9.0433 - val_loss: 4.5283\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 86ms/step - loss: 9.0673 - val_loss: 4.3792\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 8.7640 - val_loss: 4.0844\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 8.6351 - val_loss: 3.9481\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 8.3959 - val_loss: 4.1286\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 9.1116 - val_loss: 3.8489\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 8.5398 - val_loss: 4.0198\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 9.1788 - val_loss: 4.0540\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 9.3201 - val_loss: 4.5358\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.8921 - val_loss: 4.0958\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 8.7998 - val_loss: 4.5305\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 8.8642 - val_loss: 4.4934\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.5579 - val_loss: 4.1030\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 8.4336 - val_loss: 3.9461\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 8.5108 - val_loss: 3.9533\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.2537 - val_loss: 3.8536\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 8.0436 - val_loss: 3.8100\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 8.2677 - val_loss: 3.9599\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.3460 - val_loss: 3.8577\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.2430 - val_loss: 4.2830\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 8.1377 - val_loss: 3.8634\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.9000 - val_loss: 4.0312\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 8.3093 - val_loss: 3.8014\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.2474 - val_loss: 4.2196\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.3338 - val_loss: 3.8951\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.0865 - val_loss: 3.9998\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 7.6455 - val_loss: 3.7457\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.0543 - val_loss: 4.0154\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.9487 - val_loss: 4.1330\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.6312 - val_loss: 3.9689\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 7.5980 - val_loss: 4.1040\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.9795 - val_loss: 4.0814\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 8.2023 - val_loss: 4.4773\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 8.0208 - val_loss: 4.0961\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.9927 - val_loss: 4.5669\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 7.4293 - val_loss: 4.1215\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 7.4448 - val_loss: 4.1834\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.4636 - val_loss: 3.8357\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.1838 - val_loss: 3.9501\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 7.1305 - val_loss: 4.0023\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.1459 - val_loss: 3.9018\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.4392 - val_loss: 4.0363\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 7.8855 - val_loss: 4.1606\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 7.7072 - val_loss: 4.0207\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 7.4867 - val_loss: 4.1305\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 7.3383 - val_loss: 4.1307\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 7.3934 - val_loss: 3.8486\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 7.4916 - val_loss: 4.0157\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 6.8088 - val_loss: 3.9363\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.2549 - val_loss: 3.9193\n",
      "Loss: 6.808807849884033\n",
      "Validation loss: 3.7018322944641113\n",
      "Training duration: 93.13092851638794 seconds\n",
      "New best model found: {'LSTM1_units': 192, 'LSTM1_activation': 'tanh', 'DROPOUT1_rate': 0.2, 'LSTM2_units': 160, 'LSTM2_activation': 'tanh', 'DROPOUT2_rate': 0.2, 'learning_rate': 0.001} with validation loss: 3.7018322944641113\n",
      "Training with parameters: {'LSTM1_units': 192, 'LSTM1_activation': 'tanh', 'DROPOUT1_rate': 0.2, 'LSTM2_units': 208, 'LSTM2_activation': 'sigmoid', 'DROPOUT2_rate': 0.1, 'learning_rate': 0.01} (2/8)\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 5s 218ms/step - loss: 41.1779 - val_loss: 12.4253\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 33.5582 - val_loss: 21.5748\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 31.5146 - val_loss: 10.8385\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 27.9282 - val_loss: 12.0611\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 25.1388 - val_loss: 11.0999\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 87ms/step - loss: 21.9485 - val_loss: 11.4910\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 1s 82ms/step - loss: 19.8596 - val_loss: 12.2912\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 18.4014 - val_loss: 11.9349\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 18.3615 - val_loss: 10.2034\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 18.6041 - val_loss: 11.6608\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 1s 174ms/step - loss: 17.9822 - val_loss: 10.3730\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 17.5321 - val_loss: 9.8960\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 16.8168 - val_loss: 8.9638\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 16.2363 - val_loss: 9.8025\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 1s 127ms/step - loss: 16.1177 - val_loss: 9.0076\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 16.1918 - val_loss: 8.3427\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 17.7274 - val_loss: 9.1576\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 16.5611 - val_loss: 9.3786\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 16.7660 - val_loss: 8.3601\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 86ms/step - loss: 15.7120 - val_loss: 8.9534\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 15.8820 - val_loss: 8.6554\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 16.0387 - val_loss: 8.3415\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 15.2923 - val_loss: 8.3372\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 15.4548 - val_loss: 8.2756\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 15.6973 - val_loss: 7.7928\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 14.8440 - val_loss: 7.5653\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 14.2355 - val_loss: 7.7291\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 14.6879 - val_loss: 7.6243\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 14.7579 - val_loss: 7.3005\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 14.3538 - val_loss: 7.1832\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 14.0999 - val_loss: 7.8500\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 14.5141 - val_loss: 8.2467\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 15.2973 - val_loss: 7.5364\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 15.1059 - val_loss: 8.5540\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 15.2455 - val_loss: 7.8454\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 14.0628 - val_loss: 7.2568\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 84ms/step - loss: 13.9749 - val_loss: 6.7289\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 84ms/step - loss: 13.6639 - val_loss: 7.1172\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 13.5376 - val_loss: 7.0922\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 13.3387 - val_loss: 6.4335\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 13.1460 - val_loss: 5.9537\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 13.8299 - val_loss: 7.1060\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 16.0941 - val_loss: 6.1969\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 13.4497 - val_loss: 5.8106\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 13.6851 - val_loss: 6.8719\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 13.6582 - val_loss: 5.5802\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 13.0756 - val_loss: 5.5328\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 13.0956 - val_loss: 5.7308\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 13.2951 - val_loss: 5.5827\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 12.4722 - val_loss: 6.0387\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 12.5483 - val_loss: 5.9810\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 12.5014 - val_loss: 5.3552\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 12.2876 - val_loss: 5.6914\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 12.0416 - val_loss: 5.2206\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 12.4589 - val_loss: 5.0357\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 13.8009 - val_loss: 6.8909\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 13.6744 - val_loss: 5.7799\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 12.8578 - val_loss: 5.9240\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 12.8518 - val_loss: 5.6099\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 12.1144 - val_loss: 5.8576\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 12.6289 - val_loss: 6.3268\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 12.1460 - val_loss: 6.1370\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 11.9296 - val_loss: 5.4936\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 11.7282 - val_loss: 5.7942\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 11.8802 - val_loss: 5.2570\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 11.6577 - val_loss: 5.1551\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 11.8500 - val_loss: 6.0169\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 12.0001 - val_loss: 5.4783\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 12.1526 - val_loss: 5.3630\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 12.0718 - val_loss: 4.9876\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 11.8472 - val_loss: 5.3934\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 12.0080 - val_loss: 5.7046\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 11.6290 - val_loss: 5.7005\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 11.5155 - val_loss: 5.1857\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 11.7255 - val_loss: 5.7986\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 11.9100 - val_loss: 5.4870\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 11.4465 - val_loss: 5.1677\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 11.5051 - val_loss: 5.1239\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 11.4988 - val_loss: 5.2009\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 11.3985 - val_loss: 4.8872\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 85ms/step - loss: 11.6233 - val_loss: 5.4440\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 12.5785 - val_loss: 5.5247\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 11.6699 - val_loss: 5.4109\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 11.2007 - val_loss: 5.5548\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 84ms/step - loss: 11.4817 - val_loss: 5.0212\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 11.3112 - val_loss: 5.1802\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 11.0741 - val_loss: 5.1550\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 11.6474 - val_loss: 5.0353\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 86ms/step - loss: 11.6863 - val_loss: 5.3727\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 11.9134 - val_loss: 5.0988\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 11.2624 - val_loss: 5.0953\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 10.9557 - val_loss: 4.9832\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 11.2081 - val_loss: 4.8876\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 11.0681 - val_loss: 4.8709\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 84ms/step - loss: 11.4007 - val_loss: 4.9714\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 84ms/step - loss: 11.2856 - val_loss: 4.6786\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 11.6570 - val_loss: 5.8621\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 11.9343 - val_loss: 5.6259\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 11.5281 - val_loss: 5.2180\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 11.0074 - val_loss: 5.3056\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 11.0993 - val_loss: 5.0679\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 11.0900 - val_loss: 4.7097\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 11.1727 - val_loss: 4.8288\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 11.1116 - val_loss: 4.6913\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 84ms/step - loss: 10.7604 - val_loss: 4.9452\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 11.2813 - val_loss: 4.6420\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 10.9714 - val_loss: 4.9984\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 10.5544 - val_loss: 4.9290\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 11.1782 - val_loss: 4.8578\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 10.8990 - val_loss: 4.9930\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 10.9771 - val_loss: 4.8791\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 10.9524 - val_loss: 4.5892\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 10.5653 - val_loss: 4.3064\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 10.7258 - val_loss: 4.6330\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 10.6320 - val_loss: 4.3474\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 10.6436 - val_loss: 4.7445\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 10.1423 - val_loss: 4.6584\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 10.2541 - val_loss: 4.4912\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 10.7732 - val_loss: 4.6253\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 10.8504 - val_loss: 5.1300\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 10.5783 - val_loss: 4.1518\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 10.2154 - val_loss: 5.0505\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 10.3975 - val_loss: 4.8370\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 10.5844 - val_loss: 4.2456\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 10.6276 - val_loss: 4.3398\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 10.2553 - val_loss: 4.1956\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 10.7914 - val_loss: 4.9154\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 10.9054 - val_loss: 4.8043\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 10.2335 - val_loss: 4.7526\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 10.2280 - val_loss: 4.3069\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 10.6320 - val_loss: 4.6248\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 11.3285 - val_loss: 4.4967\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 85ms/step - loss: 10.7765 - val_loss: 4.4277\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 10.8550 - val_loss: 4.4212\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 11.2603 - val_loss: 4.1611\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 10.5480 - val_loss: 4.2753\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 10.4389 - val_loss: 4.6045\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 10.3765 - val_loss: 3.9276\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 10.3176 - val_loss: 4.5947\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 9.8735 - val_loss: 4.3537\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 9.9431 - val_loss: 4.0838\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 9.9802 - val_loss: 4.1961\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 9.6897 - val_loss: 4.4837\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 85ms/step - loss: 10.9420 - val_loss: 5.3599\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 11.0627 - val_loss: 5.0319\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 10.8499 - val_loss: 6.1194\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 10.6999 - val_loss: 4.4793\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 10.3989 - val_loss: 4.5267\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 84ms/step - loss: 10.5805 - val_loss: 4.8434\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 10.5676 - val_loss: 4.3167\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 10.1229 - val_loss: 4.2152\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 9.9760 - val_loss: 3.8549\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 10.6187 - val_loss: 3.9058\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 84ms/step - loss: 11.2451 - val_loss: 4.7395\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 10.1029 - val_loss: 4.1820\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 10.4382 - val_loss: 4.5294\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 10.3141 - val_loss: 3.9197\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 9.8517 - val_loss: 4.1704\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 9.9237 - val_loss: 3.6873\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 9.6313 - val_loss: 3.9913\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 9.9228 - val_loss: 3.8730\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 10.6481 - val_loss: 3.9405\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 10.5221 - val_loss: 4.2848\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 85ms/step - loss: 10.0230 - val_loss: 3.9137\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 9.6314 - val_loss: 4.2503\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 10.0507 - val_loss: 3.8831\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 9.8675 - val_loss: 4.0562\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 9.5768 - val_loss: 4.6742\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 9.8034 - val_loss: 4.3018\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 9.7670 - val_loss: 4.4268\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 9.7342 - val_loss: 3.9249\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 10.3746 - val_loss: 4.2960\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 86ms/step - loss: 9.6155 - val_loss: 4.2561\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 9.1565 - val_loss: 4.0303\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 9.2539 - val_loss: 4.1176\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 9.2698 - val_loss: 4.2639\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 86ms/step - loss: 9.2806 - val_loss: 4.2768\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 9.3975 - val_loss: 4.0471\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 9.9458 - val_loss: 3.9568\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 10.6002 - val_loss: 4.6414\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 9.8146 - val_loss: 4.3886\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 9.5715 - val_loss: 4.0479\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 86ms/step - loss: 9.6797 - val_loss: 3.7726\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 9.1754 - val_loss: 3.7714\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 9.1249 - val_loss: 4.1231\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 9.2442 - val_loss: 3.9226\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 85ms/step - loss: 9.0991 - val_loss: 3.7142\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 9.4861 - val_loss: 4.1523\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 9.5118 - val_loss: 4.1833\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 9.5904 - val_loss: 4.3849\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 9.3483 - val_loss: 4.3312\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 9.2905 - val_loss: 3.7859\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 9.0528 - val_loss: 4.0246\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 9.2325 - val_loss: 3.8763\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 9.0562 - val_loss: 4.1646\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 9.5436 - val_loss: 4.0753\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 9.4121 - val_loss: 3.9178\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 8.9870 - val_loss: 4.0502\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 9.1431 - val_loss: 3.8012\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 8.8023 - val_loss: 4.0367\n",
      "Loss: 8.802302360534668\n",
      "Validation loss: 3.6872658729553223\n",
      "Training duration: 106.61348605155945 seconds\n",
      "New best model found: {'LSTM1_units': 192, 'LSTM1_activation': 'tanh', 'DROPOUT1_rate': 0.2, 'LSTM2_units': 208, 'LSTM2_activation': 'sigmoid', 'DROPOUT2_rate': 0.1, 'learning_rate': 0.01} with validation loss: 3.6872658729553223\n",
      "Training with parameters: {'LSTM1_units': 192, 'LSTM1_activation': 'sigmoid', 'DROPOUT1_rate': 0.1, 'LSTM2_units': 160, 'LSTM2_activation': 'tanh', 'DROPOUT2_rate': 0.1, 'learning_rate': 0.01} (3/8)\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 5s 211ms/step - loss: 42.7756 - val_loss: 16.1242\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 39.8671 - val_loss: 19.0957\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 40.6816 - val_loss: 18.5519\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 40.6204 - val_loss: 17.2700\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 40.3559 - val_loss: 16.6098\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 40.3103 - val_loss: 16.8189\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 40.0999 - val_loss: 17.2830\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 86ms/step - loss: 40.1710 - val_loss: 17.5612\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 40.2502 - val_loss: 17.4499\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 40.3056 - val_loss: 17.1718\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 40.0893 - val_loss: 17.1094\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 88ms/step - loss: 39.9920 - val_loss: 17.1403\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 39.9110 - val_loss: 16.9851\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 87ms/step - loss: 39.6819 - val_loss: 16.7140\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 39.3809 - val_loss: 16.4504\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 39.0827 - val_loss: 16.4797\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 38.6786 - val_loss: 15.8910\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 38.1989 - val_loss: 15.7706\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 37.6165 - val_loss: 15.1292\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 36.9031 - val_loss: 14.9458\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 36.2891 - val_loss: 14.5646\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 35.8224 - val_loss: 13.7092\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 34.9185 - val_loss: 12.9101\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 34.8698 - val_loss: 12.7641\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 34.1063 - val_loss: 11.1176\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 33.1597 - val_loss: 10.9413\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 32.3837 - val_loss: 11.3422\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 31.9164 - val_loss: 10.6326\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 31.9465 - val_loss: 10.5956\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 84ms/step - loss: 31.8400 - val_loss: 9.7760\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 30.3362 - val_loss: 9.8861\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 29.7695 - val_loss: 10.1926\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 28.9875 - val_loss: 12.0631\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 28.8908 - val_loss: 10.8585\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 28.4852 - val_loss: 10.4488\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 28.2834 - val_loss: 10.4298\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 27.4561 - val_loss: 10.6215\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 26.7186 - val_loss: 9.7827\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 84ms/step - loss: 26.3661 - val_loss: 10.0107\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 26.3378 - val_loss: 10.7234\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 26.5040 - val_loss: 10.0130\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 25.6848 - val_loss: 10.9101\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 24.1772 - val_loss: 10.8286\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 24.0659 - val_loss: 10.6534\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 23.5776 - val_loss: 11.7323\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 23.0095 - val_loss: 10.6860\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 22.5944 - val_loss: 14.4978\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 22.9455 - val_loss: 10.4728\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 23.3895 - val_loss: 14.0092\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 22.8351 - val_loss: 11.3980\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 22.4023 - val_loss: 10.7875\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 23.0565 - val_loss: 10.0435\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 21.5988 - val_loss: 12.6578\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 21.9490 - val_loss: 10.0877\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 21.0308 - val_loss: 14.5036\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 21.7515 - val_loss: 9.9895\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 85ms/step - loss: 21.0798 - val_loss: 10.0282\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 20.3784 - val_loss: 9.3452\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 20.1201 - val_loss: 10.5854\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 19.5925 - val_loss: 9.5305\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 20.4975 - val_loss: 10.4669\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 19.6219 - val_loss: 9.3941\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 19.5971 - val_loss: 11.0962\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 19.4375 - val_loss: 10.0768\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 20.0607 - val_loss: 9.9452\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 19.3965 - val_loss: 9.4484\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 19.9249 - val_loss: 10.4941\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 19.4766 - val_loss: 11.4914\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 18.9888 - val_loss: 13.1999\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 18.7612 - val_loss: 9.4800\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 18.3142 - val_loss: 10.1715\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 18.9546 - val_loss: 10.2494\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 18.3135 - val_loss: 9.9051\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 18.9832 - val_loss: 9.7161\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 18.4660 - val_loss: 10.2090\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 18.3385 - val_loss: 9.6351\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 87ms/step - loss: 19.0011 - val_loss: 9.4420\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 19.0564 - val_loss: 10.5931\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 18.3633 - val_loss: 12.1298\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 17.4041 - val_loss: 10.2143\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 1s 97ms/step - loss: 19.0540 - val_loss: 9.2918\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 18.3540 - val_loss: 9.4115\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 18.2575 - val_loss: 10.0844\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 18.5523 - val_loss: 10.2697\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 18.2053 - val_loss: 9.6051\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 86ms/step - loss: 17.8853 - val_loss: 9.4188\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 1s 79ms/step - loss: 17.4494 - val_loss: 9.7552\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 18.1598 - val_loss: 10.2984\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 18.3186 - val_loss: 9.2524\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 85ms/step - loss: 17.6399 - val_loss: 9.2595\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 17.2643 - val_loss: 9.4727\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 17.5200 - val_loss: 9.3412\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 18.0851 - val_loss: 8.7983\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 18.6051 - val_loss: 9.6297\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 18.7960 - val_loss: 10.0206\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 1s 97ms/step - loss: 18.1145 - val_loss: 10.5789\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 17.3915 - val_loss: 10.2655\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 17.5410 - val_loss: 9.9132\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 16.9725 - val_loss: 8.9194\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 16.8986 - val_loss: 9.5178\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 17.3297 - val_loss: 9.6315\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 18.5311 - val_loss: 9.2493\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 17.4876 - val_loss: 9.6339\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 17.2959 - val_loss: 8.0119\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 19.2487 - val_loss: 9.9284\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 17.9022 - val_loss: 10.1994\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 16.9614 - val_loss: 9.1252\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 17.2228 - val_loss: 8.4820\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 17.6067 - val_loss: 9.7260\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 19.2384 - val_loss: 9.5797\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 17.4202 - val_loss: 10.6830\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 17.6772 - val_loss: 8.8405\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 17.9608 - val_loss: 10.3492\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 18.4227 - val_loss: 9.9392\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 17.6688 - val_loss: 10.0976\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 17.5235 - val_loss: 9.3067\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 17.4759 - val_loss: 9.3478\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 17.2710 - val_loss: 9.3375\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 16.6912 - val_loss: 8.4197\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 16.7365 - val_loss: 8.5463\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 17.0795 - val_loss: 9.0212\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 17.5199 - val_loss: 8.1308\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 18.2496 - val_loss: 8.4693\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 17.2344 - val_loss: 8.7013\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 17.3243 - val_loss: 8.2644\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 17.6802 - val_loss: 9.1811\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 84ms/step - loss: 17.5196 - val_loss: 9.1362\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 16.8458 - val_loss: 10.1864\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 17.1709 - val_loss: 7.4428\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 16.8727 - val_loss: 9.3678\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 18.0718 - val_loss: 9.2623\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 16.0491 - val_loss: 8.5181\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 16.5614 - val_loss: 7.7787\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 17.6179 - val_loss: 8.5303\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 16.5864 - val_loss: 8.4568\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 16.7223 - val_loss: 8.1952\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 17.2717 - val_loss: 8.2882\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 16.9225 - val_loss: 8.7497\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 16.1815 - val_loss: 8.3480\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 16.9039 - val_loss: 9.9071\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 17.7264 - val_loss: 6.8754\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 18.1013 - val_loss: 8.2168\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 18.6188 - val_loss: 10.4172\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 17.1981 - val_loss: 7.9970\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 17.8252 - val_loss: 9.3271\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 16.9648 - val_loss: 8.4969\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 16.5570 - val_loss: 8.2515\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 16.8310 - val_loss: 7.6649\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 17.2233 - val_loss: 8.4034\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 16.6189 - val_loss: 8.8049\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 16.5591 - val_loss: 8.4369\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 16.7783 - val_loss: 7.9060\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 16.3219 - val_loss: 8.1190\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 17.0997 - val_loss: 8.0208\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 16.5857 - val_loss: 7.4111\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 17.4484 - val_loss: 8.5731\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 16.7844 - val_loss: 9.2341\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 16.1122 - val_loss: 8.1633\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 16.9623 - val_loss: 8.4323\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 17.2514 - val_loss: 8.8588\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 15.9252 - val_loss: 7.6363\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 16.3155 - val_loss: 7.2067\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 16.8218 - val_loss: 8.2123\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 16.4397 - val_loss: 8.5265\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 15.9572 - val_loss: 7.3596\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 16.6420 - val_loss: 8.0262\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 17.1788 - val_loss: 7.9142\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 16.2844 - val_loss: 8.0435\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 86ms/step - loss: 16.2104 - val_loss: 8.0825\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 15.7013 - val_loss: 7.8539\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 16.4517 - val_loss: 7.9599\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 16.2501 - val_loss: 8.2279\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 15.6594 - val_loss: 7.2904\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 16.4976 - val_loss: 8.0020\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 16.3594 - val_loss: 7.2268\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 15.9659 - val_loss: 7.6055\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 16.3089 - val_loss: 7.7099\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 16.6509 - val_loss: 7.3528\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 16.9630 - val_loss: 8.6629\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 17.0274 - val_loss: 8.7027\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 17.9565 - val_loss: 7.6941\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 16.4155 - val_loss: 7.6961\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 15.5914 - val_loss: 8.0987\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 16.1511 - val_loss: 8.2348\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 16.9283 - val_loss: 8.2120\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 16.2583 - val_loss: 8.3368\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 15.7579 - val_loss: 7.9315\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 15.8906 - val_loss: 7.0840\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 15.6476 - val_loss: 7.8086\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 17.9577 - val_loss: 9.7858\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 16.5724 - val_loss: 9.0512\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 15.9767 - val_loss: 8.1162\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 16.4879 - val_loss: 8.2456\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 16.1897 - val_loss: 8.8995\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 15.8702 - val_loss: 7.8869\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 15.9755 - val_loss: 8.6832\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 15.8953 - val_loss: 7.8451\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 16.5040 - val_loss: 7.4564\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 15.9529 - val_loss: 7.2176\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 16.5245 - val_loss: 7.2446\n",
      "Loss: 15.59135913848877\n",
      "Validation loss: 6.87535285949707\n",
      "Training duration: 95.65745449066162 seconds\n",
      "Training with parameters: {'LSTM1_units': 192, 'LSTM1_activation': 'sigmoid', 'DROPOUT1_rate': 0.1, 'LSTM2_units': 208, 'LSTM2_activation': 'sigmoid', 'DROPOUT2_rate': 0.2, 'learning_rate': 0.001} (4/8)\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 5s 217ms/step - loss: 47.6732 - val_loss: 19.7364\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 44.7432 - val_loss: 16.7175\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 42.7395 - val_loss: 15.0269\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 41.3277 - val_loss: 14.8342\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 40.5255 - val_loss: 15.6262\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 40.1271 - val_loss: 16.4784\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 39.9634 - val_loss: 17.0110\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 39.9366 - val_loss: 17.1945\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 1s 167ms/step - loss: 39.7557 - val_loss: 17.0286\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 39.5425 - val_loss: 16.7001\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 39.1750 - val_loss: 16.2639\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 38.7019 - val_loss: 15.4690\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 37.9330 - val_loss: 14.3589\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 36.7356 - val_loss: 12.6749\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 35.3196 - val_loss: 11.5287\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 33.8801 - val_loss: 11.4102\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 32.7171 - val_loss: 11.5040\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 31.6568 - val_loss: 10.7228\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 31.0196 - val_loss: 10.3162\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 30.3631 - val_loss: 10.0340\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 29.3325 - val_loss: 9.8463\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 28.5876 - val_loss: 9.6474\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 27.8129 - val_loss: 9.6347\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 27.1015 - val_loss: 10.3371\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 26.3378 - val_loss: 9.6329\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 25.9486 - val_loss: 10.2266\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 25.3435 - val_loss: 9.6071\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 24.6796 - val_loss: 9.4323\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8447 - val_loss: 9.5762\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 23.1727 - val_loss: 9.5177\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 22.7635 - val_loss: 9.8385\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 22.5022 - val_loss: 9.0451\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 21.6400 - val_loss: 9.5572\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 21.1872 - val_loss: 9.3785\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 20.7901 - val_loss: 9.0665\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 20.1778 - val_loss: 10.2464\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 20.1366 - val_loss: 9.2779\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 19.2594 - val_loss: 9.1266\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 19.1206 - val_loss: 9.0797\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 18.4594 - val_loss: 8.9105\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 18.2083 - val_loss: 8.9056\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 17.8100 - val_loss: 9.0019\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 17.4843 - val_loss: 8.9284\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 17.6824 - val_loss: 8.7667\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 17.6711 - val_loss: 8.6539\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 17.5531 - val_loss: 8.6131\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 17.1677 - val_loss: 8.6429\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 17.0004 - val_loss: 9.0654\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 17.0036 - val_loss: 8.5613\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 17.2810 - val_loss: 8.7986\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 17.1422 - val_loss: 8.7980\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 16.9209 - val_loss: 8.4584\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 16.7325 - val_loss: 8.4681\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 16.8198 - val_loss: 8.8969\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 16.6223 - val_loss: 8.4700\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 16.6520 - val_loss: 8.4975\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 16.4776 - val_loss: 8.5905\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 16.5236 - val_loss: 8.3444\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 16.6406 - val_loss: 8.6455\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 16.7009 - val_loss: 8.5606\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 16.2780 - val_loss: 8.2424\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 16.2179 - val_loss: 8.2723\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 16.1001 - val_loss: 8.2808\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 1s 127ms/step - loss: 16.2458 - val_loss: 8.4255\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 16.3447 - val_loss: 8.4754\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 16.0371 - val_loss: 8.1203\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 15.8788 - val_loss: 8.0352\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 1s 97ms/step - loss: 15.9391 - val_loss: 8.1300\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 15.9238 - val_loss: 8.0021\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 15.6315 - val_loss: 8.0280\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 16.0081 - val_loss: 7.9146\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 15.6137 - val_loss: 7.9711\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 1s 97ms/step - loss: 15.7783 - val_loss: 7.9791\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 15.5010 - val_loss: 7.8067\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 1s 97ms/step - loss: 15.4695 - val_loss: 7.9340\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 15.2321 - val_loss: 7.8301\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 15.1940 - val_loss: 7.7830\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 15.3478 - val_loss: 7.7163\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 15.4114 - val_loss: 7.5952\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 15.3951 - val_loss: 7.7359\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 14.9450 - val_loss: 7.6116\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 15.1830 - val_loss: 7.6095\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 15.0297 - val_loss: 7.6397\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 14.8760 - val_loss: 7.7841\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 1s 97ms/step - loss: 14.9824 - val_loss: 7.4059\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 14.9395 - val_loss: 7.4549\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 15.1247 - val_loss: 7.5017\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 14.7038 - val_loss: 7.3841\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 14.6901 - val_loss: 7.9854\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 14.6955 - val_loss: 7.3980\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 14.8623 - val_loss: 7.5507\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 14.6064 - val_loss: 7.5532\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 1s 97ms/step - loss: 14.4985 - val_loss: 7.7303\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 14.4263 - val_loss: 7.3206\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 14.1264 - val_loss: 7.2353\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 14.1889 - val_loss: 7.2770\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 14.5684 - val_loss: 7.5982\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 14.3986 - val_loss: 7.4895\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 14.0832 - val_loss: 7.2620\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 13.9320 - val_loss: 7.1427\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 14.1081 - val_loss: 7.2163\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 13.7309 - val_loss: 7.1884\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 13.9277 - val_loss: 7.3402\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 13.9969 - val_loss: 7.1299\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 13.8882 - val_loss: 7.1913\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 13.6905 - val_loss: 7.3344\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 14.0401 - val_loss: 8.2792\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 13.7515 - val_loss: 7.5258\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 13.5936 - val_loss: 7.1224\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 13.8683 - val_loss: 6.9463\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 13.3767 - val_loss: 6.9383\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 13.4922 - val_loss: 7.2130\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 1s 97ms/step - loss: 13.3315 - val_loss: 6.9613\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 13.2639 - val_loss: 6.9715\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 13.5175 - val_loss: 7.1935\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 13.4078 - val_loss: 7.1875\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 13.1427 - val_loss: 7.0170\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 12.8993 - val_loss: 7.0813\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 12.9127 - val_loss: 6.9449\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 13.1723 - val_loss: 6.9955\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 12.9071 - val_loss: 7.4547\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 13.0653 - val_loss: 7.1872\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 12.9819 - val_loss: 7.0499\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 13.0540 - val_loss: 7.1359\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 12.7999 - val_loss: 7.3550\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 13.0453 - val_loss: 7.3756\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 12.7072 - val_loss: 8.1666\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 13.1498 - val_loss: 7.5942\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 12.9102 - val_loss: 7.3800\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 12.8797 - val_loss: 7.2301\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 12.8282 - val_loss: 7.4699\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 12.7859 - val_loss: 7.5296\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 12.7292 - val_loss: 7.4171\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 12.4985 - val_loss: 7.5241\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 12.6689 - val_loss: 7.6861\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 12.6435 - val_loss: 7.1961\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 12.5404 - val_loss: 7.2914\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 12.9639 - val_loss: 7.7360\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 12.8084 - val_loss: 7.7409\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 12.5133 - val_loss: 7.1399\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 12.5266 - val_loss: 7.8461\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 12.6159 - val_loss: 7.4742\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 12.8567 - val_loss: 7.3101\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 12.3239 - val_loss: 7.4795\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 12.3246 - val_loss: 7.2927\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 12.6574 - val_loss: 7.3913\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 12.5504 - val_loss: 8.0927\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 12.4003 - val_loss: 7.0723\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 12.6104 - val_loss: 7.1603\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 12.0739 - val_loss: 7.4495\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 1s 97ms/step - loss: 12.0198 - val_loss: 7.6295\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 11.9682 - val_loss: 7.2809\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 11.9316 - val_loss: 7.5530\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 11.7763 - val_loss: 7.2914\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 11.8411 - val_loss: 7.2027\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 11.8667 - val_loss: 7.8883\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 12.1604 - val_loss: 7.5327\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 11.7087 - val_loss: 7.2366\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 12.2540 - val_loss: 8.0149\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 11.5891 - val_loss: 7.6938\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 11.5018 - val_loss: 7.4332\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 11.7563 - val_loss: 7.2228\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 11.9114 - val_loss: 8.1831\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 11.6481 - val_loss: 7.2357\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 11.4594 - val_loss: 7.4262\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 11.7309 - val_loss: 7.7349\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 11.7124 - val_loss: 7.8636\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 11.5702 - val_loss: 7.6089\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 11.4942 - val_loss: 7.6250\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 11.7118 - val_loss: 9.0432\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 11.9577 - val_loss: 7.0332\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 11.9864 - val_loss: 7.8943\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 11.6093 - val_loss: 7.2880\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 12.0600 - val_loss: 7.3039\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 11.8488 - val_loss: 7.8702\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 11.2919 - val_loss: 7.1816\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 11.2687 - val_loss: 7.4058\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 11.6741 - val_loss: 8.5782\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 11.5063 - val_loss: 7.0412\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 12.0006 - val_loss: 7.6776\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 11.1608 - val_loss: 7.1186\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 11.9983 - val_loss: 7.0645\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 11.4790 - val_loss: 8.1685\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 11.3150 - val_loss: 6.9812\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 11.6420 - val_loss: 7.4002\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 11.3946 - val_loss: 8.1324\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 11.1094 - val_loss: 7.2200\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 11.2891 - val_loss: 7.6509\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 11.3860 - val_loss: 7.4930\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 11.0333 - val_loss: 7.0519\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 11.0891 - val_loss: 7.5698\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 10.9544 - val_loss: 7.1447\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 10.6083 - val_loss: 7.2631\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 10.8916 - val_loss: 7.4999\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 10.9551 - val_loss: 7.3597\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 10.9019 - val_loss: 7.0211\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 10.7275 - val_loss: 7.6516\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 10.6763 - val_loss: 7.4024\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 11.1958 - val_loss: 7.3757\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 11.4693 - val_loss: 8.2981\n",
      "Loss: 10.608258247375488\n",
      "Validation loss: 6.938265800476074\n",
      "Training duration: 124.41239714622498 seconds\n",
      "Training with parameters: {'LSTM1_units': 320, 'LSTM1_activation': 'tanh', 'DROPOUT1_rate': 0.1, 'LSTM2_units': 160, 'LSTM2_activation': 'sigmoid', 'DROPOUT2_rate': 0.2, 'learning_rate': 0.01} (5/8)\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 5s 286ms/step - loss: 43.1502 - val_loss: 15.8777\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 39.9404 - val_loss: 19.1877\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 40.7431 - val_loss: 18.5222\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 40.3107 - val_loss: 17.3989\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 39.8371 - val_loss: 16.3493\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 39.2929 - val_loss: 15.8285\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 38.6752 - val_loss: 15.8733\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 38.1015 - val_loss: 15.7100\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 37.5146 - val_loss: 15.4320\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 36.8884 - val_loss: 15.1646\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 36.3087 - val_loss: 14.3670\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 35.3962 - val_loss: 13.5632\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 34.4279 - val_loss: 12.9103\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 33.8990 - val_loss: 12.3832\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 33.5089 - val_loss: 11.1699\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 32.3444 - val_loss: 10.6229\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 31.6026 - val_loss: 10.6957\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 30.9307 - val_loss: 10.2602\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 30.1783 - val_loss: 9.9364\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 29.6009 - val_loss: 9.7677\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 29.3893 - val_loss: 9.9559\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 28.9108 - val_loss: 12.9798\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 29.7938 - val_loss: 10.1745\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 1s 141ms/step - loss: 27.6361 - val_loss: 9.8330\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 26.1997 - val_loss: 10.1177\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 25.4662 - val_loss: 9.7658\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 24.7967 - val_loss: 9.3477\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 24.3787 - val_loss: 9.7782\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 1s 174ms/step - loss: 22.6126 - val_loss: 9.4564\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 22.5183 - val_loss: 9.3477\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 22.5953 - val_loss: 11.7243\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 21.6085 - val_loss: 9.8783\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 20.7069 - val_loss: 9.0060\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 1s 127ms/step - loss: 21.4522 - val_loss: 9.9804\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 20.1991 - val_loss: 8.7426\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 19.9692 - val_loss: 9.5683\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 19.9945 - val_loss: 10.7855\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 19.0691 - val_loss: 9.4350\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 1s 142ms/step - loss: 20.6754 - val_loss: 12.1498\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 19.7984 - val_loss: 10.0833\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 19.3263 - val_loss: 10.9745\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 19.3787 - val_loss: 8.5944\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 19.5355 - val_loss: 11.3084\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 18.6817 - val_loss: 9.8048\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 18.6797 - val_loss: 12.3437\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 19.5566 - val_loss: 10.0178\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 1s 172ms/step - loss: 19.5959 - val_loss: 10.3212\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 19.0906 - val_loss: 11.0349\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 19.3055 - val_loss: 10.7936\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 18.2705 - val_loss: 9.3502\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 17.6935 - val_loss: 10.2276\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 18.3519 - val_loss: 9.9328\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 17.9241 - val_loss: 9.9356\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 1s 127ms/step - loss: 18.4474 - val_loss: 11.7686\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 17.8409 - val_loss: 10.6577\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 17.6416 - val_loss: 10.4099\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 17.4198 - val_loss: 10.1441\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 18.0486 - val_loss: 12.9772\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 18.3701 - val_loss: 9.2716\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 18.3533 - val_loss: 11.0518\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 19.6508 - val_loss: 12.1766\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 18.2081 - val_loss: 8.4446\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 18.5008 - val_loss: 10.1417\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 1s 175ms/step - loss: 18.0848 - val_loss: 8.5806\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 17.1688 - val_loss: 9.5843\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 17.9297 - val_loss: 8.6664\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 18.1500 - val_loss: 9.1286\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 1s 171ms/step - loss: 18.0762 - val_loss: 9.6335\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 18.4130 - val_loss: 11.7057\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 17.5814 - val_loss: 8.0799\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 18.1712 - val_loss: 8.0301\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 17.2444 - val_loss: 10.1177\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 17.1257 - val_loss: 7.8997\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 1s 174ms/step - loss: 18.2435 - val_loss: 9.1211\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 18.8804 - val_loss: 10.3024\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 16.9589 - val_loss: 9.4856\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 17.5629 - val_loss: 9.0832\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 17.2149 - val_loss: 9.4954\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 16.4524 - val_loss: 9.1736\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 17.4465 - val_loss: 8.0993\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 17.5787 - val_loss: 10.1524\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 17.9649 - val_loss: 8.5898\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 16.6912 - val_loss: 8.1428\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 16.6204 - val_loss: 8.8239\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 16.6038 - val_loss: 8.3576\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 17.4337 - val_loss: 9.0053\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 17.4011 - val_loss: 9.8056\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 17.0022 - val_loss: 8.5199\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 17.0447 - val_loss: 9.9494\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 16.8757 - val_loss: 8.0069\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 16.6481 - val_loss: 8.5149\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 16.8639 - val_loss: 9.4253\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 17.5074 - val_loss: 9.5969\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 16.6649 - val_loss: 10.8428\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 16.8617 - val_loss: 9.2615\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 17.0854 - val_loss: 8.5009\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 17.2716 - val_loss: 9.9579\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 16.5899 - val_loss: 8.4136\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 16.8953 - val_loss: 10.2502\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 17.6176 - val_loss: 10.2805\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 17.2122 - val_loss: 9.4554\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 17.3969 - val_loss: 8.3827\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 17.4448 - val_loss: 9.1508\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 17.0230 - val_loss: 10.1295\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 16.4774 - val_loss: 7.1032\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 16.8514 - val_loss: 8.1215\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 16.5271 - val_loss: 7.6720\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 15.6378 - val_loss: 8.2823\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 16.2954 - val_loss: 9.2438\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 16.7317 - val_loss: 8.8318\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 15.6009 - val_loss: 8.1685\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 16.6309 - val_loss: 7.8343\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 16.4781 - val_loss: 8.4625\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 16.7016 - val_loss: 9.4321\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 16.3386 - val_loss: 10.7854\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 17.0297 - val_loss: 8.7059\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 17.2985 - val_loss: 8.5743\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 16.8789 - val_loss: 8.7288\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 15.7854 - val_loss: 7.4718\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 16.8650 - val_loss: 7.3068\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 16.4430 - val_loss: 7.4313\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 16.8197 - val_loss: 8.8054\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 15.9372 - val_loss: 7.9633\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 16.7596 - val_loss: 8.2185\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 16.2056 - val_loss: 7.6912\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 16.3990 - val_loss: 9.5061\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 16.3773 - val_loss: 9.8224\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 16.3987 - val_loss: 7.5289\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 15.9685 - val_loss: 8.0926\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 17.0279 - val_loss: 9.4182\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 16.4864 - val_loss: 8.7459\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 16.9515 - val_loss: 8.0033\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 15.6472 - val_loss: 7.4723\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 16.8094 - val_loss: 7.2216\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 17.5643 - val_loss: 9.4855\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 17.4327 - val_loss: 10.0408\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 16.6022 - val_loss: 8.4919\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 16.9943 - val_loss: 9.7996\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 16.6086 - val_loss: 8.6610\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 16.7094 - val_loss: 9.3114\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 16.1624 - val_loss: 8.7124\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 16.9129 - val_loss: 8.3417\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 16.0946 - val_loss: 8.7519\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 15.9852 - val_loss: 7.9815\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 16.3694 - val_loss: 8.5764\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 16.3536 - val_loss: 8.2565\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 16.1661 - val_loss: 7.6545\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 1s 171ms/step - loss: 16.0701 - val_loss: 8.4981\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 15.7950 - val_loss: 7.8293\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 15.5765 - val_loss: 8.0439\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 15.0177 - val_loss: 7.5471\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 15.6353 - val_loss: 7.0407\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 15.1286 - val_loss: 7.4101\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 15.3513 - val_loss: 7.7768\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 15.3845 - val_loss: 7.3864\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 15.5044 - val_loss: 6.8927\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 14.7916 - val_loss: 7.3411\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 15.9291 - val_loss: 7.2560\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 15.6936 - val_loss: 6.9195\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 15.5798 - val_loss: 7.1831\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 15.4897 - val_loss: 7.4382\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 16.0350 - val_loss: 7.1610\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 15.3411 - val_loss: 6.6024\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 15.1740 - val_loss: 7.5162\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 15.4295 - val_loss: 7.6610\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 14.9783 - val_loss: 7.4173\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 15.9284 - val_loss: 7.3529\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 14.9682 - val_loss: 6.6619\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 15.4167 - val_loss: 7.7163\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 15.5371 - val_loss: 7.6838\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 15.1334 - val_loss: 7.4591\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 14.9057 - val_loss: 7.7186\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 15.8226 - val_loss: 7.4906\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 16.0702 - val_loss: 7.4906\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 15.7442 - val_loss: 9.1237\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 16.1101 - val_loss: 9.8964\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 16.1163 - val_loss: 7.3821\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 16.0854 - val_loss: 7.9986\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 15.4419 - val_loss: 9.4492\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 15.4344 - val_loss: 7.7957\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 15.2263 - val_loss: 7.6639\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 14.8882 - val_loss: 7.6840\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 1s 142ms/step - loss: 14.6357 - val_loss: 7.4226\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 15.1358 - val_loss: 7.4517\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 14.4093 - val_loss: 7.5893\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 14.8852 - val_loss: 7.6762\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 14.8617 - val_loss: 7.4836\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 15.5879 - val_loss: 7.6352\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 15.2222 - val_loss: 7.7211\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 15.7596 - val_loss: 8.5479\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 15.9076 - val_loss: 8.4613\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 16.0310 - val_loss: 9.0097\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 15.5293 - val_loss: 6.9232\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 15.3790 - val_loss: 7.3307\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 14.7010 - val_loss: 7.9530\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 14.3454 - val_loss: 6.6056\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 15.1723 - val_loss: 7.1106\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 15.0755 - val_loss: 6.4554\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 13.7189 - val_loss: 7.1137\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 14.6917 - val_loss: 7.1625\n",
      "Loss: 13.718942642211914\n",
      "Validation loss: 6.4554243087768555\n",
      "Training duration: 178.60242199897766 seconds\n",
      "Training with parameters: {'LSTM1_units': 320, 'LSTM1_activation': 'tanh', 'DROPOUT1_rate': 0.1, 'LSTM2_units': 208, 'LSTM2_activation': 'tanh', 'DROPOUT2_rate': 0.1, 'learning_rate': 0.001} (6/8)\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 7s 354ms/step - loss: 45.9272 - val_loss: 16.4308\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 1s 141ms/step - loss: 42.0680 - val_loss: 14.7226\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 40.4265 - val_loss: 15.2939\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 39.2968 - val_loss: 13.4504\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 37.0184 - val_loss: 13.1249\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 35.2548 - val_loss: 11.6691\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 33.5304 - val_loss: 11.2031\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 32.0601 - val_loss: 10.8112\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 30.7200 - val_loss: 10.0021\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 29.9669 - val_loss: 11.0995\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 29.3766 - val_loss: 9.5997\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 28.2714 - val_loss: 9.2571\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 27.5196 - val_loss: 9.1487\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 26.9410 - val_loss: 9.2236\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 26.1070 - val_loss: 8.8289\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 25.3211 - val_loss: 8.6070\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 24.6407 - val_loss: 8.3324\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 24.0761 - val_loss: 8.6992\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 23.3705 - val_loss: 8.0349\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 22.8424 - val_loss: 8.6882\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 22.5096 - val_loss: 8.2709\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 21.7138 - val_loss: 8.0866\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 20.7503 - val_loss: 7.6185\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 20.4093 - val_loss: 7.5179\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 19.7194 - val_loss: 8.0440\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 19.5787 - val_loss: 7.6662\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 18.6490 - val_loss: 7.3416\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 18.3038 - val_loss: 7.0780\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 1s 127ms/step - loss: 18.0950 - val_loss: 7.4087\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 18.1230 - val_loss: 7.2148\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 1s 127ms/step - loss: 16.9107 - val_loss: 7.1454\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 16.4431 - val_loss: 7.0071\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 15.7293 - val_loss: 6.9085\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 15.4102 - val_loss: 6.4606\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 15.3320 - val_loss: 6.9419\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 15.0676 - val_loss: 6.3297\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 14.8800 - val_loss: 6.8310\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 14.9232 - val_loss: 6.0609\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 14.2985 - val_loss: 6.3997\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 14.0751 - val_loss: 6.3881\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 13.6154 - val_loss: 5.7331\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 13.6524 - val_loss: 5.6617\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 13.2534 - val_loss: 5.5361\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 13.5372 - val_loss: 6.3262\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 13.5015 - val_loss: 5.6209\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 13.2933 - val_loss: 6.2926\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 12.9575 - val_loss: 6.3993\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 13.2857 - val_loss: 5.8796\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 12.8760 - val_loss: 5.3683\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 12.4907 - val_loss: 5.2384\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 12.1718 - val_loss: 4.9816\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 12.1983 - val_loss: 4.9576\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 12.1140 - val_loss: 5.0378\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 12.3058 - val_loss: 4.8481\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 12.1339 - val_loss: 4.6322\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 11.9986 - val_loss: 4.8995\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 12.0306 - val_loss: 4.9387\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 11.8001 - val_loss: 5.0407\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 11.8352 - val_loss: 4.7377\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 11.5620 - val_loss: 4.3768\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 11.3428 - val_loss: 4.7816\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 11.3640 - val_loss: 4.4814\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 12.0536 - val_loss: 4.2785\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 11.0099 - val_loss: 4.8353\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 11.1557 - val_loss: 4.8366\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 11.0995 - val_loss: 4.5998\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 10.7876 - val_loss: 4.2818\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 10.7729 - val_loss: 4.9885\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 10.6296 - val_loss: 4.4012\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 10.6427 - val_loss: 4.3289\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 10.6337 - val_loss: 4.3387\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 11.1659 - val_loss: 4.5666\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 12.4129 - val_loss: 4.3827\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 10.9590 - val_loss: 4.2357\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 10.9285 - val_loss: 4.2138\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 10.3778 - val_loss: 3.9985\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 10.4458 - val_loss: 4.1623\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 10.2950 - val_loss: 4.3547\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 10.2968 - val_loss: 4.2974\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 10.3456 - val_loss: 4.6324\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 10.1945 - val_loss: 4.4059\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 10.4863 - val_loss: 4.4387\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 1s 141ms/step - loss: 10.1479 - val_loss: 4.5772\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 10.2574 - val_loss: 4.3085\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 10.2920 - val_loss: 4.4616\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 10.0180 - val_loss: 4.4047\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 9.7681 - val_loss: 3.8594\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 9.7839 - val_loss: 3.9723\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 9.6848 - val_loss: 3.9672\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 9.9324 - val_loss: 4.3765\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 10.1218 - val_loss: 4.0957\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 10.0135 - val_loss: 4.5202\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 9.5301 - val_loss: 4.0253\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 9.7240 - val_loss: 4.4214\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 9.6021 - val_loss: 3.9957\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 9.4288 - val_loss: 4.3900\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 9.5552 - val_loss: 3.9234\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 9.6545 - val_loss: 4.0675\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 9.4445 - val_loss: 4.0160\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 9.3850 - val_loss: 4.0058\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 9.5733 - val_loss: 3.8457\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 9.3001 - val_loss: 4.1229\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 9.2560 - val_loss: 3.9232\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 9.2229 - val_loss: 4.3864\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 8.9993 - val_loss: 3.9277\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 8.8788 - val_loss: 4.1381\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 9.3508 - val_loss: 4.4242\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 9.5211 - val_loss: 3.9215\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 9.0158 - val_loss: 3.9854\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 8.6903 - val_loss: 3.9603\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 8.6159 - val_loss: 3.8003\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 8.2895 - val_loss: 4.1336\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 8.5392 - val_loss: 4.1795\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 8.1987 - val_loss: 4.1061\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 1s 141ms/step - loss: 8.2599 - val_loss: 3.8144\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 8.5087 - val_loss: 4.4115\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 9.2688 - val_loss: 4.1008\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 9.9185 - val_loss: 4.6203\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 8.6226 - val_loss: 4.3274\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 8.6982 - val_loss: 3.9374\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 1s 127ms/step - loss: 8.7348 - val_loss: 4.3437\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 8.5925 - val_loss: 4.1634\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 8.0967 - val_loss: 3.7607\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 8.1628 - val_loss: 4.3485\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 8.7663 - val_loss: 3.7343\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 8.5847 - val_loss: 3.9216\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 8.3771 - val_loss: 4.1683\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 1s 142ms/step - loss: 7.8103 - val_loss: 3.8638\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 7.8279 - val_loss: 4.0029\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 7.2855 - val_loss: 3.6628\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 7.5051 - val_loss: 4.0082\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 8.1974 - val_loss: 4.2117\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 8.2120 - val_loss: 4.0286\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 7.6101 - val_loss: 4.2222\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 7.2635 - val_loss: 4.0296\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 7.5410 - val_loss: 3.8234\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 7.4594 - val_loss: 3.6548\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 8.1512 - val_loss: 4.1262\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 7.4523 - val_loss: 3.6056\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 7.2187 - val_loss: 3.5187\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 7.5213 - val_loss: 3.8039\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 7.3123 - val_loss: 3.6829\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 1s 142ms/step - loss: 7.2407 - val_loss: 3.6453\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 7.8223 - val_loss: 4.2401\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 7.0147 - val_loss: 3.9506\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 7.8326 - val_loss: 4.1674\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 7.0858 - val_loss: 4.0547\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 7.5308 - val_loss: 3.7388\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 7.2669 - val_loss: 4.2922\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 6.6868 - val_loss: 4.0768\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 6.7025 - val_loss: 3.9276\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 6.5262 - val_loss: 4.4845\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 6.3892 - val_loss: 3.6572\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 6.6071 - val_loss: 3.7976\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 1s 168ms/step - loss: 7.4045 - val_loss: 3.9166\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 7.4507 - val_loss: 3.5442\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 7.0537 - val_loss: 3.5675\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 6.8748 - val_loss: 4.0007\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 7.2150 - val_loss: 4.0866\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 6.9453 - val_loss: 4.0891\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 6.8362 - val_loss: 4.4394\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 6.9481 - val_loss: 4.2176\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 6.5285 - val_loss: 4.2527\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 6.6321 - val_loss: 3.7004\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 6.0896 - val_loss: 4.1336\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 6.1274 - val_loss: 3.6443\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 6.1510 - val_loss: 3.8954\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 1s 142ms/step - loss: 6.4427 - val_loss: 3.9681\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 6.3167 - val_loss: 3.9433\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 1s 172ms/step - loss: 5.7072 - val_loss: 3.8892\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 1s 168ms/step - loss: 5.7627 - val_loss: 4.1084\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 5.5068 - val_loss: 3.5538\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 5.5189 - val_loss: 3.7402\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 5.5904 - val_loss: 3.6813\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 5.8053 - val_loss: 3.9756\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 1s 172ms/step - loss: 6.9396 - val_loss: 3.8873\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 1s 199ms/step - loss: 5.6428 - val_loss: 3.8890\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 5.9466 - val_loss: 4.0485\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 5.0855 - val_loss: 3.8166\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 1s 141ms/step - loss: 5.5543 - val_loss: 3.7634\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 5.4913 - val_loss: 3.7304\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 5.5994 - val_loss: 3.8864\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 5.8948 - val_loss: 3.6292\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 5.5910 - val_loss: 3.9833\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 5.1918 - val_loss: 4.1904\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 5.5324 - val_loss: 3.8101\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 1s 168ms/step - loss: 6.9251 - val_loss: 4.0105\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 1s 171ms/step - loss: 5.8662 - val_loss: 3.6475\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 1s 170ms/step - loss: 5.6189 - val_loss: 3.9474\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 5.3365 - val_loss: 3.6390\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 5.3463 - val_loss: 3.8618\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 5.3292 - val_loss: 3.9635\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 1s 174ms/step - loss: 5.6761 - val_loss: 4.0234\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 1s 177ms/step - loss: 5.7786 - val_loss: 4.1119\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 6.0795 - val_loss: 4.0868\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 5.1913 - val_loss: 4.3641\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 4.8715 - val_loss: 4.1738\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 5.3640 - val_loss: 4.0149\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 5.9269 - val_loss: 3.7709\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 5.2396 - val_loss: 3.9454\n",
      "Loss: 4.871475696563721\n",
      "Validation loss: 3.5186917781829834\n",
      "Training duration: 169.09053707122803 seconds\n",
      "New best model found: {'LSTM1_units': 320, 'LSTM1_activation': 'tanh', 'DROPOUT1_rate': 0.1, 'LSTM2_units': 208, 'LSTM2_activation': 'tanh', 'DROPOUT2_rate': 0.1, 'learning_rate': 0.001} with validation loss: 3.5186917781829834\n",
      "Training with parameters: {'LSTM1_units': 320, 'LSTM1_activation': 'sigmoid', 'DROPOUT1_rate': 0.2, 'LSTM2_units': 160, 'LSTM2_activation': 'sigmoid', 'DROPOUT2_rate': 0.1, 'learning_rate': 0.001} (7/8)\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 5s 273ms/step - loss: 47.3113 - val_loss: 18.9144\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 44.3529 - val_loss: 16.5221\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 42.7825 - val_loss: 15.2438\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 1s 220ms/step - loss: 41.7679 - val_loss: 14.7798\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 40.9941 - val_loss: 14.9973\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 1s 218ms/step - loss: 40.4106 - val_loss: 15.6857\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 1s 176ms/step - loss: 40.1436 - val_loss: 16.3660\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 40.0481 - val_loss: 16.8754\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 40.0142 - val_loss: 17.1587\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 1s 222ms/step - loss: 39.9631 - val_loss: 17.1931\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 39.8646 - val_loss: 17.0004\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 39.5940 - val_loss: 16.6631\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 1s 176ms/step - loss: 39.3556 - val_loss: 16.3342\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 38.8767 - val_loss: 15.8841\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 1s 207ms/step - loss: 38.3187 - val_loss: 15.1503\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 37.6614 - val_loss: 14.2000\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 1s 167ms/step - loss: 36.6828 - val_loss: 13.0977\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 1s 174ms/step - loss: 35.5043 - val_loss: 11.7479\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 1s 167ms/step - loss: 34.0885 - val_loss: 11.5086\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 1s 237ms/step - loss: 32.8366 - val_loss: 11.7726\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 1s 174ms/step - loss: 31.8521 - val_loss: 11.1871\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 31.1286 - val_loss: 10.3805\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 30.5747 - val_loss: 9.9170\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 29.7055 - val_loss: 9.9914\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 29.0251 - val_loss: 9.8025\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 28.5359 - val_loss: 9.7830\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 27.8468 - val_loss: 9.4336\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 2s 292ms/step - loss: 27.2339 - val_loss: 9.3864\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 1s 201ms/step - loss: 26.6906 - val_loss: 9.7529\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 2s 269ms/step - loss: 26.2374 - val_loss: 9.2370\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 25.5380 - val_loss: 9.2992\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 1s 233ms/step - loss: 24.9550 - val_loss: 9.2151\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 24.5692 - val_loss: 9.0074\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 1s 255ms/step - loss: 23.8915 - val_loss: 8.9491\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 1s 225ms/step - loss: 23.2510 - val_loss: 9.3132\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 1s 217ms/step - loss: 22.6507 - val_loss: 9.1020\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 22.0967 - val_loss: 8.8493\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 1s 175ms/step - loss: 21.4964 - val_loss: 9.1180\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 1s 174ms/step - loss: 21.0887 - val_loss: 8.8970\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 20.5423 - val_loss: 9.5506\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 20.1312 - val_loss: 8.9596\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 19.9954 - val_loss: 8.7384\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 1s 173ms/step - loss: 19.5712 - val_loss: 8.8188\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 18.9878 - val_loss: 8.5064\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 18.4982 - val_loss: 8.6198\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 18.1644 - val_loss: 8.4926\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 17.6133 - val_loss: 8.4282\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 1s 181ms/step - loss: 17.8039 - val_loss: 8.8506\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 1s 167ms/step - loss: 17.7502 - val_loss: 8.7349\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 17.2197 - val_loss: 8.6507\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 1s 168ms/step - loss: 17.0561 - val_loss: 8.3839\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 1s 225ms/step - loss: 16.7455 - val_loss: 8.1988\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 16.6875 - val_loss: 8.0885\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 16.4666 - val_loss: 8.2608\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 1s 168ms/step - loss: 16.1803 - val_loss: 8.1173\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 16.1682 - val_loss: 8.0871\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 1s 164ms/step - loss: 16.1428 - val_loss: 7.9886\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 16.0329 - val_loss: 8.2094\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 15.9592 - val_loss: 7.7959\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 15.9225 - val_loss: 7.8246\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 15.7112 - val_loss: 7.8291\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 15.6744 - val_loss: 7.7135\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 15.6825 - val_loss: 7.5056\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 15.4356 - val_loss: 7.6345\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 15.5299 - val_loss: 7.4445\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 15.1927 - val_loss: 7.7211\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 14.9973 - val_loss: 7.4647\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 14.9473 - val_loss: 7.3662\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 14.7097 - val_loss: 7.2952\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 14.8537 - val_loss: 7.1790\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 1s 164ms/step - loss: 15.0493 - val_loss: 7.3945\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 14.5391 - val_loss: 7.1696\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 1s 194ms/step - loss: 14.8488 - val_loss: 7.6375\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 1s 173ms/step - loss: 14.7684 - val_loss: 7.1015\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 14.4140 - val_loss: 7.3555\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 14.3742 - val_loss: 7.3735\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 14.4260 - val_loss: 7.4219\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 14.8854 - val_loss: 7.0535\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 14.3493 - val_loss: 7.8125\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 1s 164ms/step - loss: 13.9821 - val_loss: 6.8854\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 13.8855 - val_loss: 7.0229\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 1s 173ms/step - loss: 13.7766 - val_loss: 7.0742\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 13.6306 - val_loss: 6.9185\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 13.6009 - val_loss: 7.0821\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 1s 168ms/step - loss: 13.3105 - val_loss: 7.0665\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 13.6578 - val_loss: 7.3921\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 13.9965 - val_loss: 6.9890\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 13.4819 - val_loss: 7.2097\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 1s 164ms/step - loss: 13.5925 - val_loss: 7.3240\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 13.5414 - val_loss: 6.9473\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 13.4798 - val_loss: 7.1370\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 13.2640 - val_loss: 7.1663\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 13.0561 - val_loss: 7.3400\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 13.3043 - val_loss: 7.3356\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 13.5966 - val_loss: 7.4383\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 13.3564 - val_loss: 6.7967\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 1s 212ms/step - loss: 13.0191 - val_loss: 7.1100\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 1s 181ms/step - loss: 12.9294 - val_loss: 7.5554\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 1s 222ms/step - loss: 13.1795 - val_loss: 7.3924\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 13.0366 - val_loss: 6.9380\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 12.9711 - val_loss: 7.7069\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 1s 177ms/step - loss: 13.5350 - val_loss: 7.5904\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 13.2918 - val_loss: 7.3911\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 12.9713 - val_loss: 6.9015\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 1s 175ms/step - loss: 12.8239 - val_loss: 7.1615\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 12.6335 - val_loss: 7.1036\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 12.3187 - val_loss: 6.8569\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 12.0936 - val_loss: 6.6201\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 1s 181ms/step - loss: 12.3110 - val_loss: 6.8527\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 12.1544 - val_loss: 6.4638\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 12.3057 - val_loss: 6.9449\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 12.0794 - val_loss: 6.9200\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 11.9536 - val_loss: 6.6325\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 11.8456 - val_loss: 7.0360\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 12.1501 - val_loss: 6.6040\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 12.6914 - val_loss: 6.9480\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 12.4197 - val_loss: 7.6537\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 12.3945 - val_loss: 6.7882\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 1s 170ms/step - loss: 12.2836 - val_loss: 6.7636\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 11.8951 - val_loss: 6.8289\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 1s 231ms/step - loss: 12.2035 - val_loss: 6.9623\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 12.1835 - val_loss: 7.4429\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 12.0866 - val_loss: 6.5471\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 12.0257 - val_loss: 6.6446\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 1s 167ms/step - loss: 11.5878 - val_loss: 6.4680\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 11.7911 - val_loss: 6.9528\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 12.2141 - val_loss: 7.0535\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 12.4894 - val_loss: 7.3378\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 12.3978 - val_loss: 8.3176\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 1s 168ms/step - loss: 12.5098 - val_loss: 7.6502\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 12.4421 - val_loss: 7.6709\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 1s 171ms/step - loss: 12.3939 - val_loss: 7.0032\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 1s 174ms/step - loss: 11.8446 - val_loss: 6.6751\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 11.8154 - val_loss: 6.7865\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 1s 175ms/step - loss: 11.5129 - val_loss: 6.5173\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 11.4389 - val_loss: 6.4940\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 1s 176ms/step - loss: 11.3667 - val_loss: 6.2773\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 1s 190ms/step - loss: 11.1268 - val_loss: 6.3315\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 1s 182ms/step - loss: 11.5313 - val_loss: 6.3604\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 11.4438 - val_loss: 6.8485\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 1s 196ms/step - loss: 11.3434 - val_loss: 6.4037\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 11.3552 - val_loss: 6.6016\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 11.3556 - val_loss: 6.1337\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 11.0429 - val_loss: 6.3510\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 1s 181ms/step - loss: 10.9915 - val_loss: 6.3160\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 11.0899 - val_loss: 6.0694\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 10.9855 - val_loss: 6.1889\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 10.9824 - val_loss: 6.2515\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 1s 239ms/step - loss: 11.0278 - val_loss: 6.4291\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 1s 181ms/step - loss: 11.5289 - val_loss: 6.7102\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 1s 173ms/step - loss: 11.4696 - val_loss: 6.4389\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 11.1651 - val_loss: 6.3572\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 1s 181ms/step - loss: 11.1502 - val_loss: 6.1461\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 1s 174ms/step - loss: 11.6956 - val_loss: 6.4509\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 1s 239ms/step - loss: 11.0632 - val_loss: 7.4416\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 11.0932 - val_loss: 6.2062\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 1s 176ms/step - loss: 11.2304 - val_loss: 6.1549\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 10.7685 - val_loss: 6.1904\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 1s 205ms/step - loss: 10.7231 - val_loss: 6.2598\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 1s 164ms/step - loss: 10.5509 - val_loss: 6.1499\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 10.6930 - val_loss: 6.0055\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 10.9060 - val_loss: 6.3990\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 1s 175ms/step - loss: 10.5655 - val_loss: 5.9814\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 10.8428 - val_loss: 6.1994\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 11.2648 - val_loss: 6.7324\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 11.7441 - val_loss: 6.6243\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 1s 186ms/step - loss: 10.9700 - val_loss: 7.2665\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 11.3181 - val_loss: 6.2621\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 1s 167ms/step - loss: 11.2054 - val_loss: 5.9213\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 10.6490 - val_loss: 6.1404\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 1s 168ms/step - loss: 10.3764 - val_loss: 6.0067\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 10.4501 - val_loss: 6.0754\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 10.4360 - val_loss: 6.3056\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 10.4603 - val_loss: 6.2453\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 10.3329 - val_loss: 6.0704\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 10.6081 - val_loss: 6.0403\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 10.6184 - val_loss: 5.7571\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 10.9612 - val_loss: 6.0884\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 10.7226 - val_loss: 6.1068\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 10.6469 - val_loss: 5.7580\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 10.3820 - val_loss: 6.0045\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 1s 176ms/step - loss: 10.0294 - val_loss: 6.0044\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 1s 238ms/step - loss: 10.5033 - val_loss: 5.6977\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 10.0489 - val_loss: 6.1747\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 10.1428 - val_loss: 5.8567\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 10.2881 - val_loss: 5.7983\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 10.2158 - val_loss: 5.9375\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 10.4226 - val_loss: 6.0451\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 10.3566 - val_loss: 6.2040\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 10.7965 - val_loss: 5.5515\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 10.3599 - val_loss: 5.8154\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 1s 170ms/step - loss: 9.6848 - val_loss: 5.4558\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 9.9338 - val_loss: 5.4223\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 9.6979 - val_loss: 5.5961\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 9.7736 - val_loss: 5.7284\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 10.4290 - val_loss: 6.2763\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 10.2424 - val_loss: 5.9420\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 10.3538 - val_loss: 6.0188\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 1s 168ms/step - loss: 10.0040 - val_loss: 5.8211\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 9.8330 - val_loss: 6.1959\n",
      "Loss: 9.684808731079102\n",
      "Validation loss: 5.422255516052246\n",
      "Training duration: 211.44508743286133 seconds\n",
      "Training with parameters: {'LSTM1_units': 320, 'LSTM1_activation': 'sigmoid', 'DROPOUT1_rate': 0.2, 'LSTM2_units': 208, 'LSTM2_activation': 'tanh', 'DROPOUT2_rate': 0.2, 'learning_rate': 0.01} (8/8)\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 6s 319ms/step - loss: 41.7520 - val_loss: 18.8281\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 40.7841 - val_loss: 19.0614\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 1s 180ms/step - loss: 40.8152 - val_loss: 17.1564\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 40.5236 - val_loss: 16.5115\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 1s 205ms/step - loss: 40.2653 - val_loss: 16.9725\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 1s 172ms/step - loss: 40.3226 - val_loss: 17.3092\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 40.3433 - val_loss: 17.2383\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 40.2897 - val_loss: 17.1863\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 1s 242ms/step - loss: 40.1255 - val_loss: 17.3370\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 1s 167ms/step - loss: 40.0748 - val_loss: 17.3036\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 1s 174ms/step - loss: 40.1138 - val_loss: 17.0974\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 39.9553 - val_loss: 17.0389\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 1s 172ms/step - loss: 39.7725 - val_loss: 16.9126\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 1s 172ms/step - loss: 39.6529 - val_loss: 16.7770\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 39.5295 - val_loss: 16.5154\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 39.3591 - val_loss: 16.5467\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 1s 189ms/step - loss: 39.2256 - val_loss: 16.5509\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 39.0627 - val_loss: 16.3352\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 1s 185ms/step - loss: 38.8032 - val_loss: 16.1027\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 1s 179ms/step - loss: 38.7013 - val_loss: 16.1338\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 1s 176ms/step - loss: 38.5930 - val_loss: 15.9485\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 38.3213 - val_loss: 15.7361\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 38.1934 - val_loss: 15.4512\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 37.7101 - val_loss: 15.5365\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 1s 171ms/step - loss: 37.8622 - val_loss: 14.9460\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 1s 226ms/step - loss: 37.5263 - val_loss: 14.6464\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 36.6455 - val_loss: 15.3675\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 36.8634 - val_loss: 14.3059\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 1s 168ms/step - loss: 36.3894 - val_loss: 14.4856\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 1s 199ms/step - loss: 36.3327 - val_loss: 13.6565\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 35.0688 - val_loss: 14.5843\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 36.1335 - val_loss: 12.2405\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 1s 195ms/step - loss: 35.1986 - val_loss: 14.1434\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 35.7780 - val_loss: 11.8817\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 34.7281 - val_loss: 12.1407\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 34.5716 - val_loss: 11.5268\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 1s 232ms/step - loss: 34.1885 - val_loss: 11.8723\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 1s 239ms/step - loss: 34.1211 - val_loss: 11.5762\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 2s 275ms/step - loss: 33.8039 - val_loss: 11.2370\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 1s 168ms/step - loss: 33.4047 - val_loss: 11.4996\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 1s 223ms/step - loss: 33.6417 - val_loss: 11.2824\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 33.1775 - val_loss: 11.8633\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 1s 232ms/step - loss: 33.2457 - val_loss: 11.1097\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 1s 250ms/step - loss: 33.5156 - val_loss: 10.9095\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 1s 246ms/step - loss: 31.9761 - val_loss: 12.0078\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 33.2671 - val_loss: 11.4372\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 1s 229ms/step - loss: 32.0773 - val_loss: 11.3954\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 32.9643 - val_loss: 11.5504\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 1s 172ms/step - loss: 31.2863 - val_loss: 13.4220\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 33.6915 - val_loss: 13.7814\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 1s 225ms/step - loss: 32.1757 - val_loss: 13.9733\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 1s 211ms/step - loss: 33.5050 - val_loss: 10.4445\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 1s 253ms/step - loss: 31.7457 - val_loss: 12.0400\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 2s 258ms/step - loss: 32.9651 - val_loss: 11.8889\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 1s 224ms/step - loss: 30.6879 - val_loss: 12.3102\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 1s 233ms/step - loss: 31.1792 - val_loss: 11.0798\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 2s 268ms/step - loss: 30.6032 - val_loss: 11.0751\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 31.1100 - val_loss: 11.5289\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 1s 176ms/step - loss: 29.6386 - val_loss: 12.7482\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 29.8983 - val_loss: 10.8876\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 1s 219ms/step - loss: 29.3774 - val_loss: 10.4770\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 1s 224ms/step - loss: 29.8471 - val_loss: 10.4973\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 2s 275ms/step - loss: 28.6623 - val_loss: 11.3591\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 29.1066 - val_loss: 11.0156\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 1s 172ms/step - loss: 29.8739 - val_loss: 12.7707\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 28.1992 - val_loss: 11.3411\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 1s 218ms/step - loss: 28.1518 - val_loss: 11.7319\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 28.2221 - val_loss: 10.5486\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 1s 206ms/step - loss: 28.0284 - val_loss: 12.0301\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 1s 223ms/step - loss: 27.6132 - val_loss: 10.2671\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 27.7939 - val_loss: 10.6707\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 1s 220ms/step - loss: 26.3213 - val_loss: 12.8538\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 1s 230ms/step - loss: 28.7766 - val_loss: 11.8166\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 1s 215ms/step - loss: 26.5316 - val_loss: 10.7868\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 1s 171ms/step - loss: 26.5582 - val_loss: 10.5653\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 26.9108 - val_loss: 10.9368\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 26.5146 - val_loss: 13.9810\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 27.3453 - val_loss: 13.2262\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 27.2037 - val_loss: 10.6205\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 26.4813 - val_loss: 14.1081\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 1s 175ms/step - loss: 26.7960 - val_loss: 14.1575\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 26.1985 - val_loss: 11.6599\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 26.1161 - val_loss: 16.2973\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 1s 175ms/step - loss: 27.9960 - val_loss: 15.1458\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 26.6804 - val_loss: 12.7566\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 25.9353 - val_loss: 10.9222\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 28.6905 - val_loss: 13.7576\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 27.2097 - val_loss: 18.5612\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 28.9847 - val_loss: 17.5566\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 26.7727 - val_loss: 18.8541\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 27.5689 - val_loss: 13.2340\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 25.8063 - val_loss: 11.1649\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 26.9644 - val_loss: 14.2887\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 1s 176ms/step - loss: 26.3344 - val_loss: 14.6765\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 26.3384 - val_loss: 10.5421\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 26.3642 - val_loss: 14.5412\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 26.2149 - val_loss: 13.5021\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 26.4912 - val_loss: 10.7344\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 26.8344 - val_loss: 10.5128\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 24.9462 - val_loss: 14.1390\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 25.7057 - val_loss: 13.5938\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 25.3087 - val_loss: 10.9051\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 1s 164ms/step - loss: 26.4297 - val_loss: 11.6839\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 1s 167ms/step - loss: 24.0442 - val_loss: 12.3009\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 1s 176ms/step - loss: 25.7617 - val_loss: 14.7446\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 25.5457 - val_loss: 14.9191\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 1s 176ms/step - loss: 26.8104 - val_loss: 14.7767\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 1s 177ms/step - loss: 27.8715 - val_loss: 14.6924\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 1s 175ms/step - loss: 25.9967 - val_loss: 10.3995\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 25.9416 - val_loss: 15.1365\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 1s 198ms/step - loss: 25.3500 - val_loss: 10.7887\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 1s 213ms/step - loss: 24.4186 - val_loss: 11.0654\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 2s 261ms/step - loss: 24.2152 - val_loss: 12.1771\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 1s 200ms/step - loss: 24.7546 - val_loss: 11.1977\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 25.5186 - val_loss: 11.4350\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 26.4153 - val_loss: 13.2087\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 1s 164ms/step - loss: 25.3120 - val_loss: 10.2061\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 24.7436 - val_loss: 10.2542\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 25.6617 - val_loss: 13.4591\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 25.5599 - val_loss: 13.6546\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 1s 199ms/step - loss: 24.7968 - val_loss: 10.5516\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 1s 170ms/step - loss: 25.0413 - val_loss: 12.6451\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 24.4896 - val_loss: 10.4911\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 24.6251 - val_loss: 9.6060\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 1s 171ms/step - loss: 25.2996 - val_loss: 10.2055\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 25.9128 - val_loss: 12.3836\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 24.7205 - val_loss: 10.3052\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 1s 168ms/step - loss: 23.5755 - val_loss: 10.2519\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 26.0163 - val_loss: 13.0297\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 1s 172ms/step - loss: 26.2669 - val_loss: 12.3746\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 1s 175ms/step - loss: 23.9881 - val_loss: 11.1499\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 24.2119 - val_loss: 13.0083\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 1s 206ms/step - loss: 24.2453 - val_loss: 11.5026\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 1s 167ms/step - loss: 23.9503 - val_loss: 10.5459\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 1s 168ms/step - loss: 23.1554 - val_loss: 11.5878\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 24.2151 - val_loss: 10.9298\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 23.8581 - val_loss: 12.5015\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 23.2035 - val_loss: 10.5212\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 23.8592 - val_loss: 10.7239\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 23.7145 - val_loss: 10.9534\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 22.5305 - val_loss: 10.5735\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 23.9825 - val_loss: 11.0385\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 23.4379 - val_loss: 9.7525\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 24.0428 - val_loss: 10.6569\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 22.8657 - val_loss: 11.3053\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 1s 168ms/step - loss: 22.6805 - val_loss: 11.4597\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 22.8931 - val_loss: 14.1528\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 24.6976 - val_loss: 11.3347\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 23.7001 - val_loss: 11.5230\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 22.9040 - val_loss: 11.6044\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 23.6781 - val_loss: 10.9398\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 24.1624 - val_loss: 12.5701\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 22.8110 - val_loss: 10.4714\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 23.1102 - val_loss: 13.3471\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 1s 164ms/step - loss: 23.5909 - val_loss: 10.8533\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 22.8398 - val_loss: 12.4597\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 23.2721 - val_loss: 12.5626\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 22.1296 - val_loss: 9.9432\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 20.9176 - val_loss: 11.4984\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 21.3266 - val_loss: 10.0877\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 21.3251 - val_loss: 10.2365\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 20.5867 - val_loss: 9.8557\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 20.5696 - val_loss: 11.4751\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 23.3016 - val_loss: 10.0665\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 21.1100 - val_loss: 10.4716\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 24.1236 - val_loss: 11.0937\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 22.8926 - val_loss: 11.2972\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 1s 167ms/step - loss: 21.9691 - val_loss: 10.3120\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 22.0758 - val_loss: 10.1345\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 21.1108 - val_loss: 10.7609\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 21.6557 - val_loss: 10.2466\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 21.4249 - val_loss: 10.2238\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 21.6722 - val_loss: 9.7114\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 21.4236 - val_loss: 10.1207\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 22.1990 - val_loss: 9.9247\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 21.1867 - val_loss: 10.8988\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 20.9070 - val_loss: 10.1897\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 21.4695 - val_loss: 10.7931\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 1s 164ms/step - loss: 22.1444 - val_loss: 12.3069\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 22.3628 - val_loss: 11.1858\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 21.5148 - val_loss: 11.2329\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 21.4358 - val_loss: 10.5063\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 1s 164ms/step - loss: 22.2208 - val_loss: 10.9453\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 1s 191ms/step - loss: 21.0376 - val_loss: 10.6912\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 1s 164ms/step - loss: 21.3052 - val_loss: 10.8384\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 21.2069 - val_loss: 9.4951\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 21.5186 - val_loss: 12.4210\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 22.0586 - val_loss: 10.9148\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 1s 183ms/step - loss: 20.2292 - val_loss: 9.9467\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 1s 177ms/step - loss: 20.2904 - val_loss: 9.2730\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 1s 175ms/step - loss: 21.3499 - val_loss: 10.2321\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 1s 178ms/step - loss: 21.1121 - val_loss: 10.7652\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 20.7114 - val_loss: 10.2700\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 21.1328 - val_loss: 11.0557\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 20.2996 - val_loss: 10.0949\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 21.4898 - val_loss: 9.7775\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 21.4000 - val_loss: 11.4877\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 21.9889 - val_loss: 10.3483\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 20.6940 - val_loss: 10.0377\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 1s 171ms/step - loss: 19.8762 - val_loss: 9.3572\n",
      "Loss: 19.876245498657227\n",
      "Validation loss: 9.27303409576416\n",
      "Training duration: 215.76202702522278 seconds\n",
      "Best parameters: {'LSTM1_units': 320, 'LSTM1_activation': 'tanh', 'DROPOUT1_rate': 0.1, 'LSTM2_units': 208, 'LSTM2_activation': 'tanh', 'DROPOUT2_rate': 0.1, 'learning_rate': 0.001}\n",
      "Best validation loss: 3.5186917781829834\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found, using CPU instead.\")\n",
    "\n",
    "# Définition des niveaux des hyperparamètres\n",
    "LSTM1_units = [192, 320]\n",
    "LSTM1_activation = ['tanh', 'sigmoid']\n",
    "DROPOUT1_rate = [0.2, 0.1]\n",
    "LSTM2_units = [160, 208]\n",
    "LSTM2_activation = ['tanh', 'sigmoid']\n",
    "DROPOUT2_rate = [0.2, 0.1]\n",
    "learning_rate = [0.001, 0.01]\n",
    "epoch = 200\n",
    "\n",
    "# Création du plan orthogonal L8(2^7)\n",
    "orthogonal_array = [\n",
    "    [0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 1, 1, 1, 1],\n",
    "    [0, 1, 0, 0, 1, 1, 1],\n",
    "    [0, 1, 1, 1, 0, 0, 0],\n",
    "    [1, 0, 0, 1, 0, 1, 1],\n",
    "    [1, 0, 1, 0, 1, 0, 0],\n",
    "    [1, 1, 0, 1, 1, 0, 1],\n",
    "    [1, 1, 1, 0, 0, 1, 0]\n",
    "]\n",
    "\n",
    "orthogonal_array = [\n",
    "    [0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 1, 1, 1],\n",
    "    [0, 1, 1, 0, 0, 1, 1],\n",
    "    [0, 1, 1, 1, 1, 0, 0],\n",
    "    [1, 0, 1, 0, 1, 0, 1],\n",
    "    [1, 0, 1, 1, 0, 1, 0],\n",
    "    [1, 1, 0, 0, 1, 1, 0],\n",
    "    [1, 1, 0, 1, 0, 0, 1],\n",
    "    #[1, 1, 1, 1, 1, 1, 1],  # Ajout d'essais complémentaires\n",
    "    #[1, 1, 1, 0, 0, 0, 0],\n",
    "    #[1, 0, 0, 1, 1, 0, 0],\n",
    "    #[1, 0, 0, 0, 0, 1, 1],\n",
    "    #[0, 1, 0, 1, 0, 1, 0],\n",
    "    #[0, 1, 0, 0, 1, 0, 1],\n",
    "    #[0, 0, 1, 1, 0, 0, 1],\n",
    "    #[0, 0, 1, 0, 1, 1, 0]\n",
    "]\n",
    "\n",
    "# Générer les combinaisons d'hyperparamètres à partir du plan orthogonal\n",
    "param_combinations = []\n",
    "for row in orthogonal_array:\n",
    "    params = {\n",
    "        'LSTM1_units': LSTM1_units[row[0]],\n",
    "        'LSTM1_activation': LSTM1_activation[row[1]],\n",
    "        'DROPOUT1_rate': DROPOUT1_rate[row[2]],\n",
    "        'LSTM2_units': LSTM2_units[row[3]],\n",
    "        'LSTM2_activation': LSTM2_activation[row[4]],\n",
    "        'DROPOUT2_rate': DROPOUT2_rate[row[5]],\n",
    "        'learning_rate': learning_rate[row[6]]\n",
    "    }\n",
    "    param_combinations.append(params)\n",
    "# Prepare the data\n",
    "# x_train_windows and y_train_windows should be pre-defined\n",
    "# For the example, they are assumed to be loaded or created previously\n",
    "\n",
    "# Function to create the model\n",
    "def create_model(LSTM1_units, LSTM1_activation, DROPOUT1_rate, LSTM2_units, LSTM2_activation, DROPOUT2_rate, learning_rate):\n",
    "    model = Sequential([\n",
    "        LSTM(LSTM1_units, activation=LSTM1_activation, input_shape=x_train_windows.shape[1:], return_sequences=True),\n",
    "        Dropout(DROPOUT1_rate),\n",
    "        LSTM(LSTM2_units, activation=LSTM2_activation, return_sequences=False),\n",
    "        Dropout(DROPOUT2_rate),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_absolute_error')\n",
    "    return model\n",
    "\n",
    "# Function to start training and tracking emissions\n",
    "def start_training(model):\n",
    "    start_time = time.time()\n",
    "    history = model.fit(x=x_train_windows, y=y_train_windows, epochs=epoch, batch_size=128, validation_split=0.2, shuffle=False)\n",
    "    training_duration = time.time() - start_time\n",
    "    return history, training_duration\n",
    "\n",
    "# DataFrame to store results\n",
    "results_df = pd.DataFrame(columns=['LSTM1_units', 'LSTM1_activation', 'DROPOUT1_rate', 'LSTM2_units', 'LSTM2_activation', 'DROPOUT2_rate', 'learning_rate', 'loss', 'val_loss'])\n",
    "\n",
    "# Iterate over the parameter combinations from the orthogonal array\n",
    "best_params = None\n",
    "best_loss = np.inf\n",
    "total_combinations = len(param_combinations)\n",
    "current_combination = 0\n",
    "\n",
    "for params in param_combinations:\n",
    "    current_combination += 1\n",
    "    print(f\"Training with parameters: {params} ({current_combination}/{total_combinations})\")\n",
    "    \n",
    "    model = create_model(**params)\n",
    "    \n",
    "    try:\n",
    "        history, training_duration = start_training(model)\n",
    "        loss = min(history.history['loss'])\n",
    "        val_loss = min(history.history['val_loss'])\n",
    "        print(f\"Loss: {loss}\")\n",
    "        print(f\"Validation loss: {val_loss}\")\n",
    "        print(f\"Training duration: {training_duration} seconds\")\n",
    "        \n",
    "        # Create a temporary DataFrame with the current results\n",
    "        temp_df = pd.DataFrame([{**params, 'loss': loss, 'val_loss': val_loss}])\n",
    "        \n",
    "        # Concatenate the temporary DataFrame with the existing DataFrame\n",
    "        results_df = pd.concat([results_df, temp_df], ignore_index=True)\n",
    "        \n",
    "        # Plot and save the training history\n",
    "\n",
    "        start = 10\n",
    "        plt.figure()\n",
    "        plt.plot(history.history['loss'][start:], label='train')\n",
    "        plt.plot(history.history['val_loss'][start:], label='validation')\n",
    "        plt.title(f\"Loss Curves - Params: {params}\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"loss_curve_LSTM1_{params['LSTM1_units']}_LSTM1act_{params['LSTM1_activation']}_DROPOUT1_{params['DROPOUT1_rate']}_LSTM2_{params['LSTM2_units']}_LSTM2act_{params['LSTM2_activation']}_DROPOUT2_{params['DROPOUT2_rate']}_LR_{params['learning_rate']}_EPOCH_{epoch}.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Check if this is the best model so far\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_params = params\n",
    "            print(f\"New best model found: {best_params} with validation loss: {best_loss}\")\n",
    "    finally:\n",
    "        pass\n",
    "\n",
    "# Save the results DataFrame to a CSV file\n",
    "results_df.to_csv(f\"hyperparameter_results_LSTM1_{params['LSTM1_units']}_LSTM1act_{params['LSTM1_activation']}_DROPOUT1_{params['DROPOUT1_rate']}_LSTM2_{params['LSTM2_units']}_LSTM2act_{params['LSTM2_activation']}_DROPOUT2_{params['DROPOUT2_rate']}_LR_{params['learning_rate']}_EPOCH_{epoch}.csv\", index=False)\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best validation loss: {best_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LSTM1_units</th>\n",
       "      <th>LSTM1_activation</th>\n",
       "      <th>DROPOUT1_rate</th>\n",
       "      <th>LSTM2_units</th>\n",
       "      <th>LSTM2_activation</th>\n",
       "      <th>DROPOUT2_rate</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>160</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6.808808</td>\n",
       "      <td>3.701832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>208</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010</td>\n",
       "      <td>8.802302</td>\n",
       "      <td>3.687266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>192</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1</td>\n",
       "      <td>160</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010</td>\n",
       "      <td>15.591359</td>\n",
       "      <td>6.875353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>192</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1</td>\n",
       "      <td>208</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10.608258</td>\n",
       "      <td>6.938266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>320</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>160</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.010</td>\n",
       "      <td>13.718943</td>\n",
       "      <td>6.455424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>320</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>208</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.871476</td>\n",
       "      <td>3.518692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>320</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.2</td>\n",
       "      <td>160</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>9.684809</td>\n",
       "      <td>5.422256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>320</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.2</td>\n",
       "      <td>208</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.010</td>\n",
       "      <td>19.876245</td>\n",
       "      <td>9.273034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LSTM1_units LSTM1_activation  DROPOUT1_rate LSTM2_units LSTM2_activation  \\\n",
       "0         192             tanh            0.2         160             tanh   \n",
       "1         192             tanh            0.2         208          sigmoid   \n",
       "2         192          sigmoid            0.1         160             tanh   \n",
       "3         192          sigmoid            0.1         208          sigmoid   \n",
       "4         320             tanh            0.1         160          sigmoid   \n",
       "5         320             tanh            0.1         208             tanh   \n",
       "6         320          sigmoid            0.2         160          sigmoid   \n",
       "7         320          sigmoid            0.2         208             tanh   \n",
       "\n",
       "   DROPOUT2_rate  learning_rate       loss  val_loss  \n",
       "0            0.2          0.001   6.808808  3.701832  \n",
       "1            0.1          0.010   8.802302  3.687266  \n",
       "2            0.1          0.010  15.591359  6.875353  \n",
       "3            0.2          0.001  10.608258  6.938266  \n",
       "4            0.2          0.010  13.718943  6.455424  \n",
       "5            0.1          0.001   4.871476  3.518692  \n",
       "6            0.1          0.001   9.684809  5.422256  \n",
       "7            0.2          0.010  19.876245  9.273034  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
