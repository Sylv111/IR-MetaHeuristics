{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e849fc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset: scaled_dataset ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natha\\anaconda3\\envs\\IR_pv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 131ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 12:19:15,172] A new study created in memory with name: no-name-221d96e0-6d76-4c80-b33f-3b6e968460b6\n",
      "c:\\Users\\Natha\\anaconda3\\envs\\IR_pv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-06-04 12:23:41,403] Trial 0 finished with value: 0.026680152863264084 and parameters: {'lr': 0.0001263380230139621, 'filters1': 61, 'filters2': 52, 'dense_units': 40}. Best is trial 0 with value: 0.026680152863264084.\n",
      "[I 2025-06-04 12:29:10,727] Trial 1 finished with value: 0.024303648620843887 and parameters: {'lr': 0.00030444315451595943, 'filters1': 106, 'filters2': 84, 'dense_units': 47}. Best is trial 1 with value: 0.024303648620843887.\n",
      "[I 2025-06-04 12:33:32,984] Trial 2 finished with value: 0.02413373626768589 and parameters: {'lr': 0.0009545298487139011, 'filters1': 75, 'filters2': 49, 'dense_units': 89}. Best is trial 2 with value: 0.02413373626768589.\n",
      "[I 2025-06-04 12:40:17,963] Trial 3 finished with value: 0.025114810094237328 and parameters: {'lr': 0.00027150628960319865, 'filters1': 90, 'filters2': 108, 'dense_units': 112}. Best is trial 2 with value: 0.02413373626768589.\n",
      "[I 2025-06-04 12:45:25,019] Trial 4 finished with value: 0.024040518328547478 and parameters: {'lr': 0.00039903164763366723, 'filters1': 40, 'filters2': 120, 'dense_units': 86}. Best is trial 4 with value: 0.024040518328547478.\n",
      "[I 2025-06-04 12:51:48,336] Trial 5 finished with value: 0.026274122297763824 and parameters: {'lr': 0.0003600899968770028, 'filters1': 127, 'filters2': 64, 'dense_units': 55}. Best is trial 4 with value: 0.024040518328547478.\n",
      "[I 2025-06-04 12:56:18,529] Trial 6 finished with value: 0.024050096049904823 and parameters: {'lr': 0.0073868981047018984, 'filters1': 33, 'filters2': 64, 'dense_units': 107}. Best is trial 4 with value: 0.024040518328547478.\n",
      "[I 2025-06-04 13:01:42,779] Trial 7 finished with value: 0.024151556193828583 and parameters: {'lr': 0.006637610787153421, 'filters1': 38, 'filters2': 94, 'dense_units': 52}. Best is trial 4 with value: 0.024040518328547478.\n",
      "[I 2025-06-04 13:08:16,049] Trial 8 finished with value: 0.024295583367347717 and parameters: {'lr': 0.0006883843936886504, 'filters1': 68, 'filters2': 107, 'dense_units': 81}. Best is trial 4 with value: 0.024040518328547478.\n",
      "[I 2025-06-04 13:12:59,141] Trial 9 finished with value: 0.02384713664650917 and parameters: {'lr': 0.007255324390874631, 'filters1': 46, 'filters2': 94, 'dense_units': 40}. Best is trial 9 with value: 0.02384713664650917.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.007255324390874631, 'filters1': 46, 'filters2': 94, 'dense_units': 40}\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 147ms/step\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 126ms/step\n",
      "\n",
      "========== RÉSULTATS FINAUX ==========\n",
      "          Dataset  MAE e-base  MSE e-base  R2 e-base  \\\n",
      "0  scaled_dataset    0.018043    0.000569   0.496978   \n",
      "\n",
      "   T(entrainement-e-base)[s]  Optuna best R2  \\\n",
      "0                 342.937648         0.65186   \n",
      "\n",
      "                 Paramètres optimisés  T(entrainement-optimisé)[s]  \\\n",
      "0  lr=0.00726, f1=46, f2=94, dense=40                  1440.614727   \n",
      "\n",
      "   MAE optimisé  MSE optimisé  R2 optimisé                     Graph_scatter  \\\n",
      "0      0.013232      0.000394      0.65186  plots\\scaled_dataset_scatter.png   \n",
      "\n",
      "                      Graph_hist                     Graph_courbe  \\\n",
      "0  plots\\scaled_dataset_hist.png  plots\\scaled_dataset_courbe.png   \n",
      "\n",
      "   T(evaluation-inference)[s]  Nombre d'époques e-base  \\\n",
      "0                    5.824486                       30   \n",
      "\n",
      "   Nombre d'époques optimisé  \n",
      "0                        100  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pyswarms as ps\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.layers import ConvLSTM1D, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "globalpath = \"../DataCleaning/A - CSV par bâtiment/\"\n",
    "###############################################################################\n",
    "# 1) Chargement et préparation des données\n",
    "###############################################################################\n",
    "def load_and_prepare_data(csv_path, production_column='production', window_size=24):\n",
    "    \"\"\"\n",
    "    Charge le fichier CSV, scale les données, crée des fenêtres (x,y),\n",
    "    et renvoie les splits (x_train, y_train, x_test, y_test, x_val, y_val).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Mise à l'échelle\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data_scaled = scaler.fit_transform(df.values)\n",
    "    # ➕ Scaler dédié uniquement à la colonne cible\n",
    "    target_scaler = MinMaxScaler()\n",
    "    target_scaler.fit(df[[production_column]])\n",
    "    # Retrouver l'index de la colonne cible\n",
    "    target_col_idx = df.columns.get_loc(production_column)\n",
    "    \n",
    "    # Création des fenêtres\n",
    "    x, y = [], []\n",
    "    for i in range(window_size, len(data_scaled)):\n",
    "        x.append(data_scaled[i-window_size:i])\n",
    "        y.append(data_scaled[i, target_col_idx])\n",
    "    x, y = np.array(x), np.array(y)\n",
    "    \n",
    "    # Split train/test/val\n",
    "    # Ici, 80% train, 10% test, 10% val (à adapter si besoin)\n",
    "    train_split_index = int(0.8 * len(x))\n",
    "    test_split_index  = int(0.9 * len(x))\n",
    "    \n",
    "    x_train, y_train = x[:train_split_index], y[:train_split_index]\n",
    "    x_test,  y_test  = x[train_split_index:test_split_index], y[train_split_index:test_split_index]\n",
    "    x_val,   y_val   = x[test_split_index:], y[test_split_index:]\n",
    "    \n",
    "    # Adapter la forme pour ConvLSTM1D (on insère un channel dimension)\n",
    "    x_train_conv = np.expand_dims(x_train, axis=2)\n",
    "    x_test_conv  = np.expand_dims(x_test, axis=2)\n",
    "    x_val_conv   = np.expand_dims(x_val, axis=2)\n",
    "    \n",
    "    return (x_train_conv, y_train,\n",
    "            x_test_conv,  y_test,\n",
    "            x_val_conv,   y_val,\n",
    "            df,target_scaler  )\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2) Baseline (e-base) : un simple entraînement avec des hyperparamètres fixes\n",
    "###############################################################################\n",
    "def build_baseline_model(input_shape):\n",
    "    \"\"\"\n",
    "    Construit un modèle ConvLSTM basique avec des hyperparamètres\n",
    "    fixes (par ex. 64 filtres, 64 neurones denses, lr=0.001).\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        ConvLSTM1D(filters=64, kernel_size=(1,), activation='tanh',\n",
    "                   return_sequences=True, input_shape=input_shape),\n",
    "        ConvLSTM1D(filters=64, kernel_size=(1,), activation='tanh', return_sequences=False),\n",
    "        Flatten(),\n",
    "        Dense(units=64, activation='relu'),\n",
    "        Dense(1, activation=\"linear\")\n",
    "    ], name=\"baseline_conv_lstm\")\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss=\"mae\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3) Modèle paramétrable pour PSO\n",
    "###############################################################################\n",
    "def build_convlstm_model(lr, filters1, filters2, dense_units, input_shape):\n",
    "    \"\"\"\n",
    "    Construit et compile un modèle ConvLSTM1D avec hyperparamètres modulables.\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        ConvLSTM1D(filters=int(filters1), kernel_size=(1,), activation='tanh',\n",
    "                   return_sequences=True, input_shape=input_shape),\n",
    "        ConvLSTM1D(filters=int(filters2), kernel_size=(1,), activation='tanh', return_sequences=False),\n",
    "        Flatten(),\n",
    "        Dense(units=int(dense_units), activation='relu'),\n",
    "        Dense(1, activation=\"linear\")\n",
    "    ], name=\"model_conv_lstm\")\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"mae\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "def plot_and_save_analysis(y_test, y_pred, save_dir, dataset_name,target_scaler):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    y_test = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    y_pred = target_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # 1. Scatter plot (prédictions vs réel)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.7, color='orange')\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--', label=\"Idéal (y = ŷ)\")\n",
    "    plt.xlabel(\"Valeurs réelles (y)\")\n",
    "    plt.ylabel(\"Prédictions (ŷ)\")\n",
    "    plt.title(f\"{dataset_name} - Prédictions vs Réel\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    scatter_path = os.path.join(save_dir, f\"{dataset_name}_scatter.png\")\n",
    "    plt.savefig(scatter_path)\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Histogramme des erreurs\n",
    "    errors = y_test - y_pred\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(errors, bins=20, color='orange', edgecolor='black')\n",
    "    plt.title(f\"{dataset_name} - Distribution des erreurs\")\n",
    "    plt.xlabel(\"Erreur (y - ŷ)\")\n",
    "    plt.ylabel(\"Fréquence\")\n",
    "    plt.grid(True)\n",
    "    hist_path = os.path.join(save_dir, f\"{dataset_name}_hist.png\")\n",
    "    plt.savefig(hist_path)\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Courbe temporelle\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(y_test, label=\"Valeurs réelles\", linewidth=2)\n",
    "    plt.plot(y_pred, '--', label=\"Prédictions\")\n",
    "    plt.title(f\"{dataset_name} - Évolution temporelle\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Valeur\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    curve_path = os.path.join(save_dir, f\"{dataset_name}_courbe.png\")\n",
    "    plt.savefig(curve_path)\n",
    "    plt.close()\n",
    "\n",
    "    return scatter_path, hist_path, curve_path\n",
    "###############################################################################\n",
    "# 4) Entraînement + évaluation (MAE, MSE, R²) + temps d'exécution\n",
    "###############################################################################\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_val, y_val,\n",
    "                             epochs=50, batch_size=512, verbose=0, dataset_name=\"dataset\"):\n",
    "    \"\"\"\n",
    "    Entraîne le modèle, mesure le temps d'entraînement, et renvoie l'historique.\n",
    "    \"\"\"\n",
    "    stop_early = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=verbose,\n",
    "                        callbacks=[stop_early])\n",
    "    training_time = time.time() - start_time\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(history.history['loss'], label='train_loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.title(f\"{dataset_name}Courbe d'apprentissage (loss)\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{dataset_name}_plots_loss_curve_{model.name}.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return history, training_time\n",
    "\n",
    "\n",
    "def inference_time_and_metrics(model, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Calcule le temps d'inférence, puis renvoie MAE, MSE, R².\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    preds = model.predict(x_test)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    preds = preds.reshape(-1)\n",
    "    y_test = y_test.reshape(-1)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    r2  = r2_score(y_test, preds)\n",
    "    \n",
    "    return mae, mse, r2, inference_time\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5) Fonction objectif pour OPTUNA\n",
    "###############################################################################\n",
    "\n",
    "def objective(trial, input_shape, x_train, y_train, x_val, y_val):\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)  # nouvelle syntaxe recommandée\n",
    "    f1 = trial.suggest_int('filters1', 32, 128)\n",
    "    f2 = trial.suggest_int('filters2', 32, 128)\n",
    "    dense = trial.suggest_int('dense_units', 32, 128)\n",
    "\n",
    "    model = Sequential([\n",
    "        ConvLSTM1D(filters=f1, kernel_size=(1,), activation='tanh', return_sequences=True, input_shape=input_shape),\n",
    "        ConvLSTM1D(filters=f2, kernel_size=(1,), activation='tanh', return_sequences=False),\n",
    "        Flatten(),\n",
    "        Dense(units=dense, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mae')\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=10,\n",
    "        batch_size=256,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    return min(history.history['val_loss'])\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 6) Boucle sur les datasets et compilation des résultats\n",
    "###############################################################################\n",
    "def run_experiments_on_datasets(\n",
    "    dataset_paths,\n",
    "    production_column='production',\n",
    "    window_size=24,\n",
    "    epochs_baseline=50,\n",
    "    epochs_optimized=50\n",
    "):\n",
    "    \"\"\"\n",
    "    - Pour chaque dataset :\n",
    "        1) Prépare les données\n",
    "        2) Entraîne le modèle baseline (e-base) et mesure ses métriques\n",
    "        3) Lance l'optimisation PSO\n",
    "        4) Entraîne le modèle avec les hyperparams optimisés\n",
    "        5) Mesure les métriques et temps\n",
    "        6) Stocke les résultats dans un DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for csv_path in dataset_paths:\n",
    "        dataset_name = os.path.basename(csv_path).replace('.csv','')\n",
    "        print(f\"\\n=== Dataset: {dataset_name} ===\")\n",
    "        \n",
    "        # 1) Chargement et préparation\n",
    "        x_train, y_train, x_test, y_test, x_val, y_val, df,target_scaler  = load_and_prepare_data(\n",
    "            csv_path,\n",
    "            production_column=production_column,\n",
    "            window_size=window_size\n",
    "        )\n",
    "        input_shape = x_train.shape[1:]\n",
    "        \n",
    "        # 2) Modèle baseline\n",
    "        baseline_model = build_baseline_model(input_shape)\n",
    "        history_base, t_train_base = train_and_evaluate_model(\n",
    "            baseline_model, x_train, y_train, x_val, y_val,\n",
    "            epochs=epochs_baseline, batch_size=512, verbose=0,dataset_name=dataset_name\n",
    "        )\n",
    "        mae_base, mse_base, r2_base, t_infer_base = inference_time_and_metrics(baseline_model, x_test, y_test)\n",
    "        # 3) Optuna\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: objective(trial, input_shape, x_train, y_train, x_val, y_val), n_trials=10)\n",
    "        print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "        \n",
    "        # 4) Entraîner le modèle avec les hyperparamètres optimisés\n",
    "        best_params = study.best_params\n",
    "        lr_opt        = best_params['lr']\n",
    "        filters1_opt  = best_params['filters1']\n",
    "        filters2_opt  = best_params['filters2']\n",
    "        dense_opt     = best_params['dense_units']\n",
    "\n",
    "        # Ajoute ce format dans le dictionnaire des résultats :\n",
    "        param_opt_string = f\"lr={lr_opt:.5f}, f1={filters1_opt}, f2={filters2_opt}, dense={dense_opt}\"\n",
    "        best_model = build_convlstm_model(lr_opt, filters1_opt, filters2_opt, dense_opt, input_shape)\n",
    "        history_opt, t_train_opt = train_and_evaluate_model(\n",
    "            best_model, x_train, y_train, x_val, y_val,\n",
    "            epochs=epochs_optimized, batch_size=512, verbose=0,dataset_name=dataset_name\n",
    "        )\n",
    "        \n",
    "        # 5) Évaluation finale\n",
    "        mae_opt, mse_opt, r2_opt, t_infer_opt = inference_time_and_metrics(best_model, x_test, y_test)\n",
    "        \n",
    "        scatter_path, hist_path, curve_path = plot_and_save_analysis(\n",
    "            y_test=y_test, \n",
    "            y_pred=best_model.predict(x_test).flatten(), \n",
    "            save_dir=\"plots\", \n",
    "            dataset_name=dataset_name,\n",
    "            target_scaler=target_scaler\n",
    "        )\n",
    "        \n",
    "        # 6) Stockage des résultats dans un dictionnaire\n",
    "        result_dict = {\n",
    "            \"Dataset\": dataset_name,\n",
    "            \n",
    "            # E-base\n",
    "            \"MAE e-base\": mae_base,\n",
    "            \"MSE e-base\": mse_base,\n",
    "            \"R2 e-base\":  r2_base,\n",
    "            \"T(entrainement-e-base)[s]\": t_train_base,\n",
    "            \n",
    "            \n",
    "            #Optuna\n",
    "            \"Optuna best R2\": r2_opt,\n",
    "            \"Paramètres optimisés\": f\"lr={lr_opt:.5f}, f1={filters1_opt}, f2={filters2_opt}, dense={dense_opt}\",\n",
    "            \n",
    "            # Entraînement optimisé\n",
    "            \"T(entrainement-optimisé)[s]\": t_train_opt,\n",
    "            \"MAE optimisé\": mae_opt,\n",
    "            \"MSE optimisé\": mse_opt,\n",
    "            \"R2 optimisé\": r2_opt,\n",
    "            \"Graph_scatter\": scatter_path,\n",
    "            \"Graph_hist\": hist_path,\n",
    "            \"Graph_courbe\": curve_path,\n",
    "            \n",
    "            # Inférence\n",
    "            \"T(evaluation-inference)[s]\": t_infer_opt,\n",
    "            \n",
    "            # Nombre d'époques\n",
    "            \"Nombre d'époques e-base\": epochs_baseline,\n",
    "            \"Nombre d'époques optimisé\": epochs_optimized\n",
    "        }\n",
    "        \n",
    "        results.append(result_dict)\n",
    "    \n",
    "    # Conversion en DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    return df_results\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 7) Lancement final (exemple)\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # Liste des chemins vers vos CSV\n",
    "    dataset_paths = [\n",
    "        globalpath+\"scaled_dataset.csv\",\n",
    "        #globalpath+\"batiment_1.csv\",\n",
    "        #globalpath+\"batiment_2.csv\",\n",
    "        #globalpath+\"batiment_3.csv\",\n",
    "        #globalpath+\"batiment_4.csv\",\n",
    "        #globalpath+\"batiment_5.csv\",\n",
    "        #globalpath+\"batiment_6.csv\",\n",
    "        #globalpath+\"batiment_7.csv\",\n",
    "        #globalpath+\"batiment_8.csv\",\n",
    "        #globalpath+\"batiment_9.csv\"\n",
    "    ]\n",
    "    \n",
    "    # Paramètres globaux (à adapter)\n",
    "    production_column = 'production'\n",
    "    window_size = 24      # taille des fenêtres\n",
    "    epochs_baseline = 30  # nombre d'époques pour la baseline\n",
    "    epochs_optimized = 100 # nombre d'époques pour le modèle optimisé\n",
    "    \n",
    "    # Lancement des expériences\n",
    "    df_results = run_experiments_on_datasets(\n",
    "        dataset_paths,\n",
    "        production_column=production_column,\n",
    "        window_size=window_size,\n",
    "        epochs_baseline=epochs_baseline,\n",
    "        epochs_optimized=epochs_optimized\n",
    "    )\n",
    "    \n",
    "    # Affichage des résultats finaux\n",
    "    print(\"\\n========== RÉSULTATS FINAUX ==========\")\n",
    "    print(df_results)\n",
    "    # Sauvegarde éventuellement en CSV\n",
    "    df_results.to_csv(\"resume_resultatsOptuna.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (IRpv)",
   "language": "python",
   "name": "ir_pv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
