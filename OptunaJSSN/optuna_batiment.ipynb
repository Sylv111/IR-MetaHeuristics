{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e849fc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset: scaled_dataset4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sylva\\Documents\\CESI\\A4\\IR\\GIT\\venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-10 10:46:46,455] A new study created in memory with name: no-name-95affdda-02d0-494f-892e-423abfbda182\n",
      "c:\\Users\\sylva\\Documents\\CESI\\A4\\IR\\GIT\\venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-06-10 10:47:47,788] Trial 0 finished with value: 0.02416730672121048 and parameters: {'lr': 0.001106144835335212, 'filters1': 106, 'filters2': 37, 'dense_units': 50}. Best is trial 0 with value: 0.02416730672121048.\n",
      "[I 2025-06-10 10:49:15,005] Trial 1 finished with value: 0.02532762661576271 and parameters: {'lr': 0.00017406175812507434, 'filters1': 110, 'filters2': 124, 'dense_units': 66}. Best is trial 0 with value: 0.02416730672121048.\n",
      "[I 2025-06-10 10:49:53,626] Trial 2 finished with value: 0.024864880368113518 and parameters: {'lr': 0.0004400763804112908, 'filters1': 62, 'filters2': 88, 'dense_units': 48}. Best is trial 0 with value: 0.02416730672121048.\n",
      "[I 2025-06-10 10:50:43,683] Trial 3 finished with value: 0.024723412469029427 and parameters: {'lr': 0.0023230760302014927, 'filters1': 38, 'filters2': 128, 'dense_units': 47}. Best is trial 0 with value: 0.02416730672121048.\n",
      "[I 2025-06-10 10:51:58,803] Trial 4 finished with value: 0.0239527840167284 and parameters: {'lr': 0.0004935958168199135, 'filters1': 97, 'filters2': 93, 'dense_units': 98}. Best is trial 4 with value: 0.0239527840167284.\n",
      "[I 2025-06-10 10:52:45,532] Trial 5 finished with value: 0.025071989744901657 and parameters: {'lr': 0.000494683916086827, 'filters1': 50, 'filters2': 117, 'dense_units': 102}. Best is trial 4 with value: 0.0239527840167284.\n",
      "[I 2025-06-10 10:53:15,905] Trial 6 finished with value: 0.025974368676543236 and parameters: {'lr': 0.0002625432870540569, 'filters1': 50, 'filters2': 60, 'dense_units': 117}. Best is trial 4 with value: 0.0239527840167284.\n",
      "[I 2025-06-10 10:54:28,758] Trial 7 finished with value: 0.02380971796810627 and parameters: {'lr': 0.0004948633971173967, 'filters1': 92, 'filters2': 113, 'dense_units': 93}. Best is trial 7 with value: 0.02380971796810627.\n",
      "[I 2025-06-10 10:55:21,422] Trial 8 finished with value: 0.02548454888164997 and parameters: {'lr': 0.00036430305649429325, 'filters1': 127, 'filters2': 47, 'dense_units': 123}. Best is trial 7 with value: 0.02380971796810627.\n",
      "[I 2025-06-10 10:56:37,456] Trial 9 finished with value: 0.024223174899816513 and parameters: {'lr': 0.004387620248223863, 'filters1': 122, 'filters2': 120, 'dense_units': 72}. Best is trial 7 with value: 0.02380971796810627.\n",
      "[I 2025-06-10 10:57:45,752] Trial 10 finished with value: 0.025557035580277443 and parameters: {'lr': 0.009371862403152482, 'filters1': 79, 'filters2': 102, 'dense_units': 91}. Best is trial 7 with value: 0.02380971796810627.\n",
      "[I 2025-06-10 10:59:04,139] Trial 11 finished with value: 0.02544661983847618 and parameters: {'lr': 0.00010394175968540809, 'filters1': 93, 'filters2': 84, 'dense_units': 98}. Best is trial 7 with value: 0.02380971796810627.\n",
      "[I 2025-06-10 11:00:28,402] Trial 12 finished with value: 0.02342064492404461 and parameters: {'lr': 0.0010056697171341618, 'filters1': 92, 'filters2': 97, 'dense_units': 86}. Best is trial 12 with value: 0.02342064492404461.\n",
      "[I 2025-06-10 11:01:44,348] Trial 13 finished with value: 0.024706725031137466 and parameters: {'lr': 0.0011694867756626688, 'filters1': 80, 'filters2': 106, 'dense_units': 80}. Best is trial 12 with value: 0.02342064492404461.\n",
      "[I 2025-06-10 11:02:39,837] Trial 14 finished with value: 0.02376752533018589 and parameters: {'lr': 0.00200229150458129, 'filters1': 72, 'filters2': 74, 'dense_units': 111}. Best is trial 12 with value: 0.02342064492404461.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.0010056697171341618, 'filters1': 92, 'filters2': 97, 'dense_units': 86}\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n",
      "\n",
      "=========== RÉSULTATS FINAUX ===========\n",
      "           Dataset  MAE e-base  MSE e-base  R2 e-base  \\\n",
      "0  scaled_dataset4    0.016399     0.00052   0.536148   \n",
      "\n",
      "   T(entrainement-e-base)[s]  Optuna best R2  \\\n",
      "0                 115.135759        0.613648   \n",
      "\n",
      "                 Paramètres optimisés  T(entrainement-optimisé)[s]  \\\n",
      "0  lr=0.00101, f1=92, f2=97, dense=86                   438.469768   \n",
      "\n",
      "   MAE optimisé  MSE optimisé  R2 optimisé                      Graph_scatter  \\\n",
      "0      0.014307      0.000433     0.613648  plots\\scaled_dataset4_scatter.png   \n",
      "\n",
      "                       Graph_hist                      Graph_courbe  \\\n",
      "0  plots\\scaled_dataset4_hist.png  plots\\scaled_dataset4_courbe.png   \n",
      "\n",
      "   T(evaluation-inference)[s]  Nombre d'époques e-base  \\\n",
      "0                    2.042804                       50   \n",
      "\n",
      "   Nombre d'époques optimisé  \n",
      "0                        100  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pyswarms as ps\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.layers import ConvLSTM1D, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "globalpath = \"../DataCleaning/A - CSV par bâtiment/\"\n",
    "globalpath2 = \"../DataCleaning/\"\n",
    "###############################################################################\n",
    "# 1) Chargement et préparation des données\n",
    "###############################################################################\n",
    "def load_and_prepare_data(csv_path, production_column='production', window_size=24):\n",
    "    \"\"\"\n",
    "    Charge le fichier CSV, scale les données, crée des fenêtres (x,y),\n",
    "    et renvoie les splits (x_train, y_train, x_test, y_test, x_val, y_val).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Mise à l'échelle\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data_scaled = scaler.fit_transform(df.values)\n",
    "    # ➕ Scaler dédié uniquement à la colonne cible\n",
    "    target_scaler = MinMaxScaler()\n",
    "    target_scaler.fit(df[[production_column]])\n",
    "    # Retrouver l'index de la colonne cible\n",
    "    target_col_idx = df.columns.get_loc(production_column)\n",
    "    \n",
    "    # Création des fenêtres\n",
    "    x, y = [], []\n",
    "    for i in range(window_size, len(data_scaled)):\n",
    "        x.append(data_scaled[i-window_size:i])\n",
    "        y.append(data_scaled[i, target_col_idx])\n",
    "    x, y = np.array(x), np.array(y)\n",
    "    \n",
    "    # Split train/test/val\n",
    "    # Ici, 80% train, 10% test, 10% val (à adapter si besoin)\n",
    "    train_split_index = int(0.8 * len(x))\n",
    "    test_split_index  = int(0.9 * len(x))\n",
    "    \n",
    "    x_train, y_train = x[:train_split_index], y[:train_split_index]\n",
    "    x_test,  y_test  = x[train_split_index:test_split_index], y[train_split_index:test_split_index]\n",
    "    x_val,   y_val   = x[test_split_index:], y[test_split_index:]\n",
    "    \n",
    "    # Adapter la forme pour ConvLSTM1D (on insère un channel dimension)\n",
    "    x_train_conv = np.expand_dims(x_train, axis=2)\n",
    "    x_test_conv  = np.expand_dims(x_test, axis=2)\n",
    "    x_val_conv   = np.expand_dims(x_val, axis=2)\n",
    "    \n",
    "    return (x_train_conv, y_train,\n",
    "            x_test_conv,  y_test,\n",
    "            x_val_conv,   y_val,\n",
    "            df,target_scaler  )\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2) Baseline (e-base) : un simple entraînement avec des hyperparamètres fixes\n",
    "###############################################################################\n",
    "def build_baseline_model(input_shape):\n",
    "    \"\"\"\n",
    "    Construit un modèle ConvLSTM basique avec des hyperparamètres\n",
    "    fixes (par ex. 64 filtres, 64 neurones denses, lr=0.001).\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        ConvLSTM1D(filters=64, kernel_size=(1,), activation='tanh',\n",
    "                   return_sequences=True, input_shape=input_shape),\n",
    "        ConvLSTM1D(filters=64, kernel_size=(1,), activation='tanh', return_sequences=False),\n",
    "        Flatten(),\n",
    "        Dense(units=64, activation='relu'),\n",
    "        Dense(1, activation=\"linear\")\n",
    "    ], name=\"baseline_conv_lstm\")\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss=\"mae\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3) Modèle paramétrable pour PSO\n",
    "###############################################################################\n",
    "def build_convlstm_model(lr, filters1, filters2, dense_units, input_shape):\n",
    "    \"\"\"\n",
    "    Construit et compile un modèle ConvLSTM1D avec hyperparamètres modulables.\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        ConvLSTM1D(filters=int(filters1), kernel_size=(1,), activation='tanh',\n",
    "                   return_sequences=True, input_shape=input_shape),\n",
    "        ConvLSTM1D(filters=int(filters2), kernel_size=(1,), activation='tanh', return_sequences=False),\n",
    "        Flatten(),\n",
    "        Dense(units=int(dense_units), activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation=\"linear\")\n",
    "    ], name=\"model_conv_lstm\")\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"mae\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "def plot_and_save_analysis(y_test, y_pred, save_dir, dataset_name,target_scaler):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    y_test = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    y_pred = target_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # 1. Scatter plot (prédictions vs réel)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.7, color='orange')\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--', label=\"Idéal (y = ŷ)\")\n",
    "    plt.xlabel(\"Valeurs réelles (y)\")\n",
    "    plt.ylabel(\"Prédictions (ŷ)\")\n",
    "    plt.title(f\"{dataset_name} - Prédictions vs Réel\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    scatter_path = os.path.join(save_dir, f\"{dataset_name}_scatter.png\")\n",
    "    plt.savefig(scatter_path)\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Histogramme des erreurs\n",
    "    errors = y_test - y_pred\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(errors, bins=20, color='orange', edgecolor='black')\n",
    "    plt.title(f\"{dataset_name} - Distribution des erreurs\")\n",
    "    plt.xlabel(\"Erreur (y - ŷ)\")\n",
    "    plt.ylabel(\"Fréquence\")\n",
    "    plt.grid(True)\n",
    "    hist_path = os.path.join(save_dir, f\"{dataset_name}_hist.png\")\n",
    "    plt.savefig(hist_path)\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Courbe temporelle\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(y_test, label=\"Valeurs réelles\", linewidth=2)\n",
    "    plt.plot(y_pred, '--', label=\"Prédictions\")\n",
    "    plt.title(f\"{dataset_name} - Évolution temporelle\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Valeur\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    curve_path = os.path.join(save_dir, f\"{dataset_name}_courbe.png\")\n",
    "    plt.savefig(curve_path)\n",
    "    plt.close()\n",
    "\n",
    "    return scatter_path, hist_path, curve_path\n",
    "###############################################################################\n",
    "# 4) Entraînement + évaluation (MAE, MSE, R²) + temps d'exécution\n",
    "###############################################################################\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_val, y_val,\n",
    "                             epochs=50, batch_size=512, verbose=0, dataset_name=\"dataset\"):\n",
    "    \"\"\"\n",
    "    Entraîne le modèle, mesure le temps d'entraînement, et renvoie l'historique.\n",
    "    \"\"\"\n",
    "    stop_early = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=verbose,\n",
    "                        callbacks=[stop_early])\n",
    "    training_time = time.time() - start_time\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(history.history['loss'], label='train_loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.title(f\"{dataset_name}Courbe d'apprentissage (loss)\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{dataset_name}_plots_loss_curve_{model.name}.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return history, training_time\n",
    "\n",
    "\n",
    "def inference_time_and_metrics(model, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Calcule le temps d'inférence, puis renvoie MAE, MSE, R².\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    preds = model.predict(x_test)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    preds = preds.reshape(-1)\n",
    "    y_test = y_test.reshape(-1)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    r2  = r2_score(y_test, preds)\n",
    "    \n",
    "    return mae, mse, r2, inference_time\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5) Fonction objectif pour OPTUNA\n",
    "###############################################################################\n",
    "\n",
    "def objective(trial, input_shape, x_train, y_train, x_val, y_val):\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)  # nouvelle syntaxe recommandée\n",
    "    f1 = trial.suggest_int('filters1', 32, 128)\n",
    "    f2 = trial.suggest_int('filters2', 32, 128)\n",
    "    dense = trial.suggest_int('dense_units', 32, 128)\n",
    "\n",
    "    model = Sequential([\n",
    "        ConvLSTM1D(filters=f1, kernel_size=(1,), activation='tanh', return_sequences=True, input_shape=input_shape),\n",
    "        ConvLSTM1D(filters=f2, kernel_size=(1,), activation='tanh', return_sequences=False),\n",
    "        Flatten(),\n",
    "        Dense(units=dense, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mae')\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=10,\n",
    "        batch_size=256,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    return min(history.history['val_loss'])\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 6) Boucle sur les datasets et compilation des résultats\n",
    "###############################################################################\n",
    "def run_experiments_on_datasets(\n",
    "    dataset_paths,\n",
    "    production_column='production',\n",
    "    window_size=24,\n",
    "    epochs_baseline=50,\n",
    "    epochs_optimized=50\n",
    "):\n",
    "    \"\"\"\n",
    "    - Pour chaque dataset :\n",
    "        1) Prépare les données\n",
    "        2) Entraîne le modèle baseline (e-base) et mesure ses métriques\n",
    "        3) Lance l'optimisation PSO\n",
    "        4) Entraîne le modèle avec les hyperparams optimisés\n",
    "        5) Mesure les métriques et temps\n",
    "        6) Stocke les résultats dans un DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for csv_path in dataset_paths:\n",
    "        dataset_name = os.path.basename(csv_path).replace('.csv','')\n",
    "        print(f\"\\n=== Dataset: {dataset_name} ===\")\n",
    "        \n",
    "        # 1) Chargement et préparation\n",
    "        x_train, y_train, x_test, y_test, x_val, y_val, df,target_scaler  = load_and_prepare_data(\n",
    "            csv_path,\n",
    "            production_column=production_column,\n",
    "            window_size=window_size\n",
    "        )\n",
    "        input_shape = x_train.shape[1:]\n",
    "        \n",
    "        # 2) Modèle baseline\n",
    "        baseline_model = build_baseline_model(input_shape)\n",
    "        history_base, t_train_base = train_and_evaluate_model(\n",
    "            baseline_model, x_train, y_train, x_val, y_val,\n",
    "            epochs=epochs_baseline, batch_size=512, verbose=0,dataset_name=dataset_name\n",
    "        )\n",
    "        mae_base, mse_base, r2_base, t_infer_base = inference_time_and_metrics(baseline_model, x_test, y_test)\n",
    "        # 3) Optuna\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: objective(trial, input_shape, x_train, y_train, x_val, y_val), n_trials=15)\n",
    "        print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "        \n",
    "        # 4) Entraîner le modèle avec les hyperparamètres optimisés\n",
    "        best_params = study.best_params\n",
    "        lr_opt        = best_params['lr']\n",
    "        filters1_opt  = best_params['filters1']\n",
    "        filters2_opt  = best_params['filters2']\n",
    "        dense_opt     = best_params['dense_units']\n",
    "\n",
    "        # Ajoute ce format dans le dictionnaire des résultats :\n",
    "        param_opt_string = f\"lr={lr_opt:.5f}, f1={filters1_opt}, f2={filters2_opt}, dense={dense_opt}\"\n",
    "        best_model = build_convlstm_model(lr_opt, filters1_opt, filters2_opt, dense_opt, input_shape)\n",
    "        history_opt, t_train_opt = train_and_evaluate_model(\n",
    "            best_model, x_train, y_train, x_val, y_val,\n",
    "            epochs=epochs_optimized, batch_size=512, verbose=0,dataset_name=dataset_name\n",
    "        )\n",
    "        \n",
    "        # 5) Évaluation finale\n",
    "        mae_opt, mse_opt, r2_opt, t_infer_opt = inference_time_and_metrics(best_model, x_test, y_test)\n",
    "        \n",
    "        scatter_path, hist_path, curve_path = plot_and_save_analysis(\n",
    "            y_test=y_test, \n",
    "            y_pred=best_model.predict(x_test).flatten(), \n",
    "            save_dir=\"plots\", \n",
    "            dataset_name=dataset_name,\n",
    "            target_scaler=target_scaler\n",
    "        )\n",
    "        \n",
    "        # 6) Stockage des résultats dans un dictionnaire\n",
    "        result_dict = {\n",
    "            \"Dataset\": dataset_name,\n",
    "            \n",
    "            # E-base\n",
    "            \"MAE e-base\": mae_base,\n",
    "            \"MSE e-base\": mse_base,\n",
    "            \"R2 e-base\":  r2_base,\n",
    "            \"T(entrainement-e-base)[s]\": t_train_base,\n",
    "            \n",
    "            \n",
    "            #Optuna\n",
    "            \"Optuna best R2\": r2_opt,\n",
    "            \"Paramètres optimisés\": f\"lr={lr_opt:.5f}, f1={filters1_opt}, f2={filters2_opt}, dense={dense_opt}\",\n",
    "            \n",
    "            # Entraînement optimisé\n",
    "            \"T(entrainement-optimisé)[s]\": t_train_opt,\n",
    "            \"MAE optimisé\": mae_opt,\n",
    "            \"MSE optimisé\": mse_opt,\n",
    "            \"R2 optimisé\": r2_opt,\n",
    "            \"Graph_scatter\": scatter_path,\n",
    "            \"Graph_hist\": hist_path,\n",
    "            \"Graph_courbe\": curve_path,\n",
    "            \n",
    "            # Inférence\n",
    "            \"T(evaluation-inference)[s]\": t_infer_opt,\n",
    "            \n",
    "            # Nombre d'époques\n",
    "            \"Nombre d'époques e-base\": epochs_baseline,\n",
    "            \"Nombre d'époques optimisé\": epochs_optimized\n",
    "        }\n",
    "        \n",
    "        results.append(result_dict)\n",
    "    \n",
    "    # Conversion en DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    return df_results\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 7) Lancement final (exemple)\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # Liste des chemins vers vos CSV\n",
    "    dataset_paths = [\n",
    "        \"../DataCleaning/scaled_dataset4.csv\",\n",
    "    ]\n",
    "    \n",
    "    # Paramètres globaux (à adapter)\n",
    "    production_column = 'production'\n",
    "    window_size = 24      # taille des fenêtres\n",
    "    epochs_baseline = 50  # nombre d'époques pour la baseline\n",
    "    epochs_optimized = 100 # nombre d'époques pour le modèle optimisé\n",
    "    \n",
    "    # Lancement des expériences\n",
    "    df_results = run_experiments_on_datasets(\n",
    "        dataset_paths,\n",
    "        production_column=production_column,\n",
    "        window_size=window_size,\n",
    "        epochs_baseline=epochs_baseline,\n",
    "        epochs_optimized=epochs_optimized\n",
    "    )\n",
    "    \n",
    "    # Affichage des résultats finaux\n",
    "    print(\"\\n=========== RÉSULTATS FINAUX ===========\")\n",
    "    print(df_results)\n",
    "    # Sauvegarde éventuellement en CSV\n",
    "    df_results.to_csv(\"resume_resultatsOptuna4.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
