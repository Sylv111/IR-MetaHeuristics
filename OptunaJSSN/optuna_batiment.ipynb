{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e849fc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sylva\\Documents\\CESI\\A4\\IR\\GIT\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\sylva\\Documents\\CESI\\A4\\IR\\GIT\\venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset: scaled_dataset ===\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-10 13:35:08,840] A new study created in memory with name: no-name-6ccbb042-e6cd-4bab-b167-b0cab2ef14f1\n",
      "c:\\Users\\sylva\\Documents\\CESI\\A4\\IR\\GIT\\venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-06-10 13:35:40,845] Trial 0 finished with value: 0.02337801456451416 and parameters: {'lr': 0.0003815406039447927, 'filters1': 52, 'filters2': 95, 'dense_units': 65}. Best is trial 0 with value: 0.02337801456451416.\n",
      "[I 2025-06-10 13:36:12,534] Trial 1 finished with value: 0.02366761676967144 and parameters: {'lr': 0.0005015502111783601, 'filters1': 58, 'filters2': 94, 'dense_units': 52}. Best is trial 0 with value: 0.02337801456451416.\n",
      "[I 2025-06-10 13:36:57,812] Trial 2 finished with value: 0.025180870667099953 and parameters: {'lr': 0.00012279797846034127, 'filters1': 84, 'filters2': 104, 'dense_units': 51}. Best is trial 0 with value: 0.02337801456451416.\n",
      "[I 2025-06-10 13:37:16,806] Trial 3 finished with value: 0.024755174294114113 and parameters: {'lr': 0.0015952758607574078, 'filters1': 45, 'filters2': 67, 'dense_units': 48}. Best is trial 0 with value: 0.02337801456451416.\n",
      "[I 2025-06-10 13:37:34,749] Trial 4 finished with value: 0.022562868893146515 and parameters: {'lr': 0.002716426997111817, 'filters1': 37, 'filters2': 57, 'dense_units': 104}. Best is trial 4 with value: 0.022562868893146515.\n",
      "[I 2025-06-10 13:37:50,721] Trial 5 finished with value: 0.024857934564352036 and parameters: {'lr': 0.0007102514969112363, 'filters1': 35, 'filters2': 39, 'dense_units': 110}. Best is trial 4 with value: 0.022562868893146515.\n",
      "[I 2025-06-10 13:38:42,978] Trial 6 finished with value: 0.02355317212641239 and parameters: {'lr': 0.0011770932535201716, 'filters1': 118, 'filters2': 97, 'dense_units': 123}. Best is trial 4 with value: 0.022562868893146515.\n",
      "[I 2025-06-10 13:39:07,205] Trial 7 finished with value: 0.024193497374653816 and parameters: {'lr': 0.0008685694503587529, 'filters1': 85, 'filters2': 71, 'dense_units': 47}. Best is trial 4 with value: 0.022562868893146515.\n",
      "[I 2025-06-10 13:39:48,569] Trial 8 finished with value: 0.023061653599143028 and parameters: {'lr': 0.00834940951293462, 'filters1': 68, 'filters2': 113, 'dense_units': 124}. Best is trial 4 with value: 0.022562868893146515.\n",
      "[I 2025-06-10 13:40:22,107] Trial 9 finished with value: 0.021454786881804466 and parameters: {'lr': 0.0005712570389221931, 'filters1': 100, 'filters2': 67, 'dense_units': 119}. Best is trial 9 with value: 0.021454786881804466.\n",
      "[I 2025-06-10 13:41:24,522] Trial 10 finished with value: 0.02460257150232792 and parameters: {'lr': 0.00017285720480243792, 'filters1': 115, 'filters2': 128, 'dense_units': 85}. Best is trial 9 with value: 0.021454786881804466.\n",
      "[I 2025-06-10 13:41:55,274] Trial 11 finished with value: 0.0226457379758358 and parameters: {'lr': 0.004345565328730744, 'filters1': 100, 'filters2': 48, 'dense_units': 98}. Best is trial 9 with value: 0.021454786881804466.\n",
      "[I 2025-06-10 13:42:26,088] Trial 12 finished with value: 0.02262098900973797 and parameters: {'lr': 0.0022506970613078974, 'filters1': 100, 'filters2': 56, 'dense_units': 102}. Best is trial 9 with value: 0.021454786881804466.\n",
      "[I 2025-06-10 13:43:06,746] Trial 13 finished with value: 0.023398382589221 and parameters: {'lr': 0.0033036063566119194, 'filters1': 101, 'filters2': 77, 'dense_units': 88}. Best is trial 9 with value: 0.021454786881804466.\n",
      "[I 2025-06-10 13:43:24,480] Trial 14 finished with value: 0.024622127413749695 and parameters: {'lr': 0.00032946683447575775, 'filters1': 72, 'filters2': 32, 'dense_units': 113}. Best is trial 9 with value: 0.021454786881804466.\n",
      "[I 2025-06-10 13:44:03,546] Trial 15 finished with value: 0.025041772052645683 and parameters: {'lr': 0.007876479025622277, 'filters1': 127, 'filters2': 56, 'dense_units': 96}. Best is trial 9 with value: 0.021454786881804466.\n",
      "[I 2025-06-10 13:44:22,207] Trial 16 finished with value: 0.023421604186296463 and parameters: {'lr': 0.0020173614139179516, 'filters1': 34, 'filters2': 62, 'dense_units': 71}. Best is trial 9 with value: 0.021454786881804466.\n",
      "[I 2025-06-10 13:45:02,256] Trial 17 finished with value: 0.024133870378136635 and parameters: {'lr': 0.00023904028817850996, 'filters1': 92, 'filters2': 83, 'dense_units': 111}. Best is trial 9 with value: 0.021454786881804466.\n",
      "[I 2025-06-10 13:45:22,512] Trial 18 finished with value: 0.02319636382162571 and parameters: {'lr': 0.004231915233836095, 'filters1': 69, 'filters2': 42, 'dense_units': 33}. Best is trial 9 with value: 0.021454786881804466.\n",
      "[I 2025-06-10 13:46:08,616] Trial 19 finished with value: 0.02357528544962406 and parameters: {'lr': 0.0006791063657011204, 'filters1': 113, 'filters2': 84, 'dense_units': 128}. Best is trial 9 with value: 0.021454786881804466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.0005712570389221931, 'filters1': 100, 'filters2': 67, 'dense_units': 119}\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "=========== RÉSULTATS FINAUX ===========\n",
      "          Dataset  MAE e-base  MSE e-base  R2 e-base  \\\n",
      "0  scaled_dataset    0.017056    0.000566   0.495293   \n",
      "\n",
      "   T(entrainement-e-base)[s]  Optuna best R2  \\\n",
      "0                  59.991621        0.559887   \n",
      "\n",
      "                   Paramètres optimisés  T(entrainement-optimisé)[s]  \\\n",
      "0  lr=0.00057, f1=100, f2=67, dense=119                    193.08858   \n",
      "\n",
      "   MAE optimisé  MSE optimisé  R2 optimisé                     Graph_scatter  \\\n",
      "0      0.014969      0.000494     0.559887  plots\\scaled_dataset_scatter.png   \n",
      "\n",
      "                      Graph_hist                     Graph_courbe  \\\n",
      "0  plots\\scaled_dataset_hist.png  plots\\scaled_dataset_courbe.png   \n",
      "\n",
      "   T(evaluation-inference)[s]  Nombre d'époques e-base  \\\n",
      "0                    0.958457                       50   \n",
      "\n",
      "   Nombre d'époques optimisé  \n",
      "0                        100  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pyswarms as ps\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.layers import ConvLSTM1D, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "globalpath = \"../DataCleaning/A - CSV par bâtiment/\"\n",
    "globalpath2 = \"../DataCleaning/\"\n",
    "###############################################################################\n",
    "# 1) Chargement et préparation des données\n",
    "###############################################################################\n",
    "def load_and_prepare_data(csv_path, production_column='production', window_size=24):\n",
    "    \"\"\"\n",
    "    Charge le fichier CSV, scale les données, crée des fenêtres (x,y),\n",
    "    et renvoie les splits (x_train, y_train, x_test, y_test, x_val, y_val).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Mise à l'échelle\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data_scaled = scaler.fit_transform(df.values)\n",
    "    # ➕ Scaler dédié uniquement à la colonne cible\n",
    "    target_scaler = MinMaxScaler()\n",
    "    target_scaler.fit(df[[production_column]])\n",
    "    # Retrouver l'index de la colonne cible\n",
    "    target_col_idx = df.columns.get_loc(production_column)\n",
    "    \n",
    "    # Création des fenêtres\n",
    "    x, y = [], []\n",
    "    for i in range(window_size, len(data_scaled)):\n",
    "        x.append(data_scaled[i-window_size:i])\n",
    "        y.append(data_scaled[i, target_col_idx])\n",
    "    x, y = np.array(x), np.array(y)\n",
    "    \n",
    "    # Split train/test/val\n",
    "    # Ici, 80% train, 10% test, 10% val (à adapter si besoin)\n",
    "    train_split_index = int(0.8 * len(x))\n",
    "    test_split_index  = int(0.9 * len(x))\n",
    "    \n",
    "    x_train, y_train = x[:train_split_index], y[:train_split_index]\n",
    "    x_test,  y_test  = x[train_split_index:test_split_index], y[train_split_index:test_split_index]\n",
    "    x_val,   y_val   = x[test_split_index:], y[test_split_index:]\n",
    "    \n",
    "    # Adapter la forme pour ConvLSTM1D (on insère un channel dimension)\n",
    "    x_train_conv = np.expand_dims(x_train, axis=2)\n",
    "    x_test_conv  = np.expand_dims(x_test, axis=2)\n",
    "    x_val_conv   = np.expand_dims(x_val, axis=2)\n",
    "    \n",
    "    return (x_train_conv, y_train,\n",
    "            x_test_conv,  y_test,\n",
    "            x_val_conv,   y_val,\n",
    "            df,target_scaler  )\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2) Baseline (e-base) : un simple entraînement avec des hyperparamètres fixes\n",
    "###############################################################################\n",
    "def build_baseline_model(input_shape):\n",
    "    \"\"\"\n",
    "    Construit un modèle ConvLSTM basique avec des hyperparamètres\n",
    "    fixes (par ex. 64 filtres, 64 neurones denses, lr=0.001).\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        ConvLSTM1D(filters=64, kernel_size=(1,), activation='tanh',\n",
    "                   return_sequences=True, input_shape=input_shape),\n",
    "        ConvLSTM1D(filters=64, kernel_size=(1,), activation='tanh', return_sequences=False),\n",
    "        Flatten(),\n",
    "        Dense(units=64, activation='relu'),\n",
    "        Dense(1, activation=\"linear\")\n",
    "    ], name=\"baseline_conv_lstm\")\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss=\"mae\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3) Modèle paramétrable pour PSO\n",
    "###############################################################################\n",
    "def build_convlstm_model(lr, filters1, filters2, dense_units, input_shape):\n",
    "    \"\"\"\n",
    "    Construit et compile un modèle ConvLSTM1D avec hyperparamètres modulables.\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        ConvLSTM1D(filters=int(filters1), kernel_size=(1,), activation='tanh',\n",
    "                   return_sequences=True, input_shape=input_shape),\n",
    "        ConvLSTM1D(filters=int(filters2), kernel_size=(1,), activation='tanh', return_sequences=False),\n",
    "        Flatten(),\n",
    "        Dense(units=int(dense_units), activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation=\"linear\")\n",
    "    ], name=\"model_conv_lstm\")\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"mae\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "def plot_and_save_analysis(y_test, y_pred, save_dir, dataset_name,target_scaler):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    y_test = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    y_pred = target_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # 1. Scatter plot (prédictions vs réel)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.7, color='orange')\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--', label=\"Idéal (y = ŷ)\")\n",
    "    plt.xlabel(\"Valeurs réelles (y)\")\n",
    "    plt.ylabel(\"Prédictions (ŷ)\")\n",
    "    plt.title(f\"{dataset_name} - Prédictions vs Réel\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    scatter_path = os.path.join(save_dir, f\"{dataset_name}_scatter.png\")\n",
    "    plt.savefig(scatter_path)\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Histogramme des erreurs\n",
    "    errors = y_test - y_pred\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(errors, bins=20, color='orange', edgecolor='black')\n",
    "    plt.title(f\"{dataset_name} - Distribution des erreurs\")\n",
    "    plt.xlabel(\"Erreur (y - ŷ)\")\n",
    "    plt.ylabel(\"Fréquence\")\n",
    "    plt.grid(True)\n",
    "    hist_path = os.path.join(save_dir, f\"{dataset_name}_hist.png\")\n",
    "    plt.savefig(hist_path)\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Courbe temporelle\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(y_test, label=\"Valeurs réelles\", linewidth=2)\n",
    "    plt.plot(y_pred, '--', label=\"Prédictions\")\n",
    "    plt.title(f\"{dataset_name} - Évolution temporelle\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Valeur\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    curve_path = os.path.join(save_dir, f\"{dataset_name}_courbe.png\")\n",
    "    plt.savefig(curve_path)\n",
    "    plt.close()\n",
    "\n",
    "    return scatter_path, hist_path, curve_path\n",
    "###############################################################################\n",
    "# 4) Entraînement + évaluation (MAE, MSE, R²) + temps d'exécution\n",
    "###############################################################################\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_val, y_val,\n",
    "                             epochs=50, batch_size=512, verbose=0, dataset_name=\"dataset\"):\n",
    "    \"\"\"\n",
    "    Entraîne le modèle, mesure le temps d'entraînement, et renvoie l'historique.\n",
    "    \"\"\"\n",
    "    stop_early = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=verbose,\n",
    "                        callbacks=[stop_early])\n",
    "    training_time = time.time() - start_time\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(history.history['loss'], label='train_loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.title(f\"{dataset_name}Courbe d'apprentissage (loss)\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{dataset_name}_plots_loss_curve_{model.name}.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return history, training_time\n",
    "\n",
    "\n",
    "def inference_time_and_metrics(model, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Calcule le temps d'inférence, puis renvoie MAE, MSE, R².\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    preds = model.predict(x_test)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    preds = preds.reshape(-1)\n",
    "    y_test = y_test.reshape(-1)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    r2  = r2_score(y_test, preds)\n",
    "    \n",
    "    return mae, mse, r2, inference_time\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5) Fonction objectif pour OPTUNA\n",
    "###############################################################################\n",
    "\n",
    "def objective(trial, input_shape, x_train, y_train, x_val, y_val):\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)  # nouvelle syntaxe recommandée\n",
    "    f1 = trial.suggest_int('filters1', 32, 128)\n",
    "    f2 = trial.suggest_int('filters2', 32, 128)\n",
    "    dense = trial.suggest_int('dense_units', 32, 128)\n",
    "\n",
    "    model = Sequential([\n",
    "        ConvLSTM1D(filters=f1, kernel_size=(1,), activation='tanh', return_sequences=True, input_shape=input_shape),\n",
    "        ConvLSTM1D(filters=f2, kernel_size=(1,), activation='tanh', return_sequences=False),\n",
    "        Flatten(),\n",
    "        Dense(units=dense, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mae')\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=10,\n",
    "        batch_size=256,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    return min(history.history['val_loss'])\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 6) Boucle sur les datasets et compilation des résultats\n",
    "###############################################################################\n",
    "def run_experiments_on_datasets(\n",
    "    dataset_paths,\n",
    "    production_column='production',\n",
    "    window_size=24,\n",
    "    epochs_baseline=50,\n",
    "    epochs_optimized=50\n",
    "):\n",
    "    \"\"\"\n",
    "    - Pour chaque dataset :\n",
    "        1) Prépare les données\n",
    "        2) Entraîne le modèle baseline (e-base) et mesure ses métriques\n",
    "        3) Lance l'optimisation PSO\n",
    "        4) Entraîne le modèle avec les hyperparams optimisés\n",
    "        5) Mesure les métriques et temps\n",
    "        6) Stocke les résultats dans un DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for csv_path in dataset_paths:\n",
    "        dataset_name = os.path.basename(csv_path).replace('.csv','')\n",
    "        print(f\"\\n=== Dataset: {dataset_name} ===\")\n",
    "        \n",
    "        # 1) Chargement et préparation\n",
    "        x_train, y_train, x_test, y_test, x_val, y_val, df,target_scaler  = load_and_prepare_data(\n",
    "            csv_path,\n",
    "            production_column=production_column,\n",
    "            window_size=window_size\n",
    "        )\n",
    "        input_shape = x_train.shape[1:]\n",
    "        \n",
    "        # 2) Modèle baseline\n",
    "        baseline_model = build_baseline_model(input_shape)\n",
    "        history_base, t_train_base = train_and_evaluate_model(\n",
    "            baseline_model, x_train, y_train, x_val, y_val,\n",
    "            epochs=epochs_baseline, batch_size=512, verbose=0,dataset_name=dataset_name\n",
    "        )\n",
    "        mae_base, mse_base, r2_base, t_infer_base = inference_time_and_metrics(baseline_model, x_test, y_test)\n",
    "        # 3) Optuna\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: objective(trial, input_shape, x_train, y_train, x_val, y_val), n_trials=20)\n",
    "        print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "        \n",
    "        # 4) Entraîner le modèle avec les hyperparamètres optimisés\n",
    "        best_params = study.best_params\n",
    "        lr_opt        = best_params['lr']\n",
    "        filters1_opt  = best_params['filters1']\n",
    "        filters2_opt  = best_params['filters2']\n",
    "        dense_opt     = best_params['dense_units']\n",
    "\n",
    "        # Ajoute ce format dans le dictionnaire des résultats :\n",
    "        param_opt_string = f\"lr={lr_opt:.5f}, f1={filters1_opt}, f2={filters2_opt}, dense={dense_opt}\"\n",
    "        best_model = build_convlstm_model(lr_opt, filters1_opt, filters2_opt, dense_opt, input_shape)\n",
    "        history_opt, t_train_opt = train_and_evaluate_model(\n",
    "            best_model, x_train, y_train, x_val, y_val,\n",
    "            epochs=epochs_optimized, batch_size=512, verbose=0,dataset_name=dataset_name\n",
    "        )\n",
    "        \n",
    "        # 5) Évaluation finale\n",
    "        mae_opt, mse_opt, r2_opt, t_infer_opt = inference_time_and_metrics(best_model, x_test, y_test)\n",
    "        \n",
    "        scatter_path, hist_path, curve_path = plot_and_save_analysis(\n",
    "            y_test=y_test, \n",
    "            y_pred=best_model.predict(x_test).flatten(), \n",
    "            save_dir=\"plots\", \n",
    "            dataset_name=dataset_name,\n",
    "            target_scaler=target_scaler\n",
    "        )\n",
    "        \n",
    "        # 6) Stockage des résultats dans un dictionnaire\n",
    "        result_dict = {\n",
    "            \"Dataset\": dataset_name,\n",
    "            \n",
    "            # E-base\n",
    "            \"MAE e-base\": mae_base,\n",
    "            \"MSE e-base\": mse_base,\n",
    "            \"R2 e-base\":  r2_base,\n",
    "            \"T(entrainement-e-base)[s]\": t_train_base,\n",
    "            \n",
    "            \n",
    "            #Optuna\n",
    "            \"Optuna best R2\": r2_opt,\n",
    "            \"Paramètres optimisés\": f\"lr={lr_opt:.5f}, f1={filters1_opt}, f2={filters2_opt}, dense={dense_opt}\",\n",
    "            \n",
    "            # Entraînement optimisé\n",
    "            \"T(entrainement-optimisé)[s]\": t_train_opt,\n",
    "            \"MAE optimisé\": mae_opt,\n",
    "            \"MSE optimisé\": mse_opt,\n",
    "            \"R2 optimisé\": r2_opt,\n",
    "            \"Graph_scatter\": scatter_path,\n",
    "            \"Graph_hist\": hist_path,\n",
    "            \"Graph_courbe\": curve_path,\n",
    "            \n",
    "            # Inférence\n",
    "            \"T(evaluation-inference)[s]\": t_infer_opt,\n",
    "            \n",
    "            # Nombre d'époques\n",
    "            \"Nombre d'époques e-base\": epochs_baseline,\n",
    "            \"Nombre d'époques optimisé\": epochs_optimized\n",
    "        }\n",
    "        \n",
    "        results.append(result_dict)\n",
    "    \n",
    "    # Conversion en DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    return df_results\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 7) Lancement final (exemple)\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # Liste des chemins vers vos CSV\n",
    "    dataset_paths = [\n",
    "        globalpath2+\"scaled_dataset.csv\",\n",
    "    ]\n",
    "    \n",
    "    # Paramètres globaux (à adapter)\n",
    "    production_column = 'production'\n",
    "    window_size = 24      # taille des fenêtres\n",
    "    epochs_baseline = 50  # nombre d'époques pour la baseline\n",
    "    epochs_optimized = 100 # nombre d'époques pour le modèle optimisé\n",
    "    \n",
    "    # Lancement des expériences\n",
    "    df_results = run_experiments_on_datasets(\n",
    "        dataset_paths,\n",
    "        production_column=production_column,\n",
    "        window_size=window_size,\n",
    "        epochs_baseline=epochs_baseline,\n",
    "        epochs_optimized=epochs_optimized\n",
    "    )\n",
    "    \n",
    "    # Affichage des résultats finaux\n",
    "    print(\"\\n=========== RÉSULTATS FINAUX ===========\")\n",
    "    print(df_results)\n",
    "    # Sauvegarde éventuellement en CSV\n",
    "    df_results.to_csv(\"resume_resultatsOptuna.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
