{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e849fc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset: batiment_1_Clean3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sylva\\Documents\\CESI\\A4\\IR\\GIT\\venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000182EE80EDE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 09:14:35,085 - tensorflow - WARNING - 5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000182EE80EDE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-06 09:14:35,360] A new study created in memory with name: no-name-3628d952-0d28-49eb-87b6-d086ac9f2f9b\n",
      "c:\\Users\\sylva\\Documents\\CESI\\A4\\IR\\GIT\\venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-06-06 09:14:39,360] Trial 0 finished with value: 0.23865512013435364 and parameters: {'lr': 0.0007669888399502756, 'filters1': 80, 'filters2': 53, 'dense_units': 91}. Best is trial 0 with value: 0.23865512013435364.\n",
      "[I 2025-06-06 09:14:45,950] Trial 1 finished with value: 0.23798026144504547 and parameters: {'lr': 0.00015675154198350894, 'filters1': 107, 'filters2': 106, 'dense_units': 86}. Best is trial 1 with value: 0.23798026144504547.\n",
      "[I 2025-06-06 09:14:50,950] Trial 2 finished with value: 0.22093282639980316 and parameters: {'lr': 0.00898010217593802, 'filters1': 85, 'filters2': 102, 'dense_units': 55}. Best is trial 2 with value: 0.22093282639980316.\n",
      "[I 2025-06-06 09:14:56,534] Trial 3 finished with value: 0.2335534244775772 and parameters: {'lr': 0.0006618400281989179, 'filters1': 118, 'filters2': 77, 'dense_units': 86}. Best is trial 2 with value: 0.22093282639980316.\n",
      "[I 2025-06-06 09:15:00,843] Trial 4 finished with value: 0.23445947468280792 and parameters: {'lr': 0.0006359249382559794, 'filters1': 43, 'filters2': 68, 'dense_units': 106}. Best is trial 2 with value: 0.22093282639980316.\n",
      "[I 2025-06-06 09:15:05,332] Trial 5 finished with value: 0.2389809489250183 and parameters: {'lr': 0.00010987577723765428, 'filters1': 55, 'filters2': 59, 'dense_units': 60}. Best is trial 2 with value: 0.22093282639980316.\n",
      "[I 2025-06-06 09:15:09,636] Trial 6 finished with value: 0.2363516390323639 and parameters: {'lr': 0.0006917705885378364, 'filters1': 49, 'filters2': 57, 'dense_units': 67}. Best is trial 2 with value: 0.22093282639980316.\n",
      "[I 2025-06-06 09:15:14,536] Trial 7 finished with value: 0.2289736568927765 and parameters: {'lr': 0.0009006071833389195, 'filters1': 82, 'filters2': 106, 'dense_units': 109}. Best is trial 2 with value: 0.22093282639980316.\n",
      "[I 2025-06-06 09:15:19,374] Trial 8 finished with value: 0.23027673363685608 and parameters: {'lr': 0.0009510622352873905, 'filters1': 59, 'filters2': 116, 'dense_units': 52}. Best is trial 2 with value: 0.22093282639980316.\n",
      "[I 2025-06-06 09:15:23,388] Trial 9 finished with value: 0.21131636202335358 and parameters: {'lr': 0.0038889307253774756, 'filters1': 63, 'filters2': 38, 'dense_units': 56}. Best is trial 9 with value: 0.21131636202335358.\n",
      "[I 2025-06-06 09:15:29,188] Trial 10 finished with value: 0.21368980407714844 and parameters: {'lr': 0.006331173313227574, 'filters1': 99, 'filters2': 33, 'dense_units': 32}. Best is trial 9 with value: 0.21131636202335358.\n",
      "[I 2025-06-06 09:15:33,963] Trial 11 finished with value: 0.21480073034763336 and parameters: {'lr': 0.007607703659224066, 'filters1': 101, 'filters2': 32, 'dense_units': 32}. Best is trial 9 with value: 0.21131636202335358.\n",
      "[I 2025-06-06 09:15:37,986] Trial 12 finished with value: 0.1984192132949829 and parameters: {'lr': 0.0034390267839358897, 'filters1': 71, 'filters2': 32, 'dense_units': 34}. Best is trial 12 with value: 0.1984192132949829.\n",
      "[I 2025-06-06 09:15:41,966] Trial 13 finished with value: 0.22325022518634796 and parameters: {'lr': 0.0031796102990480847, 'filters1': 66, 'filters2': 40, 'dense_units': 44}. Best is trial 12 with value: 0.1984192132949829.\n",
      "[I 2025-06-06 09:15:46,032] Trial 14 finished with value: 0.22656255960464478 and parameters: {'lr': 0.0025844455356063985, 'filters1': 67, 'filters2': 47, 'dense_units': 72}. Best is trial 12 with value: 0.1984192132949829.\n",
      "[I 2025-06-06 09:15:50,646] Trial 15 finished with value: 0.2161806970834732 and parameters: {'lr': 0.00285359884472786, 'filters1': 37, 'filters2': 90, 'dense_units': 44}. Best is trial 12 with value: 0.1984192132949829.\n",
      "[I 2025-06-06 09:15:54,879] Trial 16 finished with value: 0.21322011947631836 and parameters: {'lr': 0.001600683053895396, 'filters1': 66, 'filters2': 42, 'dense_units': 125}. Best is trial 12 with value: 0.1984192132949829.\n",
      "[I 2025-06-06 09:15:59,358] Trial 17 finished with value: 0.21166381239891052 and parameters: {'lr': 0.004587839001476249, 'filters1': 73, 'filters2': 69, 'dense_units': 41}. Best is trial 12 with value: 0.1984192132949829.\n",
      "[I 2025-06-06 09:16:04,295] Trial 18 finished with value: 0.21674679219722748 and parameters: {'lr': 0.001606505386270929, 'filters1': 89, 'filters2': 85, 'dense_units': 72}. Best is trial 12 with value: 0.1984192132949829.\n",
      "[I 2025-06-06 09:16:08,093] Trial 19 finished with value: 0.23153267800807953 and parameters: {'lr': 0.0002805893161075787, 'filters1': 32, 'filters2': 47, 'dense_units': 61}. Best is trial 12 with value: 0.1984192132949829.\n",
      "[I 2025-06-06 09:16:11,980] Trial 20 finished with value: 0.223585844039917 and parameters: {'lr': 0.004340681857514315, 'filters1': 56, 'filters2': 65, 'dense_units': 40}. Best is trial 12 with value: 0.1984192132949829.\n",
      "[I 2025-06-06 09:16:17,645] Trial 21 finished with value: 0.22509291768074036 and parameters: {'lr': 0.005114131379261698, 'filters1': 75, 'filters2': 71, 'dense_units': 47}. Best is trial 12 with value: 0.1984192132949829.\n",
      "[I 2025-06-06 09:16:22,798] Trial 22 finished with value: 0.20551462471485138 and parameters: {'lr': 0.001831416819849218, 'filters1': 75, 'filters2': 128, 'dense_units': 39}. Best is trial 12 with value: 0.1984192132949829.\n",
      "[I 2025-06-06 09:16:27,965] Trial 23 finished with value: 0.22082175314426422 and parameters: {'lr': 0.0018484760983146027, 'filters1': 94, 'filters2': 91, 'dense_units': 32}. Best is trial 12 with value: 0.1984192132949829.\n",
      "[I 2025-06-06 09:16:32,908] Trial 24 finished with value: 0.21825480461120605 and parameters: {'lr': 0.0019090481123377205, 'filters1': 69, 'filters2': 126, 'dense_units': 53}. Best is trial 12 with value: 0.1984192132949829.\n",
      "[I 2025-06-06 09:16:36,849] Trial 25 finished with value: 0.2245834618806839 and parameters: {'lr': 0.003835378808460025, 'filters1': 59, 'filters2': 37, 'dense_units': 39}. Best is trial 12 with value: 0.1984192132949829.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pyswarms as ps\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.layers import ConvLSTM1D, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "globalpath = \"../DataCleaning/A - CSV par bâtiment/\"\n",
    "###############################################################################\n",
    "# 1) Chargement et préparation des données\n",
    "###############################################################################\n",
    "def load_and_prepare_data(csv_path, production_column='production', window_size=10):\n",
    "    \"\"\"\n",
    "    Charge le fichier CSV, scale les données, crée des fenêtres (x,y),\n",
    "    et renvoie les splits (x_train, y_train, x_test, y_test, x_val, y_val).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Mise à l'échelle\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data_scaled = scaler.fit_transform(df.values)\n",
    "    # ➕ Scaler dédié uniquement à la colonne cible\n",
    "    target_scaler = MinMaxScaler()\n",
    "    target_scaler.fit(df[[production_column]])\n",
    "    # Retrouver l'index de la colonne cible\n",
    "    target_col_idx = df.columns.get_loc(production_column)\n",
    "    \n",
    "    # Création des fenêtres\n",
    "    x, y = [], []\n",
    "    for i in range(window_size, len(data_scaled)):\n",
    "        x.append(data_scaled[i-window_size:i])\n",
    "        y.append(data_scaled[i, target_col_idx])\n",
    "    x, y = np.array(x), np.array(y)\n",
    "    \n",
    "    # Split train/test/val\n",
    "    # Ici, 80% train, 10% test, 10% val (à adapter si besoin)\n",
    "    train_split_index = int(0.8 * len(x))\n",
    "    test_split_index  = int(0.9 * len(x))\n",
    "    \n",
    "    x_train, y_train = x[:train_split_index], y[:train_split_index]\n",
    "    x_test,  y_test  = x[train_split_index:test_split_index], y[train_split_index:test_split_index]\n",
    "    x_val,   y_val   = x[test_split_index:], y[test_split_index:]\n",
    "    \n",
    "    # Adapter la forme pour ConvLSTM1D (on insère un channel dimension)\n",
    "    x_train_conv = np.expand_dims(x_train, axis=2)\n",
    "    x_test_conv  = np.expand_dims(x_test, axis=2)\n",
    "    x_val_conv   = np.expand_dims(x_val, axis=2)\n",
    "    \n",
    "    return (x_train_conv, y_train,\n",
    "            x_test_conv,  y_test,\n",
    "            x_val_conv,   y_val,\n",
    "            df,target_scaler  )\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2) Baseline (e-base) : un simple entraînement avec des hyperparamètres fixes\n",
    "###############################################################################\n",
    "def build_baseline_model(input_shape):\n",
    "    \"\"\"\n",
    "    Construit un modèle ConvLSTM basique avec des hyperparamètres\n",
    "    fixes (par ex. 64 filtres, 64 neurones denses, lr=0.001).\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        ConvLSTM1D(filters=64, kernel_size=(1,), activation='tanh',\n",
    "                   return_sequences=True, input_shape=input_shape),\n",
    "        ConvLSTM1D(filters=64, kernel_size=(1,), activation='tanh', return_sequences=False),\n",
    "        Flatten(),\n",
    "        Dense(units=64, activation='relu'),\n",
    "        Dense(1, activation=\"linear\")\n",
    "    ], name=\"baseline_conv_lstm\")\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss=\"mae\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3) Modèle paramétrable pour PSO\n",
    "###############################################################################\n",
    "def build_convlstm_model(lr, filters1, filters2, dense_units, input_shape):\n",
    "    \"\"\"\n",
    "    Construit et compile un modèle ConvLSTM1D avec hyperparamètres modulables.\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        ConvLSTM1D(filters=int(filters1), kernel_size=(1,), activation='tanh',\n",
    "                   return_sequences=True, input_shape=input_shape),\n",
    "        ConvLSTM1D(filters=int(filters2), kernel_size=(1,), activation='tanh', return_sequences=False),\n",
    "        Flatten(),\n",
    "        Dense(units=int(dense_units), activation='relu'),\n",
    "        Dense(1, activation=\"linear\")\n",
    "    ], name=\"model_conv_lstm\")\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"mae\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "def plot_and_save_analysis(y_test, y_pred, save_dir, dataset_name,target_scaler):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    y_test = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    y_pred = target_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # 1. Scatter plot (prédictions vs réel)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.7, color='orange')\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--', label=\"Idéal (y = ŷ)\")\n",
    "    plt.xlabel(\"Valeurs réelles (y)\")\n",
    "    plt.ylabel(\"Prédictions (ŷ)\")\n",
    "    plt.title(f\"{dataset_name} - Prédictions vs Réel\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    scatter_path = os.path.join(save_dir, f\"{dataset_name}_scatter.png\")\n",
    "    plt.savefig(scatter_path)\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Histogramme des erreurs\n",
    "    errors = y_test - y_pred\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(errors, bins=20, color='orange', edgecolor='black')\n",
    "    plt.title(f\"{dataset_name} - Distribution des erreurs\")\n",
    "    plt.xlabel(\"Erreur (y - ŷ)\")\n",
    "    plt.ylabel(\"Fréquence\")\n",
    "    plt.grid(True)\n",
    "    hist_path = os.path.join(save_dir, f\"{dataset_name}_hist.png\")\n",
    "    plt.savefig(hist_path)\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Courbe temporelle\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(y_test, label=\"Valeurs réelles\", linewidth=2)\n",
    "    plt.plot(y_pred, '--', label=\"Prédictions\")\n",
    "    plt.title(f\"{dataset_name} - Évolution temporelle\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Valeur\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    curve_path = os.path.join(save_dir, f\"{dataset_name}_courbe.png\")\n",
    "    plt.savefig(curve_path)\n",
    "    plt.close()\n",
    "\n",
    "    return scatter_path, hist_path, curve_path\n",
    "###############################################################################\n",
    "# 4) Entraînement + évaluation (MAE, MSE, R²) + temps d'exécution\n",
    "###############################################################################\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_val, y_val,\n",
    "                             epochs=50, batch_size=512, verbose=0, dataset_name=\"dataset\"):\n",
    "    \"\"\"\n",
    "    Entraîne le modèle, mesure le temps d'entraînement, et renvoie l'historique.\n",
    "    \"\"\"\n",
    "    stop_early = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=verbose,\n",
    "                        callbacks=[stop_early])\n",
    "    training_time = time.time() - start_time\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(history.history['loss'], label='train_loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.title(f\"{dataset_name}Courbe d'apprentissage (loss)\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{dataset_name}_plots_loss_curve_{model.name}.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return history, training_time\n",
    "\n",
    "\n",
    "def inference_time_and_metrics(model, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Calcule le temps d'inférence, puis renvoie MAE, MSE, R².\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    preds = model.predict(x_test)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    preds = preds.reshape(-1)\n",
    "    y_test = y_test.reshape(-1)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    r2  = r2_score(y_test, preds)\n",
    "    \n",
    "    return mae, mse, r2, inference_time\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5) Fonction objectif pour OPTUNA\n",
    "###############################################################################\n",
    "\n",
    "def objective(trial, input_shape, x_train, y_train, x_val, y_val):\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)  # nouvelle syntaxe recommandée\n",
    "    f1 = trial.suggest_int('filters1', 32, 128)\n",
    "    f2 = trial.suggest_int('filters2', 32, 128)\n",
    "    dense = trial.suggest_int('dense_units', 32, 128)\n",
    "\n",
    "    model = Sequential([\n",
    "        ConvLSTM1D(filters=f1, kernel_size=(1,), activation='tanh', return_sequences=True, input_shape=input_shape),\n",
    "        ConvLSTM1D(filters=f2, kernel_size=(1,), activation='tanh', return_sequences=False),\n",
    "        Flatten(),\n",
    "        Dense(units=dense, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mae')\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=10,\n",
    "        batch_size=256,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    return min(history.history['val_loss'])\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 6) Boucle sur les datasets et compilation des résultats\n",
    "###############################################################################\n",
    "def run_experiments_on_datasets(\n",
    "    dataset_paths,\n",
    "    production_column='production',\n",
    "    window_size=10,\n",
    "    epochs_baseline=50,\n",
    "    epochs_optimized=50\n",
    "):\n",
    "    \"\"\"\n",
    "    - Pour chaque dataset :\n",
    "        1) Prépare les données\n",
    "        2) Entraîne le modèle baseline (e-base) et mesure ses métriques\n",
    "        3) Lance l'optimisation PSO\n",
    "        4) Entraîne le modèle avec les hyperparams optimisés\n",
    "        5) Mesure les métriques et temps\n",
    "        6) Stocke les résultats dans un DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for csv_path in dataset_paths:\n",
    "        dataset_name = os.path.basename(csv_path).replace('.csv','')\n",
    "        print(f\"\\n=== Dataset: {dataset_name} ===\")\n",
    "        \n",
    "        # 1) Chargement et préparation\n",
    "        x_train, y_train, x_test, y_test, x_val, y_val, df,target_scaler  = load_and_prepare_data(\n",
    "            csv_path,\n",
    "            production_column=production_column,\n",
    "            window_size=window_size\n",
    "        )\n",
    "        input_shape = x_train.shape[1:]\n",
    "        \n",
    "        # 2) Modèle baseline\n",
    "        baseline_model = build_baseline_model(input_shape)\n",
    "        history_base, t_train_base = train_and_evaluate_model(\n",
    "            baseline_model, x_train, y_train, x_val, y_val,\n",
    "            epochs=epochs_baseline, batch_size=512, verbose=0,dataset_name=dataset_name\n",
    "        )\n",
    "        mae_base, mse_base, r2_base, t_infer_base = inference_time_and_metrics(baseline_model, x_test, y_test)\n",
    "        # 3) Optuna\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: objective(trial, input_shape, x_train, y_train, x_val, y_val), n_trials=30)\n",
    "        print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "        \n",
    "        # 4) Entraîner le modèle avec les hyperparamètres optimisés\n",
    "        best_params = study.best_params\n",
    "        lr_opt        = best_params['lr']\n",
    "        filters1_opt  = best_params['filters1']\n",
    "        filters2_opt  = best_params['filters2']\n",
    "        dense_opt     = best_params['dense_units']\n",
    "\n",
    "        # Ajoute ce format dans le dictionnaire des résultats :\n",
    "        param_opt_string = f\"lr={lr_opt:.5f}, f1={filters1_opt}, f2={filters2_opt}, dense={dense_opt}\"\n",
    "        best_model = build_convlstm_model(lr_opt, filters1_opt, filters2_opt, dense_opt, input_shape)\n",
    "        history_opt, t_train_opt = train_and_evaluate_model(\n",
    "            best_model, x_train, y_train, x_val, y_val,\n",
    "            epochs=epochs_optimized, batch_size=512, verbose=0,dataset_name=dataset_name\n",
    "        )\n",
    "        \n",
    "        # 5) Évaluation finale\n",
    "        mae_opt, mse_opt, r2_opt, t_infer_opt = inference_time_and_metrics(best_model, x_test, y_test)\n",
    "        \n",
    "        scatter_path, hist_path, curve_path = plot_and_save_analysis(\n",
    "            y_test=y_test, \n",
    "            y_pred=best_model.predict(x_test).flatten(), \n",
    "            save_dir=\"plots\", \n",
    "            dataset_name=dataset_name,\n",
    "            target_scaler=target_scaler\n",
    "        )\n",
    "        \n",
    "        # 6) Stockage des résultats dans un dictionnaire\n",
    "        result_dict = {\n",
    "            \"Dataset\": dataset_name,\n",
    "            \n",
    "            # E-base\n",
    "            \"MAE e-base\": mae_base,\n",
    "            \"MSE e-base\": mse_base,\n",
    "            \"R2 e-base\":  r2_base,\n",
    "            \"T(entrainement-e-base)[s]\": t_train_base,\n",
    "            \n",
    "            \n",
    "            #Optuna\n",
    "            \"Optuna best R2\": r2_opt,\n",
    "            \"Paramètres optimisés\": f\"lr={lr_opt:.5f}, f1={filters1_opt}, f2={filters2_opt}, dense={dense_opt}\",\n",
    "            \n",
    "            # Entraînement optimisé\n",
    "            \"T(entrainement-optimisé)[s]\": t_train_opt,\n",
    "            \"MAE optimisé\": mae_opt,\n",
    "            \"MSE optimisé\": mse_opt,\n",
    "            \"R2 optimisé\": r2_opt,\n",
    "            \"Graph_scatter\": scatter_path,\n",
    "            \"Graph_hist\": hist_path,\n",
    "            \"Graph_courbe\": curve_path,\n",
    "            \n",
    "            # Inférence\n",
    "            \"T(evaluation-inference)[s]\": t_infer_opt,\n",
    "            \n",
    "            # Nombre d'époques\n",
    "            \"Nombre d'époques e-base\": epochs_baseline,\n",
    "            \"Nombre d'époques optimisé\": epochs_optimized\n",
    "        }\n",
    "        \n",
    "        results.append(result_dict)\n",
    "    \n",
    "    # Conversion en DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    return df_results\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 7) Lancement final (exemple)\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # Liste des chemins vers vos CSV\n",
    "    dataset_paths = [\n",
    "        globalpath+\"batiment_1_Clean3.csv\",\n",
    "        #globalpath+\"batiment_1.csv\",\n",
    "        #globalpath+\"batiment_2.csv\",\n",
    "        #globalpath+\"batiment_3.csv\",\n",
    "        #globalpath+\"batiment_4.csv\",\n",
    "        #globalpath+\"batiment_5.csv\",\n",
    "        #globalpath+\"batiment_6.csv\",\n",
    "        #globalpath+\"batiment_7.csv\",\n",
    "        #globalpath+\"batiment_8.csv\",\n",
    "        #globalpath+\"batiment_9.csv\"\n",
    "    ]\n",
    "    \n",
    "    # Paramètres globaux (à adapter)\n",
    "    production_column = 'production'\n",
    "    window_size = 10      # taille des fenêtres\n",
    "    epochs_baseline = 30  # nombre d'époques pour la baseline\n",
    "    epochs_optimized = 100 # nombre d'époques pour le modèle optimisé\n",
    "    \n",
    "    # Lancement des expériences\n",
    "    df_results = run_experiments_on_datasets(\n",
    "        dataset_paths,\n",
    "        production_column=production_column,\n",
    "        window_size=window_size,\n",
    "        epochs_baseline=epochs_baseline,\n",
    "        epochs_optimized=epochs_optimized\n",
    "    )\n",
    "    \n",
    "    # Affichage des résultats finaux\n",
    "    print(\"\\n=========== RÉSULTATS FINAUX ===========\")\n",
    "    print(df_results)\n",
    "    # Sauvegarde éventuellement en CSV\n",
    "    df_results.to_csv(\"resume_resultatsOptuna.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
